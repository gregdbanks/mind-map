{
  "nodes": [
    {
      "id": "root",
      "text": "AWS Security SCS-C03 \u2014 Vendor Lock-In Analysis",
      "x": 900,
      "y": 700,
      "parent": null,
      "collapsed": false,
      "color": "#C0392B",
      "fx": 900,
      "fy": 700,
      "depth": 0,
      "hasNote": true,
      "noteId": "note-root"
    },
    {
      "id": "security-hub",
      "text": "AWS Security Hub",
      "x": 900.0,
      "y": 450.0,
      "parent": "root",
      "collapsed": false,
      "color": "#E74C3C",
      "fx": 900.0,
      "fy": 450.0,
      "depth": 1,
      "cluster": "security-hub",
      "hasNote": true,
      "noteId": "note-security-hub"
    },
    {
      "id": "cloudtrail",
      "text": "AWS CloudTrail",
      "x": 1076.78,
      "y": 523.22,
      "parent": "root",
      "collapsed": false,
      "color": "#E74C3C",
      "fx": 1076.78,
      "fy": 523.22,
      "depth": 1,
      "cluster": "cloudtrail",
      "hasNote": true,
      "noteId": "note-cloudtrail"
    },
    {
      "id": "iam",
      "text": "AWS IAM",
      "x": 1076.78,
      "y": 876.78,
      "parent": "root",
      "collapsed": false,
      "color": "#E74C3C",
      "fx": 1076.78,
      "fy": 876.78,
      "depth": 1,
      "cluster": "iam",
      "hasNote": true,
      "noteId": "note-iam"
    },
    {
      "id": "kms",
      "text": "AWS KMS",
      "x": 723.22,
      "y": 876.78,
      "parent": "root",
      "collapsed": false,
      "color": "#E74C3C",
      "fx": 723.22,
      "fy": 876.78,
      "depth": 1,
      "cluster": "kms",
      "hasNote": true,
      "noteId": "note-kms"
    },
    {
      "id": "s3",
      "text": "Amazon S3",
      "x": 658.52,
      "y": 764.7,
      "parent": "root",
      "collapsed": false,
      "color": "#F39C12",
      "fx": 658.52,
      "fy": 764.7,
      "depth": 1,
      "cluster": "s3",
      "hasNote": true,
      "noteId": "note-s3"
    },
    {
      "id": "organizations",
      "text": "AWS Organizations",
      "x": 665.08,
      "y": 614.49,
      "parent": "root",
      "collapsed": false,
      "color": "#E74C3C",
      "fx": 665.08,
      "fy": 614.49,
      "depth": 1,
      "cluster": "organizations",
      "hasNote": true,
      "noteId": "note-organizations"
    },
    {
      "id": "cloudwatch",
      "text": "Amazon CloudWatch",
      "x": 1134.92,
      "y": 614.49,
      "parent": "root",
      "collapsed": false,
      "color": "#F39C12",
      "fx": 1134.92,
      "fy": 614.49,
      "depth": 1,
      "cluster": "cloudwatch",
      "hasNote": true,
      "noteId": "note-cloudwatch"
    },
    {
      "id": "guardduty",
      "text": "Amazon GuardDuty",
      "x": 775.0,
      "y": 483.49,
      "parent": "root",
      "collapsed": false,
      "color": "#E74C3C",
      "fx": 775.0,
      "fy": 483.49,
      "depth": 1,
      "cluster": "guardduty",
      "hasNote": true,
      "noteId": "note-guardduty"
    },
    {
      "id": "lambda",
      "text": "AWS Lambda",
      "x": 1150.0,
      "y": 700.0,
      "parent": "root",
      "collapsed": false,
      "color": "#F39C12",
      "fx": 1150.0,
      "fy": 700.0,
      "depth": 1,
      "cluster": "lambda",
      "hasNote": true,
      "noteId": "note-lambda"
    },
    {
      "id": "vpc",
      "text": "Amazon VPC",
      "x": 900.0,
      "y": 950.0,
      "parent": "root",
      "collapsed": false,
      "color": "#F39C12",
      "fx": 900.0,
      "fy": 950.0,
      "depth": 1,
      "cluster": "vpc",
      "hasNote": true,
      "noteId": "note-vpc"
    },
    {
      "id": "inspector",
      "text": "Amazon Inspector",
      "x": 735.79,
      "y": 406.0,
      "parent": "security-hub",
      "collapsed": false,
      "color": "#EC7063",
      "fx": 735.79,
      "fy": 406.0,
      "depth": 2,
      "cluster": "security-hub",
      "hasNote": true,
      "noteId": "note-inspector"
    },
    {
      "id": "macie",
      "text": "Amazon Macie",
      "x": 779.79,
      "y": 329.79,
      "parent": "security-hub",
      "collapsed": false,
      "color": "#EC7063",
      "fx": 779.79,
      "fy": 329.79,
      "depth": 2,
      "cluster": "security-hub",
      "hasNote": true,
      "noteId": "note-macie"
    },
    {
      "id": "config",
      "text": "AWS Config",
      "x": 856.0,
      "y": 285.79,
      "parent": "security-hub",
      "collapsed": false,
      "color": "#EC7063",
      "fx": 856.0,
      "fy": 285.79,
      "depth": 2,
      "cluster": "security-hub",
      "hasNote": true,
      "noteId": "note-config"
    },
    {
      "id": "eventbridge",
      "text": "Amazon EventBridge",
      "x": 944.0,
      "y": 285.79,
      "parent": "security-hub",
      "collapsed": false,
      "color": "#EC7063",
      "fx": 944.0,
      "fy": 285.79,
      "depth": 2,
      "cluster": "security-hub",
      "hasNote": true,
      "noteId": "note-eventbridge"
    },
    {
      "id": "shield",
      "text": "AWS Shield",
      "x": 1020.21,
      "y": 329.79,
      "parent": "security-hub",
      "collapsed": false,
      "color": "#F5B041",
      "fx": 1020.21,
      "fy": 329.79,
      "depth": 2,
      "cluster": "security-hub",
      "hasNote": true,
      "noteId": "note-shield"
    },
    {
      "id": "trusted-advisor",
      "text": "AWS Trusted Advisor",
      "x": 1064.21,
      "y": 406.0,
      "parent": "security-hub",
      "collapsed": false,
      "color": "#F5B041",
      "fx": 1064.21,
      "fy": 406.0,
      "depth": 2,
      "cluster": "security-hub",
      "hasNote": true,
      "noteId": "note-trusted-advisor"
    },
    {
      "id": "detective",
      "text": "Amazon Detective",
      "x": 690.0,
      "y": 336.27,
      "parent": "guardduty",
      "collapsed": false,
      "color": "#EC7063",
      "fx": 690.0,
      "fy": 336.27,
      "depth": 2,
      "cluster": "guardduty",
      "hasNote": true,
      "noteId": "note-detective"
    },
    {
      "id": "security-lake",
      "text": "Amazon Security Lake",
      "x": 1098.97,
      "y": 354.67,
      "parent": "cloudtrail",
      "collapsed": false,
      "color": "#EC7063",
      "fx": 1098.97,
      "fy": 354.67,
      "depth": 2,
      "cluster": "cloudtrail",
      "hasNote": true,
      "noteId": "note-security-lake"
    },
    {
      "id": "athena",
      "text": "Amazon Athena",
      "x": 1196.99,
      "y": 403.01,
      "parent": "cloudtrail",
      "collapsed": false,
      "color": "#52BE80",
      "fx": 1196.99,
      "fy": 403.01,
      "depth": 2,
      "cluster": "cloudtrail",
      "hasNote": true,
      "noteId": "note-athena"
    },
    {
      "id": "cloudtrail-lake",
      "text": "AWS CloudTrail Lake",
      "x": 1245.33,
      "y": 501.03,
      "parent": "cloudtrail",
      "collapsed": false,
      "color": "#EC7063",
      "fx": 1245.33,
      "fy": 501.03,
      "depth": 2,
      "cluster": "cloudtrail",
      "hasNote": true,
      "noteId": "note-cloudtrail-lake"
    },
    {
      "id": "opensearch",
      "text": "Amazon OpenSearch Service",
      "x": 1193.06,
      "y": 454.74,
      "parent": "cloudwatch",
      "collapsed": false,
      "color": "#52BE80",
      "fx": 1193.06,
      "fy": 454.74,
      "depth": 2,
      "cluster": "cloudwatch",
      "hasNote": true,
      "noteId": "note-opensearch"
    },
    {
      "id": "managed-grafana",
      "text": "Amazon Managed Grafana",
      "x": 1271.28,
      "y": 512.97,
      "parent": "cloudwatch",
      "collapsed": false,
      "color": "#52BE80",
      "fx": 1271.28,
      "fy": 512.97,
      "depth": 2,
      "cluster": "cloudwatch",
      "hasNote": true,
      "noteId": "note-managed-grafana"
    },
    {
      "id": "sns",
      "text": "Amazon SNS",
      "x": 1304.63,
      "y": 604.6,
      "parent": "cloudwatch",
      "collapsed": false,
      "color": "#F5B041",
      "fx": 1304.63,
      "fy": 604.6,
      "depth": 2,
      "cluster": "cloudwatch",
      "hasNote": true,
      "noteId": "note-sns"
    },
    {
      "id": "user-notifications",
      "text": "AWS User Notifications",
      "x": 1282.15,
      "y": 699.49,
      "parent": "cloudwatch",
      "collapsed": false,
      "color": "#F5B041",
      "fx": 1282.15,
      "fy": 699.49,
      "depth": 2,
      "cluster": "cloudwatch",
      "hasNote": true,
      "noteId": "note-user-notifications"
    },
    {
      "id": "step-functions",
      "text": "AWS Step Functions",
      "x": 1228.5,
      "y": 549.21,
      "parent": "lambda",
      "collapsed": false,
      "color": "#EC7063",
      "fx": 1228.5,
      "fy": 549.21,
      "depth": 2,
      "cluster": "lambda",
      "hasNote": true,
      "noteId": "note-step-functions"
    },
    {
      "id": "systems-manager",
      "text": "AWS Systems Manager",
      "x": 1295.34,
      "y": 611.81,
      "parent": "lambda",
      "collapsed": false,
      "color": "#EC7063",
      "fx": 1295.34,
      "fy": 611.81,
      "depth": 2,
      "cluster": "lambda",
      "hasNote": true,
      "noteId": "note-systems-manager"
    },
    {
      "id": "api-gateway",
      "text": "Amazon API Gateway",
      "x": 1320.0,
      "y": 700.0,
      "parent": "lambda",
      "collapsed": false,
      "color": "#F5B041",
      "fx": 1320.0,
      "fy": 700.0,
      "depth": 2,
      "cluster": "lambda",
      "hasNote": true,
      "noteId": "note-api-gateway"
    },
    {
      "id": "bedrock",
      "text": "Amazon Bedrock",
      "x": 1295.34,
      "y": 788.19,
      "parent": "lambda",
      "collapsed": false,
      "color": "#EC7063",
      "fx": 1295.34,
      "fy": 788.19,
      "depth": 2,
      "cluster": "lambda",
      "hasNote": true,
      "noteId": "note-bedrock"
    },
    {
      "id": "sagemaker",
      "text": "Amazon SageMaker AI",
      "x": 1228.5,
      "y": 850.79,
      "parent": "lambda",
      "collapsed": false,
      "color": "#EC7063",
      "fx": 1228.5,
      "fy": 850.79,
      "depth": 2,
      "cluster": "lambda",
      "hasNote": true,
      "noteId": "note-sagemaker"
    },
    {
      "id": "forensics-orchestrator",
      "text": "Automated Forensics Orchestrator",
      "x": 1273.48,
      "y": 385.27,
      "parent": "step-functions",
      "collapsed": false,
      "color": "#EC7063",
      "fx": 1273.48,
      "fy": 385.27,
      "depth": 2,
      "cluster": "step-functions",
      "hasNote": true,
      "noteId": "note-forensics-orchestrator"
    },
    {
      "id": "fis",
      "text": "AWS Fault Injection Service",
      "x": 1356.17,
      "y": 436.96,
      "parent": "step-functions",
      "collapsed": false,
      "color": "#F5B041",
      "fx": 1356.17,
      "fy": 436.96,
      "depth": 2,
      "cluster": "step-functions",
      "hasNote": true,
      "noteId": "note-fis"
    },
    {
      "id": "resilience-hub",
      "text": "AWS Resilience Hub",
      "x": 1396.85,
      "y": 525.58,
      "parent": "step-functions",
      "collapsed": false,
      "color": "#F5B041",
      "fx": 1396.85,
      "fy": 525.58,
      "depth": 2,
      "cluster": "step-functions",
      "hasNote": true,
      "noteId": "note-resilience-hub"
    },
    {
      "id": "app-recovery-controller",
      "text": "Application Recovery Controller",
      "x": 1382.14,
      "y": 621.98,
      "parent": "step-functions",
      "collapsed": false,
      "color": "#EC7063",
      "fx": 1382.14,
      "fy": 621.98,
      "depth": 2,
      "cluster": "step-functions",
      "hasNote": true,
      "noteId": "note-app-recovery-controller"
    },
    {
      "id": "waf",
      "text": "AWS WAF",
      "x": 1067.42,
      "y": 920.48,
      "parent": "vpc",
      "collapsed": false,
      "color": "#F5B041",
      "fx": 1067.42,
      "fy": 920.48,
      "depth": 2,
      "cluster": "vpc",
      "hasNote": true,
      "noteId": "note-waf"
    },
    {
      "id": "cloudfront",
      "text": "Amazon CloudFront",
      "x": 1068.27,
      "y": 974.19,
      "parent": "vpc",
      "collapsed": false,
      "color": "#F5B041",
      "fx": 1068.27,
      "fy": 974.19,
      "depth": 2,
      "cluster": "vpc",
      "hasNote": true,
      "noteId": "note-cloudfront"
    },
    {
      "id": "route53",
      "text": "Amazon Route 53",
      "x": 1052.32,
      "y": 1025.49,
      "parent": "vpc",
      "collapsed": false,
      "color": "#F5B041",
      "fx": 1052.32,
      "fy": 1025.49,
      "depth": 2,
      "cluster": "vpc",
      "hasNote": true,
      "noteId": "note-route53"
    },
    {
      "id": "elb",
      "text": "Elastic Load Balancing",
      "x": 1021.16,
      "y": 1069.25,
      "parent": "vpc",
      "collapsed": false,
      "color": "#F5B041",
      "fx": 1021.16,
      "fy": 1069.25,
      "depth": 2,
      "cluster": "vpc",
      "hasNote": true,
      "noteId": "note-elb"
    },
    {
      "id": "network-firewall",
      "text": "AWS Network Firewall",
      "x": 977.9,
      "y": 1101.1,
      "parent": "vpc",
      "collapsed": false,
      "color": "#F5B041",
      "fx": 977.9,
      "fy": 1101.1,
      "depth": 2,
      "cluster": "vpc",
      "hasNote": true,
      "noteId": "note-network-firewall"
    },
    {
      "id": "direct-connect",
      "text": "AWS Direct Connect",
      "x": 926.86,
      "y": 1117.86,
      "parent": "vpc",
      "collapsed": false,
      "color": "#EC7063",
      "fx": 926.86,
      "fy": 1117.86,
      "depth": 2,
      "cluster": "vpc",
      "hasNote": true,
      "noteId": "note-direct-connect"
    },
    {
      "id": "vpn",
      "text": "AWS VPN",
      "x": 873.14,
      "y": 1117.86,
      "parent": "vpc",
      "collapsed": false,
      "color": "#52BE80",
      "fx": 873.14,
      "fy": 1117.86,
      "depth": 2,
      "cluster": "vpc",
      "hasNote": true,
      "noteId": "note-vpn"
    },
    {
      "id": "transit-gateway",
      "text": "AWS Transit Gateway",
      "x": 822.1,
      "y": 1101.1,
      "parent": "vpc",
      "collapsed": false,
      "color": "#F5B041",
      "fx": 822.1,
      "fy": 1101.1,
      "depth": 2,
      "cluster": "vpc",
      "hasNote": true,
      "noteId": "note-transit-gateway"
    },
    {
      "id": "verified-access",
      "text": "AWS Verified Access",
      "x": 778.84,
      "y": 1069.25,
      "parent": "vpc",
      "collapsed": false,
      "color": "#EC7063",
      "fx": 778.84,
      "fy": 1069.25,
      "depth": 2,
      "cluster": "vpc",
      "hasNote": true,
      "noteId": "note-verified-access"
    },
    {
      "id": "network-access-analyzer",
      "text": "Network Access Analyzer",
      "x": 747.68,
      "y": 1025.49,
      "parent": "vpc",
      "collapsed": false,
      "color": "#F5B041",
      "fx": 747.68,
      "fy": 1025.49,
      "depth": 2,
      "cluster": "vpc",
      "hasNote": true,
      "noteId": "note-network-access-analyzer"
    },
    {
      "id": "ec2",
      "text": "Amazon EC2",
      "x": 731.73,
      "y": 974.19,
      "parent": "vpc",
      "collapsed": false,
      "color": "#F5B041",
      "fx": 731.73,
      "fy": 974.19,
      "depth": 2,
      "cluster": "vpc",
      "hasNote": true,
      "noteId": "note-ec2"
    },
    {
      "id": "iot-core",
      "text": "AWS IoT Core",
      "x": 732.58,
      "y": 920.48,
      "parent": "vpc",
      "collapsed": false,
      "color": "#EC7063",
      "fx": 732.58,
      "fy": 920.48,
      "depth": 2,
      "cluster": "vpc",
      "hasNote": true,
      "noteId": "note-iot-core"
    },
    {
      "id": "firewall-manager",
      "text": "AWS Firewall Manager",
      "x": 635.56,
      "y": 781.91,
      "parent": "organizations",
      "collapsed": false,
      "color": "#EC7063",
      "fx": 635.56,
      "fy": 781.91,
      "depth": 2,
      "cluster": "organizations",
      "hasNote": true,
      "noteId": "note-firewall-manager"
    },
    {
      "id": "control-tower",
      "text": "AWS Control Tower",
      "x": 559.08,
      "y": 747.4,
      "parent": "organizations",
      "collapsed": false,
      "color": "#EC7063",
      "fx": 559.08,
      "fy": 747.4,
      "depth": 2,
      "cluster": "organizations",
      "hasNote": true,
      "noteId": "note-control-tower"
    },
    {
      "id": "cloudformation",
      "text": "AWS CloudFormation",
      "x": 508.43,
      "y": 680.52,
      "parent": "organizations",
      "collapsed": false,
      "color": "#EC7063",
      "fx": 508.43,
      "fy": 680.52,
      "depth": 2,
      "cluster": "organizations",
      "hasNote": true,
      "noteId": "note-cloudformation"
    },
    {
      "id": "service-catalog",
      "text": "AWS Service Catalog",
      "x": 495.93,
      "y": 597.56,
      "parent": "organizations",
      "collapsed": false,
      "color": "#EC7063",
      "fx": 495.93,
      "fy": 597.56,
      "depth": 2,
      "cluster": "organizations",
      "hasNote": true,
      "noteId": "note-service-catalog"
    },
    {
      "id": "ram",
      "text": "AWS Resource Access Manager",
      "x": 524.62,
      "y": 518.72,
      "parent": "organizations",
      "collapsed": false,
      "color": "#F5B041",
      "fx": 524.62,
      "fy": 518.72,
      "depth": 2,
      "cluster": "organizations",
      "hasNote": true,
      "noteId": "note-ram"
    },
    {
      "id": "audit-manager",
      "text": "AWS Audit Manager",
      "x": 587.53,
      "y": 463.21,
      "parent": "organizations",
      "collapsed": false,
      "color": "#EC7063",
      "fx": 587.53,
      "fy": 463.21,
      "depth": 2,
      "cluster": "organizations",
      "hasNote": true,
      "noteId": "note-audit-manager"
    },
    {
      "id": "artifact",
      "text": "AWS Artifact",
      "x": 669.32,
      "y": 444.54,
      "parent": "organizations",
      "collapsed": false,
      "color": "#52BE80",
      "fx": 669.32,
      "fy": 444.54,
      "depth": 2,
      "cluster": "organizations",
      "hasNote": true,
      "noteId": "note-artifact"
    },
    {
      "id": "well-architected",
      "text": "AWS Well-Architected Tool",
      "x": 750.08,
      "y": 467.27,
      "parent": "organizations",
      "collapsed": false,
      "color": "#52BE80",
      "fx": 750.08,
      "fy": 467.27,
      "depth": 2,
      "cluster": "organizations",
      "hasNote": true,
      "noteId": "note-well-architected"
    },
    {
      "id": "ec2-image-builder",
      "text": "EC2 Image Builder",
      "x": 889.86,
      "y": 1036.6,
      "parent": "ec2",
      "collapsed": false,
      "color": "#F5B041",
      "fx": 889.86,
      "fy": 1036.6,
      "depth": 2,
      "cluster": "ec2",
      "hasNote": true,
      "noteId": "note-ec2-image-builder"
    },
    {
      "id": "ecr",
      "text": "Amazon ECR",
      "x": 840.76,
      "y": 1104.62,
      "parent": "ec2",
      "collapsed": false,
      "color": "#52BE80",
      "fx": 840.76,
      "fy": 1104.62,
      "depth": 2,
      "cluster": "ec2",
      "hasNote": true,
      "noteId": "note-ecr"
    },
    {
      "id": "dlm",
      "text": "Amazon Data Lifecycle Manager",
      "x": 765.1,
      "y": 1140.88,
      "parent": "ec2",
      "collapsed": false,
      "color": "#F5B041",
      "fx": 765.1,
      "fy": 1140.88,
      "depth": 2,
      "cluster": "ec2",
      "hasNote": true,
      "noteId": "note-dlm"
    },
    {
      "id": "ebs",
      "text": "Amazon EBS",
      "x": 681.31,
      "y": 1136.54,
      "parent": "ec2",
      "collapsed": false,
      "color": "#F5B041",
      "fx": 681.31,
      "fy": 1136.54,
      "depth": 2,
      "cluster": "ec2",
      "hasNote": true,
      "noteId": "note-ebs"
    },
    {
      "id": "q-developer",
      "text": "Amazon Q Developer",
      "x": 609.81,
      "y": 1092.66,
      "parent": "ec2",
      "collapsed": false,
      "color": "#F5B041",
      "fx": 609.81,
      "fy": 1092.66,
      "depth": 2,
      "cluster": "ec2",
      "hasNote": true,
      "noteId": "note-q-developer"
    },
    {
      "id": "codeguru",
      "text": "Amazon CodeGuru Security",
      "x": 568.0,
      "y": 1019.92,
      "parent": "ec2",
      "collapsed": false,
      "color": "#F5B041",
      "fx": 568.0,
      "fy": 1019.92,
      "depth": 2,
      "cluster": "ec2",
      "hasNote": true,
      "noteId": "note-codeguru"
    },
    {
      "id": "eks",
      "text": "Amazon EKS",
      "x": 566.06,
      "y": 936.05,
      "parent": "ec2",
      "collapsed": false,
      "color": "#52BE80",
      "fx": 566.06,
      "fy": 936.05,
      "depth": 2,
      "cluster": "ec2",
      "hasNote": true,
      "noteId": "note-eks"
    },
    {
      "id": "emr",
      "text": "Amazon EMR",
      "x": 604.48,
      "y": 861.46,
      "parent": "ec2",
      "collapsed": false,
      "color": "#F5B041",
      "fx": 604.48,
      "fy": 861.46,
      "depth": 2,
      "cluster": "ec2",
      "hasNote": true,
      "noteId": "note-emr"
    },
    {
      "id": "identity-center",
      "text": "IAM Identity Center",
      "x": 1224.0,
      "y": 791.78,
      "parent": "iam",
      "collapsed": false,
      "color": "#EC7063",
      "fx": 1224.0,
      "fy": 791.78,
      "depth": 2,
      "cluster": "iam",
      "hasNote": true,
      "noteId": "note-identity-center"
    },
    {
      "id": "cognito",
      "text": "Amazon Cognito",
      "x": 1246.78,
      "y": 876.78,
      "parent": "iam",
      "collapsed": false,
      "color": "#F5B041",
      "fx": 1246.78,
      "fy": 876.78,
      "depth": 2,
      "cluster": "iam",
      "hasNote": true,
      "noteId": "note-cognito"
    },
    {
      "id": "sts",
      "text": "AWS STS",
      "x": 1224.0,
      "y": 961.78,
      "parent": "iam",
      "collapsed": false,
      "color": "#F5B041",
      "fx": 1224.0,
      "fy": 961.78,
      "depth": 2,
      "cluster": "iam",
      "hasNote": true,
      "noteId": "note-sts"
    },
    {
      "id": "directory-service",
      "text": "AWS Directory Service",
      "x": 1161.78,
      "y": 1024.0,
      "parent": "iam",
      "collapsed": false,
      "color": "#52BE80",
      "fx": 1161.78,
      "fy": 1024.0,
      "depth": 2,
      "cluster": "iam",
      "hasNote": true,
      "noteId": "note-directory-service"
    },
    {
      "id": "verified-permissions",
      "text": "Amazon Verified Permissions",
      "x": 1076.78,
      "y": 1046.78,
      "parent": "iam",
      "collapsed": false,
      "color": "#EC7063",
      "fx": 1076.78,
      "fy": 1046.78,
      "depth": 2,
      "cluster": "iam",
      "hasNote": true,
      "noteId": "note-verified-permissions"
    },
    {
      "id": "iam-roles-anywhere",
      "text": "IAM Roles Anywhere",
      "x": 991.78,
      "y": 1024.0,
      "parent": "iam",
      "collapsed": false,
      "color": "#F5B041",
      "fx": 991.78,
      "fy": 1024.0,
      "depth": 2,
      "cluster": "iam",
      "hasNote": true,
      "noteId": "note-iam-roles-anywhere"
    },
    {
      "id": "cloudhsm",
      "text": "AWS CloudHSM",
      "x": 738.04,
      "y": 1046.13,
      "parent": "kms",
      "collapsed": false,
      "color": "#52BE80",
      "fx": 738.04,
      "fy": 1046.13,
      "depth": 2,
      "cluster": "kms",
      "hasNote": true,
      "noteId": "note-cloudhsm"
    },
    {
      "id": "acm",
      "text": "AWS Certificate Manager",
      "x": 642.54,
      "y": 1026.41,
      "parent": "kms",
      "collapsed": false,
      "color": "#F5B041",
      "fx": 642.54,
      "fy": 1026.41,
      "depth": 2,
      "cluster": "kms",
      "hasNote": true,
      "noteId": "note-acm"
    },
    {
      "id": "private-ca",
      "text": "AWS Private Certificate Authority",
      "x": 573.59,
      "y": 957.46,
      "parent": "kms",
      "collapsed": false,
      "color": "#F5B041",
      "fx": 573.59,
      "fy": 957.46,
      "depth": 2,
      "cluster": "kms",
      "hasNote": true,
      "noteId": "note-private-ca"
    },
    {
      "id": "secrets-manager",
      "text": "AWS Secrets Manager",
      "x": 553.87,
      "y": 861.96,
      "parent": "kms",
      "collapsed": false,
      "color": "#F5B041",
      "fx": 553.87,
      "fy": 861.96,
      "depth": 2,
      "cluster": "kms",
      "hasNote": true,
      "noteId": "note-secrets-manager"
    },
    {
      "id": "parameter-store",
      "text": "Systems Manager Parameter Store",
      "x": 1461.26,
      "y": 574.8,
      "parent": "systems-manager",
      "collapsed": false,
      "color": "#F5B041",
      "fx": 1461.26,
      "fy": 574.8,
      "depth": 2,
      "cluster": "systems-manager",
      "hasNote": true,
      "noteId": "note-parameter-store"
    },
    {
      "id": "backup",
      "text": "AWS Backup",
      "x": 555.03,
      "y": 899.57,
      "parent": "s3",
      "collapsed": false,
      "color": "#F5B041",
      "fx": 555.03,
      "fy": 899.57,
      "depth": 2,
      "cluster": "s3",
      "hasNote": true,
      "noteId": "note-backup"
    },
    {
      "id": "datasync",
      "text": "AWS DataSync",
      "x": 494.31,
      "y": 808.7,
      "parent": "s3",
      "collapsed": false,
      "color": "#52BE80",
      "fx": 494.31,
      "fy": 808.7,
      "depth": 2,
      "cluster": "s3",
      "hasNote": true,
      "noteId": "note-datasync"
    },
    {
      "id": "efs",
      "text": "Amazon EFS",
      "x": 501.46,
      "y": 699.64,
      "parent": "s3",
      "collapsed": false,
      "color": "#F5B041",
      "fx": 501.46,
      "fy": 699.64,
      "depth": 2,
      "cluster": "s3",
      "hasNote": true,
      "noteId": "note-efs"
    }
  ],
  "links": [
    {
      "source": "root",
      "target": "security-hub"
    },
    {
      "source": "root",
      "target": "cloudtrail"
    },
    {
      "source": "root",
      "target": "iam"
    },
    {
      "source": "root",
      "target": "kms"
    },
    {
      "source": "root",
      "target": "s3"
    },
    {
      "source": "root",
      "target": "organizations"
    },
    {
      "source": "root",
      "target": "cloudwatch"
    },
    {
      "source": "root",
      "target": "guardduty"
    },
    {
      "source": "root",
      "target": "lambda"
    },
    {
      "source": "root",
      "target": "vpc"
    },
    {
      "source": "security-hub",
      "target": "inspector"
    },
    {
      "source": "inspector",
      "target": "guardduty"
    },
    {
      "source": "inspector",
      "target": "lambda"
    },
    {
      "source": "security-hub",
      "target": "macie"
    },
    {
      "source": "macie",
      "target": "s3"
    },
    {
      "source": "security-hub",
      "target": "config"
    },
    {
      "source": "config",
      "target": "organizations"
    },
    {
      "source": "security-hub",
      "target": "eventbridge"
    },
    {
      "source": "eventbridge",
      "target": "guardduty"
    },
    {
      "source": "eventbridge",
      "target": "lambda"
    },
    {
      "source": "eventbridge",
      "target": "cloudwatch"
    },
    {
      "source": "security-hub",
      "target": "shield"
    },
    {
      "source": "shield",
      "target": "waf"
    },
    {
      "source": "shield",
      "target": "cloudfront"
    },
    {
      "source": "shield",
      "target": "route53"
    },
    {
      "source": "security-hub",
      "target": "trusted-advisor"
    },
    {
      "source": "guardduty",
      "target": "detective"
    },
    {
      "source": "detective",
      "target": "security-hub"
    },
    {
      "source": "detective",
      "target": "cloudtrail"
    },
    {
      "source": "cloudtrail",
      "target": "security-lake"
    },
    {
      "source": "security-lake",
      "target": "security-hub"
    },
    {
      "source": "security-lake",
      "target": "s3"
    },
    {
      "source": "cloudtrail",
      "target": "athena"
    },
    {
      "source": "athena",
      "target": "s3"
    },
    {
      "source": "athena",
      "target": "security-lake"
    },
    {
      "source": "cloudtrail",
      "target": "cloudtrail-lake"
    },
    {
      "source": "cloudwatch",
      "target": "opensearch"
    },
    {
      "source": "opensearch",
      "target": "lambda"
    },
    {
      "source": "cloudwatch",
      "target": "managed-grafana"
    },
    {
      "source": "managed-grafana",
      "target": "opensearch"
    },
    {
      "source": "cloudwatch",
      "target": "sns"
    },
    {
      "source": "sns",
      "target": "eventbridge"
    },
    {
      "source": "sns",
      "target": "kms"
    },
    {
      "source": "cloudwatch",
      "target": "user-notifications"
    },
    {
      "source": "lambda",
      "target": "step-functions"
    },
    {
      "source": "step-functions",
      "target": "eventbridge"
    },
    {
      "source": "lambda",
      "target": "systems-manager"
    },
    {
      "source": "systems-manager",
      "target": "config"
    },
    {
      "source": "systems-manager",
      "target": "inspector"
    },
    {
      "source": "lambda",
      "target": "api-gateway"
    },
    {
      "source": "api-gateway",
      "target": "cognito"
    },
    {
      "source": "api-gateway",
      "target": "waf"
    },
    {
      "source": "lambda",
      "target": "bedrock"
    },
    {
      "source": "lambda",
      "target": "sagemaker"
    },
    {
      "source": "sagemaker",
      "target": "kms"
    },
    {
      "source": "step-functions",
      "target": "forensics-orchestrator"
    },
    {
      "source": "forensics-orchestrator",
      "target": "lambda"
    },
    {
      "source": "step-functions",
      "target": "fis"
    },
    {
      "source": "step-functions",
      "target": "resilience-hub"
    },
    {
      "source": "step-functions",
      "target": "app-recovery-controller"
    },
    {
      "source": "vpc",
      "target": "waf"
    },
    {
      "source": "waf",
      "target": "cloudfront"
    },
    {
      "source": "waf",
      "target": "firewall-manager"
    },
    {
      "source": "vpc",
      "target": "cloudfront"
    },
    {
      "source": "cloudfront",
      "target": "s3"
    },
    {
      "source": "vpc",
      "target": "route53"
    },
    {
      "source": "vpc",
      "target": "elb"
    },
    {
      "source": "elb",
      "target": "waf"
    },
    {
      "source": "elb",
      "target": "s3"
    },
    {
      "source": "vpc",
      "target": "network-firewall"
    },
    {
      "source": "network-firewall",
      "target": "firewall-manager"
    },
    {
      "source": "vpc",
      "target": "direct-connect"
    },
    {
      "source": "direct-connect",
      "target": "vpn"
    },
    {
      "source": "vpc",
      "target": "vpn"
    },
    {
      "source": "vpc",
      "target": "transit-gateway"
    },
    {
      "source": "transit-gateway",
      "target": "network-firewall"
    },
    {
      "source": "vpc",
      "target": "verified-access"
    },
    {
      "source": "verified-access",
      "target": "identity-center"
    },
    {
      "source": "vpc",
      "target": "network-access-analyzer"
    },
    {
      "source": "vpc",
      "target": "ec2"
    },
    {
      "source": "ec2",
      "target": "inspector"
    },
    {
      "source": "ec2",
      "target": "systems-manager"
    },
    {
      "source": "vpc",
      "target": "iot-core"
    },
    {
      "source": "organizations",
      "target": "firewall-manager"
    },
    {
      "source": "firewall-manager",
      "target": "shield"
    },
    {
      "source": "organizations",
      "target": "control-tower"
    },
    {
      "source": "organizations",
      "target": "cloudformation"
    },
    {
      "source": "organizations",
      "target": "service-catalog"
    },
    {
      "source": "service-catalog",
      "target": "cloudformation"
    },
    {
      "source": "organizations",
      "target": "ram"
    },
    {
      "source": "organizations",
      "target": "audit-manager"
    },
    {
      "source": "audit-manager",
      "target": "config"
    },
    {
      "source": "audit-manager",
      "target": "cloudtrail"
    },
    {
      "source": "audit-manager",
      "target": "security-hub"
    },
    {
      "source": "organizations",
      "target": "artifact"
    },
    {
      "source": "organizations",
      "target": "well-architected"
    },
    {
      "source": "ec2",
      "target": "ec2-image-builder"
    },
    {
      "source": "ec2-image-builder",
      "target": "inspector"
    },
    {
      "source": "ec2",
      "target": "ecr"
    },
    {
      "source": "ecr",
      "target": "inspector"
    },
    {
      "source": "ecr",
      "target": "kms"
    },
    {
      "source": "ec2",
      "target": "dlm"
    },
    {
      "source": "dlm",
      "target": "backup"
    },
    {
      "source": "ec2",
      "target": "ebs"
    },
    {
      "source": "ebs",
      "target": "kms"
    },
    {
      "source": "ec2",
      "target": "q-developer"
    },
    {
      "source": "ec2",
      "target": "codeguru"
    },
    {
      "source": "codeguru",
      "target": "q-developer"
    },
    {
      "source": "ec2",
      "target": "eks"
    },
    {
      "source": "eks",
      "target": "guardduty"
    },
    {
      "source": "eks",
      "target": "ecr"
    },
    {
      "source": "ec2",
      "target": "emr"
    },
    {
      "source": "emr",
      "target": "kms"
    },
    {
      "source": "iam",
      "target": "identity-center"
    },
    {
      "source": "identity-center",
      "target": "organizations"
    },
    {
      "source": "iam",
      "target": "cognito"
    },
    {
      "source": "cognito",
      "target": "waf"
    },
    {
      "source": "iam",
      "target": "sts"
    },
    {
      "source": "sts",
      "target": "s3"
    },
    {
      "source": "iam",
      "target": "directory-service"
    },
    {
      "source": "directory-service",
      "target": "identity-center"
    },
    {
      "source": "iam",
      "target": "verified-permissions"
    },
    {
      "source": "verified-permissions",
      "target": "cognito"
    },
    {
      "source": "iam",
      "target": "iam-roles-anywhere"
    },
    {
      "source": "iam-roles-anywhere",
      "target": "private-ca"
    },
    {
      "source": "kms",
      "target": "cloudhsm"
    },
    {
      "source": "kms",
      "target": "acm"
    },
    {
      "source": "acm",
      "target": "cloudfront"
    },
    {
      "source": "acm",
      "target": "elb"
    },
    {
      "source": "kms",
      "target": "private-ca"
    },
    {
      "source": "kms",
      "target": "secrets-manager"
    },
    {
      "source": "secrets-manager",
      "target": "lambda"
    },
    {
      "source": "systems-manager",
      "target": "parameter-store"
    },
    {
      "source": "parameter-store",
      "target": "kms"
    },
    {
      "source": "s3",
      "target": "backup"
    },
    {
      "source": "backup",
      "target": "kms"
    },
    {
      "source": "s3",
      "target": "datasync"
    },
    {
      "source": "s3",
      "target": "efs"
    },
    {
      "source": "efs",
      "target": "kms"
    },
    {
      "source": "security-hub",
      "target": "guardduty"
    },
    {
      "source": "security-hub",
      "target": "cloudtrail"
    },
    {
      "source": "security-hub",
      "target": "cloudwatch"
    },
    {
      "source": "security-hub",
      "target": "organizations"
    },
    {
      "source": "cloudtrail",
      "target": "s3"
    },
    {
      "source": "cloudtrail",
      "target": "cloudwatch"
    },
    {
      "source": "cloudtrail",
      "target": "lambda"
    },
    {
      "source": "kms",
      "target": "s3"
    },
    {
      "source": "iam",
      "target": "organizations"
    },
    {
      "source": "iam",
      "target": "lambda"
    },
    {
      "source": "iam",
      "target": "vpc"
    },
    {
      "source": "vpc",
      "target": "cloudwatch"
    },
    {
      "source": "guardduty",
      "target": "cloudtrail"
    },
    {
      "source": "guardduty",
      "target": "vpc"
    },
    {
      "source": "lambda",
      "target": "cloudwatch"
    },
    {
      "source": "kms",
      "target": "iam"
    }
  ],
  "selectedNodeId": null,
  "editingNodeId": null,
  "notes": [
    {
      "id": "note-root",
      "nodeId": "root",
      "content": "<p><strong>AWS Certified Security - Specialty (SCS-C03)</strong><br><br><strong>6 Domains:</strong> Detection (16%) \u00b7 Incident Response (14%) \u00b7 Infrastructure Security (18%) \u00b7 Identity & Access Management (20%) \u00b7 Data Protection (18%) \u00b7 Security Foundations & Governance (14%)<br><br><strong>Exam:</strong> 50 scored + 15 unscored questions \u00b7 750/1000 to pass \u00b7 170 minutes<br><br><strong>Exam guide:</strong> https://docs.aws.amazon.com/aws-certification/latest/examguides/security-specialty-03.html<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udd34 AWS-Proprietary (High Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Catastrophic. A security architecture this deep in AWS is essentially a full platform rebuild.<br><strong>Control Plane Dependency:</strong> Total \u2014 this entire map IS the AWS control plane<br><strong>Data Gravity:</strong> Extreme \u2014 data lives across dozens of AWS-specific storage backends<br><strong>Runtime Coupling:</strong> Total \u2014 every automation, policy, and workflow is AWS API-native<br><br><strong>Why migration is hard:</strong> Security is the deepest integration layer in any cloud. IAM policies, SCPs, GuardDuty ML models, Security Hub aggregation, CloudTrail audit trails \u2014 none of this transfers. You're not migrating services, you're migrating an entire trust model.<br><strong>Dependency risks:</strong> Every service on this map depends on IAM, KMS, and CloudTrail. Removing any one hub collapses the security posture.<br><strong>Alternatives:</strong> Azure Security Center + Sentinel + Entra ID, or GCP Security Command Center + Chronicle + Cloud IAM. Either way: 12-24 month rebuild minimum.<br><strong>Blast radius if migrating:</strong> Total. Security architecture is the skeleton of the cloud deployment. Everything breaks.<br><br><strong>Honest take:</strong> This map is a lock-in map. AWS security services are designed to interlock. That's what makes them powerful \u2014 and what makes leaving catastrophically expensive. The question isn't whether you're locked in. You are. The question is whether the lock-in is worth the capability.</p>",
      "contentType": "tiptap",
      "plainText": "AWS Certified Security - Specialty (SCS-C03)\n\n6 Domains: Detection (16%) \u00b7 Incident Response (14%) \u00b7 Infrastructure Security (18%) \u00b7 Identity & Access Management (20%) \u00b7 Data Protection (18%) \u00b7 Security Foundations & Governance (14%)\n\nExam: 50 scored + 15 unscored questions \u00b7 750/1000 to pass \u00b7 170 minutes\n\nExam guide: https://docs.aws.amazon.com/aws-certification/latest/examguides/security-specialty-03.html\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udd34 AWS-Proprietary (High Lock-In)\n\nMigration Difficulty: Catastrophic. A security architecture this deep in AWS is essentially a full platform rebuild.\nControl Plane Dependency: Total \u2014 this entire map IS the AWS control plane\nData Gravity: Extreme \u2014 data lives across dozens of AWS-specific storage backends\nRuntime Coupling: Total \u2014 every automation, policy, and workflow is AWS API-native\n\nWhy migration is hard: Security is the deepest integration layer in any cloud. IAM policies, SCPs, GuardDuty ML models, Security Hub aggregation, CloudTrail audit trails \u2014 none of this transfers. You're not migrating services, you're migrating an entire trust model.\nDependency risks: Every service on this map depends on IAM, KMS, and CloudTrail. Removing any one hub collapses the security posture.\nAlternatives: Azure Security Center + Sentinel + Entra ID, or GCP Security Command Center + Chronicle + Cloud IAM. Either way: 12-24 month rebuild minimum.\nBlast radius if migrating: Total. Security architecture is the skeleton of the cloud deployment. Everything breaks.\n\nHonest take: This map is a lock-in map. AWS security services are designed to interlock. That's what makes them powerful \u2014 and what makes leaving catastrophically expensive. The question isn't whether you're locked in. You are. The question is whether the lock-in is worth the capability.",
      "tags": [
        "aws-proprietary"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-security-hub",
      "nodeId": "security-hub",
      "content": "<p><strong>Domains: 1, 2, 6</strong><br><br>Central aggregation point for security findings from GuardDuty, Inspector, Macie, Config, Firewall Manager, IAM Access Analyzer, and third-party tools. Maps findings to compliance frameworks (CIS, PCI DSS, NIST). Custom insights for trend analysis. Automated response via EventBridge rules. Cross-account aggregation with delegated administrator. Key for Domain 1 (monitoring/alerting) and Domain 6 (compliance evaluation).<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udd34 AWS-Proprietary (High Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Very High. Security Hub is the central nervous system of AWS security \u2014 everything reports to it and everything reacts from it.<br><strong>Control Plane Dependency:</strong> 5/5 \u2014 Entirely AWS-proprietary. ASFF (AWS Security Finding Format) is a custom schema. All integrations are AWS\u2192AWS.<br><strong>Data Gravity:</strong> 4/5 \u2014 Findings can be exported, but the aggregation logic, custom insights, compliance scoring, and cross-account federation cannot.<br><strong>Runtime Coupling:</strong> 5/5 \u2014 EventBridge rules, Lambda automations, and compliance dashboards all depend on Security Hub's specific finding structure.<br><br><strong>Why migration is hard:</strong> ASFF is AWS-only. Compliance standards (CIS AWS Benchmarks, AWS Foundational Security) are AWS-specific. Every automation you've built triggers from Security Hub findings. Moving means rebuilding your entire detection-to-response pipeline.<br><strong>Why migration could work:</strong> The concept of a SIEM/SOAR aggregator is universal. You could move to Splunk, Datadog, or Azure Sentinel \u2014 but you're rebuilding years of tuning.<br><strong>Dependency risks:</strong> GuardDuty, Inspector, Macie, Config, IAM Access Analyzer, Firewall Manager all feed into it. EventBridge and Lambda consume from it. Remove Security Hub and you lose centralized visibility.<br><strong>Alternatives:</strong> Azure Sentinel, GCP Security Command Center, Splunk SIEM, Datadog Cloud SIEM, Wiz, Orca<br><strong>Blast radius if migrating:</strong> Massive. Every security automation, compliance dashboard, and alert routing breaks.<br><br><strong>Honest take:</strong> Security Hub is AWS's gravitational center for security. Once you build automations around ASFF findings, you've written AWS-specific glue code that has zero portability. The aggregation concept is portable; the implementation is a prison.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 1, 2, 6\n\nCentral aggregation point for security findings from GuardDuty, Inspector, Macie, Config, Firewall Manager, IAM Access Analyzer, and third-party tools. Maps findings to compliance frameworks (CIS, PCI DSS, NIST). Custom insights for trend analysis. Automated response via EventBridge rules. Cross-account aggregation with delegated administrator. Key for Domain 1 (monitoring/alerting) and Domain 6 (compliance evaluation).\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udd34 AWS-Proprietary (High Lock-In)\n\nMigration Difficulty: Very High. Security Hub is the central nervous system of AWS security \u2014 everything reports to it and everything reacts from it.\nControl Plane Dependency: 5/5 \u2014 Entirely AWS-proprietary. ASFF (AWS Security Finding Format) is a custom schema. All integrations are AWS\u2192AWS.\nData Gravity: 4/5 \u2014 Findings can be exported, but the aggregation logic, custom insights, compliance scoring, and cross-account federation cannot.\nRuntime Coupling: 5/5 \u2014 EventBridge rules, Lambda automations, and compliance dashboards all depend on Security Hub's specific finding structure.\n\nWhy migration is hard: ASFF is AWS-only. Compliance standards (CIS AWS Benchmarks, AWS Foundational Security) are AWS-specific. Every automation you've built triggers from Security Hub findings. Moving means rebuilding your entire detection-to-response pipeline.\nWhy migration could work: The concept of a SIEM/SOAR aggregator is universal. You could move to Splunk, Datadog, or Azure Sentinel \u2014 but you're rebuilding years of tuning.\nDependency risks: GuardDuty, Inspector, Macie, Config, IAM Access Analyzer, Firewall Manager all feed into it. EventBridge and Lambda consume from it. Remove Security Hub and you lose centralized visibility.\nAlternatives: Azure Sentinel, GCP Security Command Center, Splunk SIEM, Datadog Cloud SIEM, Wiz, Orca\nBlast radius if migrating: Massive. Every security automation, compliance dashboard, and alert routing breaks.\n\nHonest take: Security Hub is AWS's gravitational center for security. Once you build automations around ASFF findings, you've written AWS-specific glue code that has zero portability. The aggregation concept is portable; the implementation is a prison.",
      "tags": [
        "aws-proprietary"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-cloudtrail",
      "nodeId": "cloudtrail",
      "content": "<p><strong>Domains: 1, 4, 6</strong><br><br>Records API calls across your AWS account \u2014 the definitive audit log. Organization trails for multi-account coverage. CloudTrail Lake enables SQL-based querying of events directly (new for C03). Integrates with CloudWatch Logs for real-time alerting, Athena for ad-hoc S3 log analysis, and Security Lake for OCSF normalization. CloudTrail Insights detects unusual API activity patterns automatically. Critical for incident investigation, authentication troubleshooting (Domain 4), and compliance evidence (Domain 6).<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udd34 AWS-Proprietary (High Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> High. The raw data is exportable, but every query, alert rule, and forensic playbook references AWS-specific API event names.<br><strong>Control Plane Dependency:</strong> 5/5 \u2014 AWS-proprietary API audit log. Every field, event structure, and query pattern is AWS-specific.<br><strong>Data Gravity:</strong> 3/5 \u2014 Logs are JSON in S3 (exportable), but CloudTrail Lake, Insights, and organization trails are AWS-only features.<br><strong>Runtime Coupling:</strong> 4/5 \u2014 Any automation that parses CloudTrail event names (e.g., 'StopInstances', 'PutBucketPolicy') is coupled to AWS API naming.<br><br><strong>Why migration is hard:</strong> CloudTrail events ARE the AWS API. Every detective control, incident response runbook, and compliance query you've written uses AWS-specific event names and structures. You can export the JSON, but it's meaningless outside AWS.<br><strong>Why migration could work:</strong> The concept of API audit logging exists everywhere (GCP Audit Logs, Azure Activity Log). If you abstract your SIEM queries, you could adapt.<br><strong>Dependency risks:</strong> Athena queries, CloudWatch metric filters, Security Lake ingestion, Detective analysis \u2014 all depend on CloudTrail's specific event format.<br><strong>Alternatives:</strong> GCP Cloud Audit Logs, Azure Activity Log + Monitor, self-hosted audit via OpenTelemetry (covers app-level only, not control plane)<br><strong>Blast radius if migrating:</strong> Very high. Audit logging is the foundation of compliance and forensics. Migrating means rewriting every query and alert.<br><br><strong>Honest take:</strong> CloudTrail data is technically portable (it's JSON in S3). But the value isn't the data \u2014 it's the years of queries, alerts, and playbooks built around AWS event names. That institutional knowledge doesn't transfer.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 1, 4, 6\n\nRecords API calls across your AWS account \u2014 the definitive audit log. Organization trails for multi-account coverage. CloudTrail Lake enables SQL-based querying of events directly (new for C03). Integrates with CloudWatch Logs for real-time alerting, Athena for ad-hoc S3 log analysis, and Security Lake for OCSF normalization. CloudTrail Insights detects unusual API activity patterns automatically. Critical for incident investigation, authentication troubleshooting (Domain 4), and compliance evidence (Domain 6).\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udd34 AWS-Proprietary (High Lock-In)\n\nMigration Difficulty: High. The raw data is exportable, but every query, alert rule, and forensic playbook references AWS-specific API event names.\nControl Plane Dependency: 5/5 \u2014 AWS-proprietary API audit log. Every field, event structure, and query pattern is AWS-specific.\nData Gravity: 3/5 \u2014 Logs are JSON in S3 (exportable), but CloudTrail Lake, Insights, and organization trails are AWS-only features.\nRuntime Coupling: 4/5 \u2014 Any automation that parses CloudTrail event names (e.g., 'StopInstances', 'PutBucketPolicy') is coupled to AWS API naming.\n\nWhy migration is hard: CloudTrail events ARE the AWS API. Every detective control, incident response runbook, and compliance query you've written uses AWS-specific event names and structures. You can export the JSON, but it's meaningless outside AWS.\nWhy migration could work: The concept of API audit logging exists everywhere (GCP Audit Logs, Azure Activity Log). If you abstract your SIEM queries, you could adapt.\nDependency risks: Athena queries, CloudWatch metric filters, Security Lake ingestion, Detective analysis \u2014 all depend on CloudTrail's specific event format.\nAlternatives: GCP Cloud Audit Logs, Azure Activity Log + Monitor, self-hosted audit via OpenTelemetry (covers app-level only, not control plane)\nBlast radius if migrating: Very high. Audit logging is the foundation of compliance and forensics. Migrating means rewriting every query and alert.\n\nHonest take: CloudTrail data is technically portable (it's JSON in S3). But the value isn't the data \u2014 it's the years of queries, alerts, and playbooks built around AWS event names. That institutional knowledge doesn't transfer.",
      "tags": [
        "aws-proprietary"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-iam",
      "nodeId": "iam",
      "content": "<p><strong>Domains: 4 (primary), 3, 6</strong><br><br>Foundation of all AWS access control. Users, groups, roles, policies. Exam focuses heavily on: policy evaluation logic (identity vs resource vs session vs SCP vs permissions boundary), least privilege design, ABAC with tags, RBAC patterns, IAM Roles Anywhere for on-prem workloads, and cross-account role assumption. Access Analyzer validates policies and finds unintended external access. Access Advisor shows last-accessed data. Policy Simulator tests before deployment. Domain 4 is 20% of the exam \u2014 IAM is the biggest topic.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udd34 AWS-Proprietary (High Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Extreme. IAM is not a service \u2014 it's the operating system of AWS. You cannot migrate IAM; you must rewrite your entire authorization model.<br><strong>Control Plane Dependency:</strong> 5/5 \u2014 THE lock-in service. Every AWS resource, API call, and automation depends on IAM. The policy language, evaluation logic, role chaining, and trust model are entirely AWS-proprietary.<br><strong>Data Gravity:</strong> 2/5 \u2014 Policies are JSON (exportable but useless elsewhere). No user data to migrate per se.<br><strong>Runtime Coupling:</strong> 5/5 \u2014 Application code, IaC templates, CI/CD pipelines, and every service configuration references IAM roles and policies.<br><br><strong>Why migration is hard:</strong> IAM policy JSON is AWS-specific (Principal, Action, Resource, Condition operators are all AWS vocabulary). Permission boundaries, session policies, SCPs, resource policies \u2014 these concepts don't map 1:1 to any other platform. ABAC tag-based policies would need complete redesign.<br><strong>Why migration could work:</strong> Conceptually, RBAC/ABAC exist everywhere. If you've documented your access model abstractly (who needs what), you can rebuild. But you haven't. Nobody has.<br><strong>Dependency risks:</strong> Literally everything. Every AWS service call evaluates IAM. Remove IAM and nothing works \u2014 not S3, not EC2, not Lambda, nothing.<br><strong>Alternatives:</strong> Azure Entra ID (formerly AAD) + Azure RBAC, GCP Cloud IAM, HashiCorp Vault + OPA (Open Policy Agent) for self-hosted<br><strong>Blast radius if migrating:</strong> Total. IAM is the gravity well of AWS. Every resource, policy, and trust relationship is defined here. Migration is a full-platform rewrite.<br><br><strong>Honest take:</strong> IAM is the reason people stay on AWS. Not because it's good (the policy language is painful), but because the cost of recreating your permission model elsewhere is astronomical. Every cross-account role, every service-linked role, every condition key \u2014 it's all bespoke AWS. This is lock-in by design, and it's the most effective lock-in in all of cloud computing.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 4 (primary), 3, 6\n\nFoundation of all AWS access control. Users, groups, roles, policies. Exam focuses heavily on: policy evaluation logic (identity vs resource vs session vs SCP vs permissions boundary), least privilege design, ABAC with tags, RBAC patterns, IAM Roles Anywhere for on-prem workloads, and cross-account role assumption. Access Analyzer validates policies and finds unintended external access. Access Advisor shows last-accessed data. Policy Simulator tests before deployment. Domain 4 is 20% of the exam \u2014 IAM is the biggest topic.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udd34 AWS-Proprietary (High Lock-In)\n\nMigration Difficulty: Extreme. IAM is not a service \u2014 it's the operating system of AWS. You cannot migrate IAM; you must rewrite your entire authorization model.\nControl Plane Dependency: 5/5 \u2014 THE lock-in service. Every AWS resource, API call, and automation depends on IAM. The policy language, evaluation logic, role chaining, and trust model are entirely AWS-proprietary.\nData Gravity: 2/5 \u2014 Policies are JSON (exportable but useless elsewhere). No user data to migrate per se.\nRuntime Coupling: 5/5 \u2014 Application code, IaC templates, CI/CD pipelines, and every service configuration references IAM roles and policies.\n\nWhy migration is hard: IAM policy JSON is AWS-specific (Principal, Action, Resource, Condition operators are all AWS vocabulary). Permission boundaries, session policies, SCPs, resource policies \u2014 these concepts don't map 1:1 to any other platform. ABAC tag-based policies would need complete redesign.\nWhy migration could work: Conceptually, RBAC/ABAC exist everywhere. If you've documented your access model abstractly (who needs what), you can rebuild. But you haven't. Nobody has.\nDependency risks: Literally everything. Every AWS service call evaluates IAM. Remove IAM and nothing works \u2014 not S3, not EC2, not Lambda, nothing.\nAlternatives: Azure Entra ID (formerly AAD) + Azure RBAC, GCP Cloud IAM, HashiCorp Vault + OPA (Open Policy Agent) for self-hosted\nBlast radius if migrating: Total. IAM is the gravity well of AWS. Every resource, policy, and trust relationship is defined here. Migration is a full-platform rewrite.\n\nHonest take: IAM is the reason people stay on AWS. Not because it's good (the policy language is painful), but because the cost of recreating your permission model elsewhere is astronomical. Every cross-account role, every service-linked role, every condition key \u2014 it's all bespoke AWS. This is lock-in by design, and it's the most effective lock-in in all of cloud computing.",
      "tags": [
        "aws-proprietary"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-kms",
      "nodeId": "kms",
      "content": "<p><strong>Domains: 5 (primary), 1, 3</strong><br><br>Managed encryption key service. Symmetric (AES-256) and asymmetric keys. Key policies + IAM policies + grants control access. Automatic annual rotation for customer managed keys. Envelope encryption pattern is heavily tested. Multi-Region keys for cross-region encrypt/decrypt. Imported key material vs AWS-generated (new C03 topic). External key stores (XKS) for keys outside AWS. Integrates with virtually every storage/database service. Know: key policy default, key deletion waiting period (7-30 days), grants vs policies, CMK types.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udd34 AWS-Proprietary (High Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Extreme. KMS is the lock on the lock. Every encrypted byte in S3, EBS, RDS, DynamoDB, SQS, Kinesis, etc. is encrypted with KMS keys that cannot be extracted.<br><strong>Control Plane Dependency:</strong> 5/5 \u2014 Key policies, grants, and the key hierarchy are AWS-proprietary. Every encrypted resource references KMS key ARNs.<br><strong>Data Gravity:</strong> 5/5 \u2014 You CANNOT export AWS-generated key material. Period. Your data is encrypted with keys that physically cannot leave AWS.<br><strong>Runtime Coupling:</strong> 5/5 \u2014 Every Encrypt/Decrypt/GenerateDataKey API call is AWS KMS-specific. Envelope encryption wrappers in your code call KMS directly.<br><br><strong>Why migration is hard:</strong> AWS-managed and customer-managed keys cannot be exported. To migrate encrypted data, you must: decrypt in AWS, transfer plaintext (security risk), re-encrypt with new provider's keys. For petabytes of data, this is a months-long project with massive security exposure. External Key Stores (XKS) and imported key material offer some portability, but <5% of customers use them.<br><strong>Why migration could work:</strong> If you designed with imported key material or XKS from day one, you have key portability. HSM-backed keys (CloudHSM) are extractable. But if you used default KMS (like 99% of people), you're stuck.<br><strong>Dependency risks:</strong> S3, EBS, RDS, DynamoDB, EFS, SQS, Kinesis, Secrets Manager, Parameter Store, Redshift, Lambda env vars \u2014 everything encrypted touches KMS.<br><strong>Alternatives:</strong> Azure Key Vault, GCP Cloud KMS, HashiCorp Vault, self-hosted PKCS#11 HSMs. None accept AWS KMS key material.<br><strong>Blast radius if migrating:</strong> Catastrophic. KMS is woven into every data service. Migration requires decrypting and re-encrypting all data \u2014 the single largest migration workstream.<br><br><strong>Honest take:</strong> KMS is the quietest, deepest lock-in in AWS. It doesn't look scary \u2014 'just encryption.' But the inability to export key material means every encrypted byte is an anchor. If you're designing for portability, use imported key material or CloudHSM custom key stores from day one. If you didn't, your data migration just became a decryption marathon.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 5 (primary), 1, 3\n\nManaged encryption key service. Symmetric (AES-256) and asymmetric keys. Key policies + IAM policies + grants control access. Automatic annual rotation for customer managed keys. Envelope encryption pattern is heavily tested. Multi-Region keys for cross-region encrypt/decrypt. Imported key material vs AWS-generated (new C03 topic). External key stores (XKS) for keys outside AWS. Integrates with virtually every storage/database service. Know: key policy default, key deletion waiting period (7-30 days), grants vs policies, CMK types.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udd34 AWS-Proprietary (High Lock-In)\n\nMigration Difficulty: Extreme. KMS is the lock on the lock. Every encrypted byte in S3, EBS, RDS, DynamoDB, SQS, Kinesis, etc. is encrypted with KMS keys that cannot be extracted.\nControl Plane Dependency: 5/5 \u2014 Key policies, grants, and the key hierarchy are AWS-proprietary. Every encrypted resource references KMS key ARNs.\nData Gravity: 5/5 \u2014 You CANNOT export AWS-generated key material. Period. Your data is encrypted with keys that physically cannot leave AWS.\nRuntime Coupling: 5/5 \u2014 Every Encrypt/Decrypt/GenerateDataKey API call is AWS KMS-specific. Envelope encryption wrappers in your code call KMS directly.\n\nWhy migration is hard: AWS-managed and customer-managed keys cannot be exported. To migrate encrypted data, you must: decrypt in AWS, transfer plaintext (security risk), re-encrypt with new provider's keys. For petabytes of data, this is a months-long project with massive security exposure. External Key Stores (XKS) and imported key material offer some portability, but <5% of customers use them.\nWhy migration could work: If you designed with imported key material or XKS from day one, you have key portability. HSM-backed keys (CloudHSM) are extractable. But if you used default KMS (like 99% of people), you're stuck.\nDependency risks: S3, EBS, RDS, DynamoDB, EFS, SQS, Kinesis, Secrets Manager, Parameter Store, Redshift, Lambda env vars \u2014 everything encrypted touches KMS.\nAlternatives: Azure Key Vault, GCP Cloud KMS, HashiCorp Vault, self-hosted PKCS#11 HSMs. None accept AWS KMS key material.\nBlast radius if migrating: Catastrophic. KMS is woven into every data service. Migration requires decrypting and re-encrypting all data \u2014 the single largest migration workstream.\n\nHonest take: KMS is the quietest, deepest lock-in in AWS. It doesn't look scary \u2014 'just encryption.' But the inability to export key material means every encrypted byte is an anchor. If you're designing for portability, use imported key material or CloudHSM custom key stores from day one. If you didn't, your data migration just became a decryption marathon.",
      "tags": [
        "aws-proprietary"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-s3",
      "nodeId": "s3",
      "content": "<p><strong>Domains: 5, 1, 2, 3</strong><br><br>Appears everywhere on this exam. Bucket policies, ACLs (legacy), Block Public Access (account + bucket level), Object Lock (Governance vs Compliance mode + legal hold), SSE-S3/SSE-KMS/SSE-C/DSSE-KMS encryption. Presigned URLs (Domain 4 \u2014 temporary access). CORS configuration (Domain 3 edge security). Access Logs and S3 event notifications. Lifecycle policies for tiering/expiration. Cross-region and same-region replication. Glacier Vault Lock for archival compliance. Macie scans S3 for sensitive data. Central log destination for CloudTrail, VPC Flow Logs, ELB, CloudFront.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udfe1 Cloud-Portable (Medium Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Medium. The data and API are portable. The governance, encryption, and event-driven integrations are not.<br><strong>Control Plane Dependency:</strong> 3/5 \u2014 The S3 API is a de facto industry standard. MinIO, GCS, Azure Blob all support S3-compatible APIs.<br><strong>Data Gravity:</strong> 4/5 \u2014 Data is exportable but potentially massive. Multi-petabyte migrations take months. Lifecycle rules, Object Lock configs, and event notifications don't transfer.<br><strong>Runtime Coupling:</strong> 3/5 \u2014 S3 API calls in application code are portable to S3-compatible backends. But bucket policies, presigned URL logic, and event-driven architectures (S3\u2192Lambda\u2192EventBridge) are AWS-coupled.<br><br><strong>Why migration is hard:</strong> Bucket policies are IAM policy language (AWS-specific). KMS encryption means decrypt-and-re-encrypt. Object Lock retention policies must be manually recreated. S3 event notifications trigger Lambda functions that are AWS-specific. Cross-region replication configurations are AWS-only. Access Logs format is AWS-specific.<br><strong>Why migration could work:</strong> S3 API is the industry standard. MinIO is a drop-in replacement. GCS has S3 interoperability. Azure Blob has S3-compatible gateway. Your GET/PUT/DELETE operations will mostly just work elsewhere.<br><strong>Dependency risks:</strong> CloudTrail logs to S3, VPC Flow Logs to S3, ALB/CloudFront logs to S3, Athena queries S3, Security Lake stores in S3. S3 is the default sink for everything.<br><strong>Alternatives:</strong> GCS (S3-compatible), Azure Blob Storage, MinIO (self-hosted, S3 API), Cloudflare R2 (S3-compatible, no egress fees)<br><strong>Blast radius if migrating:</strong> High due to data volume. Low for API compatibility. The data moves; the governance doesn't.<br><br><strong>Honest take:</strong> S3 is AWS's most portable service, ironically. The API is everywhere. But AWS has built an entire ecosystem of S3 integrations (Macie scans it, CloudTrail logs to it, Athena queries it, Lambda triggers from it) that create indirect lock-in. S3 itself is portable; the S3 ecosystem is not.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 5, 1, 2, 3\n\nAppears everywhere on this exam. Bucket policies, ACLs (legacy), Block Public Access (account + bucket level), Object Lock (Governance vs Compliance mode + legal hold), SSE-S3/SSE-KMS/SSE-C/DSSE-KMS encryption. Presigned URLs (Domain 4 \u2014 temporary access). CORS configuration (Domain 3 edge security). Access Logs and S3 event notifications. Lifecycle policies for tiering/expiration. Cross-region and same-region replication. Glacier Vault Lock for archival compliance. Macie scans S3 for sensitive data. Central log destination for CloudTrail, VPC Flow Logs, ELB, CloudFront.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udfe1 Cloud-Portable (Medium Lock-In)\n\nMigration Difficulty: Medium. The data and API are portable. The governance, encryption, and event-driven integrations are not.\nControl Plane Dependency: 3/5 \u2014 The S3 API is a de facto industry standard. MinIO, GCS, Azure Blob all support S3-compatible APIs.\nData Gravity: 4/5 \u2014 Data is exportable but potentially massive. Multi-petabyte migrations take months. Lifecycle rules, Object Lock configs, and event notifications don't transfer.\nRuntime Coupling: 3/5 \u2014 S3 API calls in application code are portable to S3-compatible backends. But bucket policies, presigned URL logic, and event-driven architectures (S3\u2192Lambda\u2192EventBridge) are AWS-coupled.\n\nWhy migration is hard: Bucket policies are IAM policy language (AWS-specific). KMS encryption means decrypt-and-re-encrypt. Object Lock retention policies must be manually recreated. S3 event notifications trigger Lambda functions that are AWS-specific. Cross-region replication configurations are AWS-only. Access Logs format is AWS-specific.\nWhy migration could work: S3 API is the industry standard. MinIO is a drop-in replacement. GCS has S3 interoperability. Azure Blob has S3-compatible gateway. Your GET/PUT/DELETE operations will mostly just work elsewhere.\nDependency risks: CloudTrail logs to S3, VPC Flow Logs to S3, ALB/CloudFront logs to S3, Athena queries S3, Security Lake stores in S3. S3 is the default sink for everything.\nAlternatives: GCS (S3-compatible), Azure Blob Storage, MinIO (self-hosted, S3 API), Cloudflare R2 (S3-compatible, no egress fees)\nBlast radius if migrating: High due to data volume. Low for API compatibility. The data moves; the governance doesn't.\n\nHonest take: S3 is AWS's most portable service, ironically. The API is everywhere. But AWS has built an entire ecosystem of S3 integrations (Macie scans it, CloudTrail logs to it, Athena queries it, Lambda triggers from it) that create indirect lock-in. S3 itself is portable; the S3 ecosystem is not.",
      "tags": [
        "cloud-portable"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-organizations",
      "nodeId": "organizations",
      "content": "<p><strong>Domains: 6 (primary), 4</strong><br><br>Multi-account management. OUs for structural governance. SCPs restrict maximum permissions across accounts (new for C03: also RCPs \u2014 Resource Control Policies, AI service opt-out policies, declarative policies). Delegated administrator for security services (Security Hub, GuardDuty, Config, Macie, Inspector). Root account credential management \u2014 centralized root access for member accounts, break-glass procedures. Integration with Control Tower, RAM, CloudFormation StackSets, Firewall Manager.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udd34 AWS-Proprietary (High Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Very High. Organizations isn't a service you migrate \u2014 it's the account structure that everything else is built on top of.<br><strong>Control Plane Dependency:</strong> 5/5 \u2014 Entirely AWS-proprietary. OUs, SCPs, RCPs, delegated admin, account factory \u2014 no portable equivalent.<br><strong>Data Gravity:</strong> 1/5 \u2014 No significant data stored. The 'data' is your organizational structure and policies, which are configuration, not data.<br><strong>Runtime Coupling:</strong> 5/5 \u2014 Every multi-account security pattern (centralized logging, delegated SecurityHub, cross-account roles) is built on Organizations.<br><br><strong>Why migration is hard:</strong> SCPs, RCPs, AI service opt-out policies, tag policies, and backup policies are all AWS-only governance mechanisms. Control Tower guardrails, Account Factory, and the delegated administrator pattern have no portable equivalent. Your entire multi-account strategy is an AWS construct.<br><strong>Why migration could work:</strong> The concept of multi-account/multi-subscription governance exists in Azure (Management Groups) and GCP (Organization/Folders). You'd rebuild, not migrate.<br><strong>Dependency risks:</strong> Every multi-account security pattern: Security Hub aggregation, GuardDuty multi-account, Config aggregation, CloudTrail org trails, Firewall Manager, RAM sharing.<br><strong>Alternatives:</strong> Azure Management Groups + Azure Policy, GCP Organization + Folders + Organization Policies, Terraform Cloud for multi-account IaC (partial)<br><strong>Blast radius if migrating:</strong> Total for multi-account architectures. Organizations is the skeleton. Remove it and every cross-account pattern collapses.<br><br><strong>Honest take:</strong> Organizations is invisible lock-in. Nobody thinks of it as a 'service' \u2014 it's just how you organize accounts. But your entire governance model \u2014 SCPs, delegated admin, centralized logging \u2014 is built on it. It's the foundation you can't lift.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 6 (primary), 4\n\nMulti-account management. OUs for structural governance. SCPs restrict maximum permissions across accounts (new for C03: also RCPs \u2014 Resource Control Policies, AI service opt-out policies, declarative policies). Delegated administrator for security services (Security Hub, GuardDuty, Config, Macie, Inspector). Root account credential management \u2014 centralized root access for member accounts, break-glass procedures. Integration with Control Tower, RAM, CloudFormation StackSets, Firewall Manager.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udd34 AWS-Proprietary (High Lock-In)\n\nMigration Difficulty: Very High. Organizations isn't a service you migrate \u2014 it's the account structure that everything else is built on top of.\nControl Plane Dependency: 5/5 \u2014 Entirely AWS-proprietary. OUs, SCPs, RCPs, delegated admin, account factory \u2014 no portable equivalent.\nData Gravity: 1/5 \u2014 No significant data stored. The 'data' is your organizational structure and policies, which are configuration, not data.\nRuntime Coupling: 5/5 \u2014 Every multi-account security pattern (centralized logging, delegated SecurityHub, cross-account roles) is built on Organizations.\n\nWhy migration is hard: SCPs, RCPs, AI service opt-out policies, tag policies, and backup policies are all AWS-only governance mechanisms. Control Tower guardrails, Account Factory, and the delegated administrator pattern have no portable equivalent. Your entire multi-account strategy is an AWS construct.\nWhy migration could work: The concept of multi-account/multi-subscription governance exists in Azure (Management Groups) and GCP (Organization/Folders). You'd rebuild, not migrate.\nDependency risks: Every multi-account security pattern: Security Hub aggregation, GuardDuty multi-account, Config aggregation, CloudTrail org trails, Firewall Manager, RAM sharing.\nAlternatives: Azure Management Groups + Azure Policy, GCP Organization + Folders + Organization Policies, Terraform Cloud for multi-account IaC (partial)\nBlast radius if migrating: Total for multi-account architectures. Organizations is the skeleton. Remove it and every cross-account pattern collapses.\n\nHonest take: Organizations is invisible lock-in. Nobody thinks of it as a 'service' \u2014 it's just how you organize accounts. But your entire governance model \u2014 SCPs, delegated admin, centralized logging \u2014 is built on it. It's the foundation you can't lift.",
      "tags": [
        "aws-proprietary"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-cloudwatch",
      "nodeId": "cloudwatch",
      "content": "<p><strong>Domains: 1, 3</strong><br><br>Metrics, alarms, dashboards, and logs. CloudWatch Logs agent for custom log ingestion. Metric filters create custom metrics from log patterns (e.g., failed SSH attempts). CloudWatch Logs Insights for interactive SQL-like queries. Logs data protection policies mask sensitive data (new C03 topic \u2014 PII/PHI masking in logs). Cross-account log aggregation with dedicated logging account. Anomaly detection on metrics. Composite alarms. Integration with SNS for alerts, EventBridge for automation.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udfe1 Cloud-Portable (Medium Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Medium-High. The observability concepts are universal, but years of dashboards, alarms, and queries need manual recreation.<br><strong>Control Plane Dependency:</strong> 4/5 \u2014 Metrics, alarms, dashboards, and log groups are all AWS-proprietary APIs. Metric filter syntax is AWS-specific.<br><strong>Data Gravity:</strong> 3/5 \u2014 Metrics and logs can be exported. But CloudWatch Logs Insights queries, composite alarms, and anomaly detection configs don't transfer.<br><strong>Runtime Coupling:</strong> 4/5 \u2014 Applications using CloudWatch agent, embedded metrics format, or Logs API are coupled. But the concepts are universal.<br><br><strong>Why migration is hard:</strong> CloudWatch Logs Insights queries, metric math expressions, composite alarm trees, anomaly detection baselines, and dashboard JSON are all AWS-specific. Log subscription filters and metric filters feed Lambda/EventBridge automations that are also AWS-specific.<br><strong>Why migration could work:</strong> Prometheus + Grafana is a mature, portable observability stack. Datadog, Splunk, and New Relic work across clouds. If you've been shipping to a third-party observability platform all along, CloudWatch is a thin layer you can drop.<br><strong>Dependency risks:</strong> Every AWS service emits CloudWatch metrics natively. Lambda logs to CloudWatch. VPC Flow Logs to CloudWatch. SSM, Config, and dozens more feed CloudWatch.<br><strong>Alternatives:</strong> Prometheus + Grafana (self-hosted, portable), Datadog, Splunk, New Relic, Elastic Observability, GCP Cloud Monitoring, Azure Monitor<br><strong>Blast radius if migrating:</strong> Medium. Dashboards and alarms need recreation, but if you use a third-party observability tool, CloudWatch is replaceable.<br><br><strong>Honest take:</strong> CloudWatch is moderately portable because the observability market is fiercely competitive. Prometheus+Grafana does most of what CloudWatch does. The real lock-in isn't CloudWatch itself \u2014 it's the native integration where every AWS service auto-emits CloudWatch metrics. That zero-config telemetry is impossible to replicate outside AWS.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 1, 3\n\nMetrics, alarms, dashboards, and logs. CloudWatch Logs agent for custom log ingestion. Metric filters create custom metrics from log patterns (e.g., failed SSH attempts). CloudWatch Logs Insights for interactive SQL-like queries. Logs data protection policies mask sensitive data (new C03 topic \u2014 PII/PHI masking in logs). Cross-account log aggregation with dedicated logging account. Anomaly detection on metrics. Composite alarms. Integration with SNS for alerts, EventBridge for automation.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udfe1 Cloud-Portable (Medium Lock-In)\n\nMigration Difficulty: Medium-High. The observability concepts are universal, but years of dashboards, alarms, and queries need manual recreation.\nControl Plane Dependency: 4/5 \u2014 Metrics, alarms, dashboards, and log groups are all AWS-proprietary APIs. Metric filter syntax is AWS-specific.\nData Gravity: 3/5 \u2014 Metrics and logs can be exported. But CloudWatch Logs Insights queries, composite alarms, and anomaly detection configs don't transfer.\nRuntime Coupling: 4/5 \u2014 Applications using CloudWatch agent, embedded metrics format, or Logs API are coupled. But the concepts are universal.\n\nWhy migration is hard: CloudWatch Logs Insights queries, metric math expressions, composite alarm trees, anomaly detection baselines, and dashboard JSON are all AWS-specific. Log subscription filters and metric filters feed Lambda/EventBridge automations that are also AWS-specific.\nWhy migration could work: Prometheus + Grafana is a mature, portable observability stack. Datadog, Splunk, and New Relic work across clouds. If you've been shipping to a third-party observability platform all along, CloudWatch is a thin layer you can drop.\nDependency risks: Every AWS service emits CloudWatch metrics natively. Lambda logs to CloudWatch. VPC Flow Logs to CloudWatch. SSM, Config, and dozens more feed CloudWatch.\nAlternatives: Prometheus + Grafana (self-hosted, portable), Datadog, Splunk, New Relic, Elastic Observability, GCP Cloud Monitoring, Azure Monitor\nBlast radius if migrating: Medium. Dashboards and alarms need recreation, but if you use a third-party observability tool, CloudWatch is replaceable.\n\nHonest take: CloudWatch is moderately portable because the observability market is fiercely competitive. Prometheus+Grafana does most of what CloudWatch does. The real lock-in isn't CloudWatch itself \u2014 it's the native integration where every AWS service auto-emits CloudWatch metrics. That zero-config telemetry is impossible to replicate outside AWS.",
      "tags": [
        "cloud-portable"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-guardduty",
      "nodeId": "guardduty",
      "content": "<p><strong>Domains: 1, 3</strong><br><br>ML-powered threat detection. Analyzes CloudTrail management/data events, VPC Flow Logs, DNS logs, S3 data events, EKS audit logs, Lambda network activity, and RDS login activity. Finding types: Recon, UnauthorizedAccess, Trojan, CryptoCurrency, Impact, etc. Multi-account management via delegated admin. Findings flow to Security Hub and EventBridge. Suppression rules filter expected findings. Trusted/threat IP lists. Runtime monitoring for EC2 and EKS containers (Domain 3 \u2014 compute security). Malware Protection scans EBS volumes attached to EC2.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udd34 AWS-Proprietary (High Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> High. The concept (ML-based threat detection) is portable, but the implementation is a black box you can't take with you.<br><strong>Control Plane Dependency:</strong> 5/5 \u2014 Fully managed, opaque ML threat detection. You cannot export models, tuning, or detection logic.<br><strong>Data Gravity:</strong> 3/5 \u2014 Findings are exportable (via EventBridge/S3). But the ML models, baselines, and behavioral profiles are trapped in AWS.<br><strong>Runtime Coupling:</strong> 4/5 \u2014 Your incident response automation triggers from GuardDuty finding types. Those types are AWS-specific (e.g., 'UnauthorizedAccess:EC2/RDPBruteForce').<br><br><strong>Why migration is hard:</strong> GuardDuty's ML models are trained on YOUR account's behavior patterns over months/years. That baseline doesn't transfer. Finding types are AWS-specific. Suppression rules, trusted IP lists, and threat intelligence are config you'd rebuild from scratch. Every EventBridge rule that reacts to 'aws.guardduty' events breaks.<br><strong>Why migration could work:</strong> Third-party CSPM/CDR tools (Wiz, Orca, Lacework, CrowdStrike) are multi-cloud and increasingly match GuardDuty's detection capability.<br><strong>Dependency risks:</strong> Detective consumes GuardDuty findings. Security Hub aggregates them. EventBridge routes them. Lambda remediates from them. The entire threat-detection-to-response pipeline starts here.<br><strong>Alternatives:</strong> Azure Defender for Cloud, GCP Security Command Center + Event Threat Detection, CrowdStrike Falcon Cloud, Wiz, Lacework, Orca Security<br><strong>Blast radius if migrating:</strong> High. Detection pipeline breaks. Months of ML baseline learning lost. Incident response automations need complete rewrite.<br><br><strong>Honest take:</strong> GuardDuty is a black box you rent. You don't own the models, the baselines, or the detection logic. When you leave, you lose it all and start from zero on another platform. The multi-cloud CSPM market (Wiz, Orca) exists specifically because people realized this.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 1, 3\n\nML-powered threat detection. Analyzes CloudTrail management/data events, VPC Flow Logs, DNS logs, S3 data events, EKS audit logs, Lambda network activity, and RDS login activity. Finding types: Recon, UnauthorizedAccess, Trojan, CryptoCurrency, Impact, etc. Multi-account management via delegated admin. Findings flow to Security Hub and EventBridge. Suppression rules filter expected findings. Trusted/threat IP lists. Runtime monitoring for EC2 and EKS containers (Domain 3 \u2014 compute security). Malware Protection scans EBS volumes attached to EC2.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udd34 AWS-Proprietary (High Lock-In)\n\nMigration Difficulty: High. The concept (ML-based threat detection) is portable, but the implementation is a black box you can't take with you.\nControl Plane Dependency: 5/5 \u2014 Fully managed, opaque ML threat detection. You cannot export models, tuning, or detection logic.\nData Gravity: 3/5 \u2014 Findings are exportable (via EventBridge/S3). But the ML models, baselines, and behavioral profiles are trapped in AWS.\nRuntime Coupling: 4/5 \u2014 Your incident response automation triggers from GuardDuty finding types. Those types are AWS-specific (e.g., 'UnauthorizedAccess:EC2/RDPBruteForce').\n\nWhy migration is hard: GuardDuty's ML models are trained on YOUR account's behavior patterns over months/years. That baseline doesn't transfer. Finding types are AWS-specific. Suppression rules, trusted IP lists, and threat intelligence are config you'd rebuild from scratch. Every EventBridge rule that reacts to 'aws.guardduty' events breaks.\nWhy migration could work: Third-party CSPM/CDR tools (Wiz, Orca, Lacework, CrowdStrike) are multi-cloud and increasingly match GuardDuty's detection capability.\nDependency risks: Detective consumes GuardDuty findings. Security Hub aggregates them. EventBridge routes them. Lambda remediates from them. The entire threat-detection-to-response pipeline starts here.\nAlternatives: Azure Defender for Cloud, GCP Security Command Center + Event Threat Detection, CrowdStrike Falcon Cloud, Wiz, Lacework, Orca Security\nBlast radius if migrating: High. Detection pipeline breaks. Months of ML baseline learning lost. Incident response automations need complete rewrite.\n\nHonest take: GuardDuty is a black box you rent. You don't own the models, the baselines, or the detection logic. When you leave, you lose it all and start from zero on another platform. The multi-cloud CSPM market (Wiz, Orca) exists specifically because people realized this.",
      "tags": [
        "aws-proprietary"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-lambda",
      "nodeId": "lambda",
      "content": "<p><strong>Domains: 1, 2, 3, 5</strong><br><br>The glue of security automation. Executes remediation in response to EventBridge/Security Hub events (isolate EC2, revoke credentials, update NACLs). Processes and enriches log data. Rotates secrets in Secrets Manager. Domain 3: execution roles, VPC configuration, code signing, reserved concurrency, function URLs. Inspector scans Lambda functions for vulnerabilities. GuardDuty monitors Lambda network activity. Domain 1: log normalization and parsing alongside OpenSearch. Domain 5: data protection in transit.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udfe1 Cloud-Portable (Medium Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Medium. The code logic is portable; the event plumbing and deployment model are not.<br><strong>Control Plane Dependency:</strong> 4/5 \u2014 Handler signatures, layer management, event source mappings, and permissions model are AWS-specific.<br><strong>Data Gravity:</strong> 1/5 \u2014 Code is yours. No significant data stored in Lambda itself.<br><strong>Runtime Coupling:</strong> 4/5 \u2014 Event payloads (S3 events, API Gateway proxy events, EventBridge events) are AWS-specific JSON structures. Middleware and libraries (powertools) are AWS-tuned.<br><br><strong>Why migration is hard:</strong> Event source mappings (SQS\u2192Lambda, S3\u2192Lambda, DynamoDB Streams\u2192Lambda) are AWS-specific integration patterns. Your Lambda code likely imports boto3 and calls AWS APIs directly. Lambda Layers, extensions, provisioned concurrency, and function URLs are AWS-only features. Step Functions orchestration doesn't transfer.<br><strong>Why migration could work:</strong> The actual business logic inside Lambda functions is just code \u2014 Python, Node.js, Go, etc. Refactor to containers and you're portable. Azure Functions and GCP Cloud Functions have similar concepts. Knative/OpenFaaS for self-hosted.<br><strong>Dependency risks:</strong> Every automated security response uses Lambda. Secrets Manager rotation, Config remediation, EventBridge rules, CloudFormation custom resources \u2014 Lambda is the glue.<br><strong>Alternatives:</strong> Azure Functions, GCP Cloud Functions, Knative (Kubernetes-native), OpenFaaS (self-hosted), containers on any platform<br><strong>Blast radius if migrating:</strong> High in volume (likely hundreds of functions), but medium in difficulty per function. The hard part is the event wiring, not the code.<br><br><strong>Honest take:</strong> Lambda's lock-in is sneaky. The function code is portable. What's NOT portable is the event-driven architecture around it \u2014 the triggers, permissions, environment variables referencing ARNs, and the assumption that every security automation is a Lambda function. Containerize your logic from day one if you care about portability.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 1, 2, 3, 5\n\nThe glue of security automation. Executes remediation in response to EventBridge/Security Hub events (isolate EC2, revoke credentials, update NACLs). Processes and enriches log data. Rotates secrets in Secrets Manager. Domain 3: execution roles, VPC configuration, code signing, reserved concurrency, function URLs. Inspector scans Lambda functions for vulnerabilities. GuardDuty monitors Lambda network activity. Domain 1: log normalization and parsing alongside OpenSearch. Domain 5: data protection in transit.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udfe1 Cloud-Portable (Medium Lock-In)\n\nMigration Difficulty: Medium. The code logic is portable; the event plumbing and deployment model are not.\nControl Plane Dependency: 4/5 \u2014 Handler signatures, layer management, event source mappings, and permissions model are AWS-specific.\nData Gravity: 1/5 \u2014 Code is yours. No significant data stored in Lambda itself.\nRuntime Coupling: 4/5 \u2014 Event payloads (S3 events, API Gateway proxy events, EventBridge events) are AWS-specific JSON structures. Middleware and libraries (powertools) are AWS-tuned.\n\nWhy migration is hard: Event source mappings (SQS\u2192Lambda, S3\u2192Lambda, DynamoDB Streams\u2192Lambda) are AWS-specific integration patterns. Your Lambda code likely imports boto3 and calls AWS APIs directly. Lambda Layers, extensions, provisioned concurrency, and function URLs are AWS-only features. Step Functions orchestration doesn't transfer.\nWhy migration could work: The actual business logic inside Lambda functions is just code \u2014 Python, Node.js, Go, etc. Refactor to containers and you're portable. Azure Functions and GCP Cloud Functions have similar concepts. Knative/OpenFaaS for self-hosted.\nDependency risks: Every automated security response uses Lambda. Secrets Manager rotation, Config remediation, EventBridge rules, CloudFormation custom resources \u2014 Lambda is the glue.\nAlternatives: Azure Functions, GCP Cloud Functions, Knative (Kubernetes-native), OpenFaaS (self-hosted), containers on any platform\nBlast radius if migrating: High in volume (likely hundreds of functions), but medium in difficulty per function. The hard part is the event wiring, not the code.\n\nHonest take: Lambda's lock-in is sneaky. The function code is portable. What's NOT portable is the event-driven architecture around it \u2014 the triggers, permissions, environment variables referencing ARNs, and the assumption that every security automation is a Lambda function. Containerize your logic from day one if you care about portability.",
      "tags": [
        "cloud-portable"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-vpc",
      "nodeId": "vpc",
      "content": "<p><strong>Domains: 3 (primary), 1, 5</strong><br><br>Network foundation. Security groups (stateful, allow-only, instance-level), NACLs (stateless, allow+deny, subnet-level). VPC endpoints (gateway for S3/DynamoDB, interface for other services via PrivateLink). VPC Flow Logs \u2014 critical log source for detection and troubleshooting. Transit Gateway for hub-spoke. Network segmentation (north/south, east/west). Peering, route tables, NAT gateways. Reachability Analyzer tests paths. Network Access Analyzer identifies unintended access. Traffic Mirroring for deep packet inspection.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udfe1 Cloud-Portable (Medium Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Medium. Network topology is rebuildable. The hard part is the integrations \u2014 endpoint policies, PrivateLink, Transit Gateway topologies.<br><strong>Control Plane Dependency:</strong> 3/5 \u2014 Networking concepts (subnets, route tables, firewalls) are universal. Specific implementations differ but map reasonably.<br><strong>Data Gravity:</strong> 1/5 \u2014 No data stored. VPC is configuration/topology.<br><strong>Runtime Coupling:</strong> 3/5 \u2014 Security groups, NACLs, and routing are AWS-specific configs, but the networking primitives are standard.<br><br><strong>Why migration is hard:</strong> VPC endpoint policies (IAM-based), PrivateLink service connections, Transit Gateway route tables, Network Firewall Suricata rules, and traffic mirroring configs are all AWS-specific. Security group rules reference AWS-specific constructs (prefix lists, security group IDs as sources).<br><strong>Why migration could work:</strong> Subnets, CIDR blocks, route tables, NAT, VPN \u2014 these are universal networking concepts. Every cloud and every data center has equivalents. IaC tools like Terraform abstract much of it.<br><strong>Dependency risks:</strong> EC2, RDS, EKS, Lambda (VPC-attached), PrivateLink, Direct Connect, Transit Gateway \u2014 every compute and database service lives in VPCs.<br><strong>Alternatives:</strong> Azure VNet, GCP VPC, any physical or virtual network infrastructure. Terraform can abstract multi-cloud networking to a degree.<br><strong>Blast radius if migrating:</strong> Medium. Network topology is rebuildable. The blast radius comes from everything INSIDE the VPC, not the VPC itself.<br><br><strong>Honest take:</strong> VPC is one of the more portable AWS constructs because networking is networking. The lock-in comes from the integrations: VPC endpoint policies use IAM, security groups reference other security groups, and PrivateLink is AWS-specific. But if you're using Terraform, your network is already semi-portable.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 3 (primary), 1, 5\n\nNetwork foundation. Security groups (stateful, allow-only, instance-level), NACLs (stateless, allow+deny, subnet-level). VPC endpoints (gateway for S3/DynamoDB, interface for other services via PrivateLink). VPC Flow Logs \u2014 critical log source for detection and troubleshooting. Transit Gateway for hub-spoke. Network segmentation (north/south, east/west). Peering, route tables, NAT gateways. Reachability Analyzer tests paths. Network Access Analyzer identifies unintended access. Traffic Mirroring for deep packet inspection.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udfe1 Cloud-Portable (Medium Lock-In)\n\nMigration Difficulty: Medium. Network topology is rebuildable. The hard part is the integrations \u2014 endpoint policies, PrivateLink, Transit Gateway topologies.\nControl Plane Dependency: 3/5 \u2014 Networking concepts (subnets, route tables, firewalls) are universal. Specific implementations differ but map reasonably.\nData Gravity: 1/5 \u2014 No data stored. VPC is configuration/topology.\nRuntime Coupling: 3/5 \u2014 Security groups, NACLs, and routing are AWS-specific configs, but the networking primitives are standard.\n\nWhy migration is hard: VPC endpoint policies (IAM-based), PrivateLink service connections, Transit Gateway route tables, Network Firewall Suricata rules, and traffic mirroring configs are all AWS-specific. Security group rules reference AWS-specific constructs (prefix lists, security group IDs as sources).\nWhy migration could work: Subnets, CIDR blocks, route tables, NAT, VPN \u2014 these are universal networking concepts. Every cloud and every data center has equivalents. IaC tools like Terraform abstract much of it.\nDependency risks: EC2, RDS, EKS, Lambda (VPC-attached), PrivateLink, Direct Connect, Transit Gateway \u2014 every compute and database service lives in VPCs.\nAlternatives: Azure VNet, GCP VPC, any physical or virtual network infrastructure. Terraform can abstract multi-cloud networking to a degree.\nBlast radius if migrating: Medium. Network topology is rebuildable. The blast radius comes from everything INSIDE the VPC, not the VPC itself.\n\nHonest take: VPC is one of the more portable AWS constructs because networking is networking. The lock-in comes from the integrations: VPC endpoint policies use IAM, security groups reference other security groups, and PrivateLink is AWS-specific. But if you're using Terraform, your network is already semi-portable.",
      "tags": [
        "cloud-portable"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-inspector",
      "nodeId": "inspector",
      "content": "<p><strong>Domains: 1, 3</strong><br><br>Automated vulnerability scanning for EC2 instances, Lambda functions, and ECR container images \u2014 all three are testable. Continuous scanning (not one-time). Findings in Security Hub with severity scores. Network reachability assessments identify exposed ports/paths (Domain 3 troubleshooting). Integrates with Systems Manager for patch remediation. EventBridge triggers automated response.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udd34 AWS-Proprietary (High Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Medium-High. Vulnerability scanning is a commoditized market, but the AWS-native integration is what you'd lose.<br><strong>Control Plane Dependency:</strong> 5/5 \u2014 AWS-managed scanning. Vulnerability findings are in ASFF format. Scan targets are AWS resource types (EC2, Lambda, ECR).<br><strong>Data Gravity:</strong> 2/5 \u2014 Findings exportable. No significant user data.<br><strong>Runtime Coupling:</strong> 4/5 \u2014 Automated response triggers from Inspector findings via EventBridge are AWS-specific.<br><br><strong>Why migration is hard:</strong> Inspector automatically discovers and scans all EC2, Lambda, and ECR resources using the SSM agent. This zero-config discovery doesn't exist outside AWS. Findings feed Security Hub's compliance scoring.<br><strong>Why migration could work:</strong> Vulnerability scanning has excellent alternatives: Qualys, Tenable/Nessus, Snyk, Trivy, Grype. These work across clouds and on-prem. The scanning capability is not AWS-differentiated.<br><strong>Dependency risks:</strong> Security Hub aggregates findings. Systems Manager enables agent-based scanning. ECR triggers container image scans.<br><strong>Alternatives:</strong> Qualys, Tenable Nessus, Snyk Container, Trivy (open-source), Grype (open-source), Wiz, Prisma Cloud<br><strong>Blast radius if migrating:</strong> Low-Medium. Swap to a third-party scanner. The findings just go to a different dashboard.<br><br><strong>Honest take:</strong> Inspector is one of the easier services to replace. The vulnerability scanning market is mature and multi-cloud. The only thing you lose is the zero-config auto-discovery of AWS resources and the Security Hub integration. Use Snyk or Trivy if you want portability.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 1, 3\n\nAutomated vulnerability scanning for EC2 instances, Lambda functions, and ECR container images \u2014 all three are testable. Continuous scanning (not one-time). Findings in Security Hub with severity scores. Network reachability assessments identify exposed ports/paths (Domain 3 troubleshooting). Integrates with Systems Manager for patch remediation. EventBridge triggers automated response.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udd34 AWS-Proprietary (High Lock-In)\n\nMigration Difficulty: Medium-High. Vulnerability scanning is a commoditized market, but the AWS-native integration is what you'd lose.\nControl Plane Dependency: 5/5 \u2014 AWS-managed scanning. Vulnerability findings are in ASFF format. Scan targets are AWS resource types (EC2, Lambda, ECR).\nData Gravity: 2/5 \u2014 Findings exportable. No significant user data.\nRuntime Coupling: 4/5 \u2014 Automated response triggers from Inspector findings via EventBridge are AWS-specific.\n\nWhy migration is hard: Inspector automatically discovers and scans all EC2, Lambda, and ECR resources using the SSM agent. This zero-config discovery doesn't exist outside AWS. Findings feed Security Hub's compliance scoring.\nWhy migration could work: Vulnerability scanning has excellent alternatives: Qualys, Tenable/Nessus, Snyk, Trivy, Grype. These work across clouds and on-prem. The scanning capability is not AWS-differentiated.\nDependency risks: Security Hub aggregates findings. Systems Manager enables agent-based scanning. ECR triggers container image scans.\nAlternatives: Qualys, Tenable Nessus, Snyk Container, Trivy (open-source), Grype (open-source), Wiz, Prisma Cloud\nBlast radius if migrating: Low-Medium. Swap to a third-party scanner. The findings just go to a different dashboard.\n\nHonest take: Inspector is one of the easier services to replace. The vulnerability scanning market is mature and multi-cloud. The only thing you lose is the zero-config auto-discovery of AWS resources and the Security Hub integration. Use Snyk or Trivy if you want portability.",
      "tags": [
        "aws-proprietary"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-macie",
      "nodeId": "macie",
      "content": "<p><strong>Domains: 1, 6</strong><br><br>ML-powered sensitive data discovery in S3. Identifies PII, PHI, financial data, credentials. Automated and scheduled discovery jobs. Custom data identifiers for org-specific patterns. Findings go to Security Hub. Multi-account via Organizations. Domain 6: data governance compliance \u2014 know which S3 buckets contain sensitive data.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udd34 AWS-Proprietary (High Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Medium. Data classification tools exist across clouds and from third parties.<br><strong>Control Plane Dependency:</strong> 5/5 \u2014 AWS-proprietary data classification. Custom data identifiers, discovery jobs, and findings are all AWS-specific.<br><strong>Data Gravity:</strong> 2/5 \u2014 Findings exportable. Macie doesn't store your data.<br><strong>Runtime Coupling:</strong> 3/5 \u2014 Primarily operates on S3. Less runtime coupling than other detection services.<br><br><strong>Why migration is hard:</strong> Macie's ML-based classifiers are AWS-trained and AWS-managed. Custom data identifiers and allow lists don't transfer. Multi-account discovery jobs through Organizations are AWS-specific.<br><strong>Why migration could work:</strong> Data Loss Prevention (DLP) is a well-established market. Tools like Google Cloud DLP, Azure Purview, BigID, Nightfall, and Spirion work across platforms.<br><strong>Dependency risks:</strong> S3 (the only data source Macie scans), Security Hub (finding aggregation), Organizations (multi-account).<br><strong>Alternatives:</strong> Google Cloud DLP, Azure Purview, BigID, Nightfall AI, Spirion, open-source regex classifiers<br><strong>Blast radius if migrating:</strong> Low. Macie is S3-only. Replace with a third-party DLP tool and you gain coverage across more data stores.<br><br><strong>Honest take:</strong> Macie is narrow (S3 only) and replaceable. Third-party DLP tools cover more data sources across more platforms. Macie's value is convenience, not capability.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 1, 6\n\nML-powered sensitive data discovery in S3. Identifies PII, PHI, financial data, credentials. Automated and scheduled discovery jobs. Custom data identifiers for org-specific patterns. Findings go to Security Hub. Multi-account via Organizations. Domain 6: data governance compliance \u2014 know which S3 buckets contain sensitive data.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udd34 AWS-Proprietary (High Lock-In)\n\nMigration Difficulty: Medium. Data classification tools exist across clouds and from third parties.\nControl Plane Dependency: 5/5 \u2014 AWS-proprietary data classification. Custom data identifiers, discovery jobs, and findings are all AWS-specific.\nData Gravity: 2/5 \u2014 Findings exportable. Macie doesn't store your data.\nRuntime Coupling: 3/5 \u2014 Primarily operates on S3. Less runtime coupling than other detection services.\n\nWhy migration is hard: Macie's ML-based classifiers are AWS-trained and AWS-managed. Custom data identifiers and allow lists don't transfer. Multi-account discovery jobs through Organizations are AWS-specific.\nWhy migration could work: Data Loss Prevention (DLP) is a well-established market. Tools like Google Cloud DLP, Azure Purview, BigID, Nightfall, and Spirion work across platforms.\nDependency risks: S3 (the only data source Macie scans), Security Hub (finding aggregation), Organizations (multi-account).\nAlternatives: Google Cloud DLP, Azure Purview, BigID, Nightfall AI, Spirion, open-source regex classifiers\nBlast radius if migrating: Low. Macie is S3-only. Replace with a third-party DLP tool and you gain coverage across more data stores.\n\nHonest take: Macie is narrow (S3 only) and replaceable. Third-party DLP tools cover more data sources across more platforms. Macie's value is convenience, not capability.",
      "tags": [
        "aws-proprietary"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-config",
      "nodeId": "config",
      "content": "<p><strong>Domains: 1, 6</strong><br><br>Tracks resource configuration history and evaluates compliance. Managed rules (150+) and custom rules (Lambda). Conformance packs bundle rules for frameworks (CIS, PCI). Auto-remediation via Systems Manager Automation. Config Aggregator for multi-account/multi-region views. Findings flow to Security Hub. Domain 1: continuous monitoring/alerting. Domain 6: compliance evaluation and evidence.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udd34 AWS-Proprietary (High Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> High. Configuration compliance is deeply AWS-specific because the resources being evaluated are AWS resources.<br><strong>Control Plane Dependency:</strong> 5/5 \u2014 Config rules, conformance packs, and the resource configuration database are entirely AWS-proprietary.<br><strong>Data Gravity:</strong> 3/5 \u2014 Configuration snapshots are exportable. Custom rules are Lambda functions (code is yours).<br><strong>Runtime Coupling:</strong> 4/5 \u2014 Auto-remediation via Systems Manager, compliance scoring, and Security Hub integration are all AWS-coupled.<br><br><strong>Why migration is hard:</strong> Every Config rule evaluates an AWS-specific resource type (e.g., 'AWS::S3::Bucket'). Conformance packs map to AWS compliance frameworks. Auto-remediation uses SSM Automation runbooks. Multi-account aggregation uses Organizations. None of this transfers.<br><strong>Why migration could work:</strong> The concept of configuration compliance exists everywhere \u2014 Azure Policy, GCP Organization Policies, Open Policy Agent. If you've documented your compliance rules abstractly, you can reimplement.<br><strong>Dependency risks:</strong> Security Hub (finding aggregation), Organizations (multi-account), Systems Manager (remediation), CloudFormation (drift detection context).<br><strong>Alternatives:</strong> Azure Policy, GCP Organization Policies, Open Policy Agent (OPA), Terraform Sentinel, Chef InSpec, Prowler (open-source AWS-specific, but the model is portable)<br><strong>Blast radius if migrating:</strong> High for compliance teams. Every compliance rule needs rewriting for the target platform's resource model.<br><br><strong>Honest take:</strong> Config rules are inherently cloud-specific because they evaluate cloud-specific resources. There's no way around this. The mitigation is OPA/Terraform Sentinel as an abstraction layer \u2014 but you'd still need cloud-specific rules underneath.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 1, 6\n\nTracks resource configuration history and evaluates compliance. Managed rules (150+) and custom rules (Lambda). Conformance packs bundle rules for frameworks (CIS, PCI). Auto-remediation via Systems Manager Automation. Config Aggregator for multi-account/multi-region views. Findings flow to Security Hub. Domain 1: continuous monitoring/alerting. Domain 6: compliance evaluation and evidence.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udd34 AWS-Proprietary (High Lock-In)\n\nMigration Difficulty: High. Configuration compliance is deeply AWS-specific because the resources being evaluated are AWS resources.\nControl Plane Dependency: 5/5 \u2014 Config rules, conformance packs, and the resource configuration database are entirely AWS-proprietary.\nData Gravity: 3/5 \u2014 Configuration snapshots are exportable. Custom rules are Lambda functions (code is yours).\nRuntime Coupling: 4/5 \u2014 Auto-remediation via Systems Manager, compliance scoring, and Security Hub integration are all AWS-coupled.\n\nWhy migration is hard: Every Config rule evaluates an AWS-specific resource type (e.g., 'AWS::S3::Bucket'). Conformance packs map to AWS compliance frameworks. Auto-remediation uses SSM Automation runbooks. Multi-account aggregation uses Organizations. None of this transfers.\nWhy migration could work: The concept of configuration compliance exists everywhere \u2014 Azure Policy, GCP Organization Policies, Open Policy Agent. If you've documented your compliance rules abstractly, you can reimplement.\nDependency risks: Security Hub (finding aggregation), Organizations (multi-account), Systems Manager (remediation), CloudFormation (drift detection context).\nAlternatives: Azure Policy, GCP Organization Policies, Open Policy Agent (OPA), Terraform Sentinel, Chef InSpec, Prowler (open-source AWS-specific, but the model is portable)\nBlast radius if migrating: High for compliance teams. Every compliance rule needs rewriting for the target platform's resource model.\n\nHonest take: Config rules are inherently cloud-specific because they evaluate cloud-specific resources. There's no way around this. The mitigation is OPA/Terraform Sentinel as an abstraction layer \u2014 but you'd still need cloud-specific rules underneath.",
      "tags": [
        "aws-proprietary"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-eventbridge",
      "nodeId": "eventbridge",
      "content": "<p><strong>Domains: 1, 2</strong><br><br>Event bus that routes security findings to automated targets. Pattern matching on Security Hub findings, GuardDuty findings, Config rule changes, CloudTrail API calls. Targets: Lambda, Step Functions, SNS, SQS, Systems Manager. Critical for building automated incident response pipelines. Example: GuardDuty detects crypto mining \u2192 EventBridge \u2192 Lambda isolates the EC2 instance.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udd34 AWS-Proprietary (High Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> High. EventBridge is the central nervous system of AWS event-driven automation.<br><strong>Control Plane Dependency:</strong> 5/5 \u2014 Event patterns match AWS-specific event structures. Rule targets are AWS services.<br><strong>Data Gravity:</strong> 1/5 \u2014 Transient events, no persistent data.<br><strong>Runtime Coupling:</strong> 5/5 \u2014 Every security automation is wired through EventBridge. Event patterns reference AWS event detail types, source names, and field paths.<br><br><strong>Why migration is hard:</strong> Every EventBridge rule pattern matches AWS-specific event structures ('source': 'aws.guardduty', 'detail-type': 'GuardDuty Finding'). Targets are AWS services (Lambda, Step Functions, SNS, SQS). The event bus model, archive/replay, and cross-account event routing are all AWS-specific.<br><strong>Why migration could work:</strong> Message brokers and event buses exist everywhere \u2014 Azure Event Grid, GCP Eventarc, Apache Kafka, RabbitMQ. The routing concept is universal.<br><strong>Dependency risks:</strong> GuardDuty, Security Hub, Config, CloudTrail \u2014 as event sources. Lambda, Step Functions, SNS, SQS \u2014 as targets.<br><strong>Alternatives:</strong> Azure Event Grid, GCP Eventarc/Pub/Sub, Apache Kafka, RabbitMQ, NATS. Self-hosted event mesh with CloudEvents spec.<br><strong>Blast radius if migrating:</strong> Very high. Every automated security response flows through EventBridge. Dozens to hundreds of rules to rebuild.<br><br><strong>Honest take:</strong> EventBridge is the wiring. It's not the lock-in itself \u2014 it's the multiplier that connects all the other lock-in together. Every EventBridge rule you write cements your dependency on the services it connects.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 1, 2\n\nEvent bus that routes security findings to automated targets. Pattern matching on Security Hub findings, GuardDuty findings, Config rule changes, CloudTrail API calls. Targets: Lambda, Step Functions, SNS, SQS, Systems Manager. Critical for building automated incident response pipelines. Example: GuardDuty detects crypto mining \u2192 EventBridge \u2192 Lambda isolates the EC2 instance.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udd34 AWS-Proprietary (High Lock-In)\n\nMigration Difficulty: High. EventBridge is the central nervous system of AWS event-driven automation.\nControl Plane Dependency: 5/5 \u2014 Event patterns match AWS-specific event structures. Rule targets are AWS services.\nData Gravity: 1/5 \u2014 Transient events, no persistent data.\nRuntime Coupling: 5/5 \u2014 Every security automation is wired through EventBridge. Event patterns reference AWS event detail types, source names, and field paths.\n\nWhy migration is hard: Every EventBridge rule pattern matches AWS-specific event structures ('source': 'aws.guardduty', 'detail-type': 'GuardDuty Finding'). Targets are AWS services (Lambda, Step Functions, SNS, SQS). The event bus model, archive/replay, and cross-account event routing are all AWS-specific.\nWhy migration could work: Message brokers and event buses exist everywhere \u2014 Azure Event Grid, GCP Eventarc, Apache Kafka, RabbitMQ. The routing concept is universal.\nDependency risks: GuardDuty, Security Hub, Config, CloudTrail \u2014 as event sources. Lambda, Step Functions, SNS, SQS \u2014 as targets.\nAlternatives: Azure Event Grid, GCP Eventarc/Pub/Sub, Apache Kafka, RabbitMQ, NATS. Self-hosted event mesh with CloudEvents spec.\nBlast radius if migrating: Very high. Every automated security response flows through EventBridge. Dozens to hundreds of rules to rebuild.\n\nHonest take: EventBridge is the wiring. It's not the lock-in itself \u2014 it's the multiplier that connects all the other lock-in together. Every EventBridge rule you write cements your dependency on the services it connects.",
      "tags": [
        "aws-proprietary"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-shield",
      "nodeId": "shield",
      "content": "<p><strong>Domains: 2, 3</strong><br><br>DDoS protection. Standard: automatic, free, always-on for all AWS resources. Advanced: $3000/month \u2014 dedicated DRT (DDoS Response Team), cost protection during attacks, enhanced detection for CloudFront, Route 53, ELB, EC2, Global Accelerator. Health-based detection. Proactive engagement. SRT access to WAF rules. Domain 2: Shield Advanced protections as incident preparedness (Skill 2.1.2). Domain 3: edge protection.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udfe1 Cloud-Portable (Medium Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Medium. DDoS protection is a service you buy, not something you build.<br><strong>Control Plane Dependency:</strong> 4/5 \u2014 Shield Advanced requires AWS support team engagement. Protection groups reference AWS resources.<br><strong>Data Gravity:</strong> 1/5 \u2014 No significant data.<br><strong>Runtime Coupling:</strong> 2/5 \u2014 Shield Standard is automatic. Advanced is configuration, not runtime coupling.<br><br><strong>Why migration is hard:</strong> Shield Advanced's DDoS Response Team (DRT), cost protection, and proactive engagement are AWS-specific benefits. Health-based detection uses CloudWatch.<br><strong>Why migration could work:</strong> DDoS protection is a commodity: Cloudflare, Akamai, Azure DDoS Protection, GCP Cloud Armor. All provide similar capability.<br><strong>Dependency risks:</strong> CloudFront, Route 53, ELB, EC2 \u2014 protected resources. WAF \u2014 for L7 rules during attacks.<br><strong>Alternatives:</strong> Cloudflare (excellent DDoS), Akamai Prolexic, Azure DDoS Protection, GCP Cloud Armor, Imperva<br><strong>Blast radius if migrating:</strong> Low. Switch DDoS providers. DDoS protection is a commodity service.<br><br><strong>Honest take:</strong> Shield Standard is free and automatic \u2014 no lock-in. Shield Advanced is $3000/month for the DRT and cost protection. Cloudflare's free tier honestly provides better DDoS protection for web applications. Shield Advanced is only worth it for non-HTTP workloads (NLB, EC2 direct).</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 2, 3\n\nDDoS protection. Standard: automatic, free, always-on for all AWS resources. Advanced: $3000/month \u2014 dedicated DRT (DDoS Response Team), cost protection during attacks, enhanced detection for CloudFront, Route 53, ELB, EC2, Global Accelerator. Health-based detection. Proactive engagement. SRT access to WAF rules. Domain 2: Shield Advanced protections as incident preparedness (Skill 2.1.2). Domain 3: edge protection.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udfe1 Cloud-Portable (Medium Lock-In)\n\nMigration Difficulty: Medium. DDoS protection is a service you buy, not something you build.\nControl Plane Dependency: 4/5 \u2014 Shield Advanced requires AWS support team engagement. Protection groups reference AWS resources.\nData Gravity: 1/5 \u2014 No significant data.\nRuntime Coupling: 2/5 \u2014 Shield Standard is automatic. Advanced is configuration, not runtime coupling.\n\nWhy migration is hard: Shield Advanced's DDoS Response Team (DRT), cost protection, and proactive engagement are AWS-specific benefits. Health-based detection uses CloudWatch.\nWhy migration could work: DDoS protection is a commodity: Cloudflare, Akamai, Azure DDoS Protection, GCP Cloud Armor. All provide similar capability.\nDependency risks: CloudFront, Route 53, ELB, EC2 \u2014 protected resources. WAF \u2014 for L7 rules during attacks.\nAlternatives: Cloudflare (excellent DDoS), Akamai Prolexic, Azure DDoS Protection, GCP Cloud Armor, Imperva\nBlast radius if migrating: Low. Switch DDoS providers. DDoS protection is a commodity service.\n\nHonest take: Shield Standard is free and automatic \u2014 no lock-in. Shield Advanced is $3000/month for the DRT and cost protection. Cloudflare's free tier honestly provides better DDoS protection for web applications. Shield Advanced is only worth it for non-HTTP workloads (NLB, EC2 direct).",
      "tags": [
        "cloud-portable"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-trusted-advisor",
      "nodeId": "trusted-advisor",
      "content": "<p><strong>Domains: 6</strong><br><br>Checks for security best practices: open security groups, IAM use, MFA on root, S3 bucket permissions, exposed access keys, CloudTrail enabled. Full checks require Business or Enterprise Support. Integrates with Security Hub. Priority-based alerting with EventBridge.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udfe1 Cloud-Portable (Medium Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Low. Best practice scanning tools are a commodity.<br><strong>Control Plane Dependency:</strong> 4/5 \u2014 AWS-proprietary best practice checks. Findings reference AWS resource types.<br><strong>Data Gravity:</strong> 1/5 \u2014 Findings are informational.<br><strong>Runtime Coupling:</strong> 1/5 \u2014 Advisory, not runtime.<br><br><strong>Why migration is hard:</strong> Checks are AWS-specific (e.g., 'S3 bucket permissions', 'exposed access keys').<br><strong>Why migration could work:</strong> Prowler (open-source AWS scanner), ScoutSuite, CloudSploit, Wiz \u2014 all provide equivalent and often superior checks.<br><strong>Dependency risks:</strong> Security Hub (finding integration), Support plan (full checks require Business/Enterprise).<br><strong>Alternatives:</strong> Prowler (open-source, AWS + Azure + GCP), ScoutSuite, CloudSploit, Wiz, Prisma Cloud<br><strong>Blast radius if migrating:</strong> None. Advisory tool.<br><br><strong>Honest take:</strong> Prowler is open-source and better than Trusted Advisor. It covers AWS, Azure, and GCP. Use Prowler.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 6\n\nChecks for security best practices: open security groups, IAM use, MFA on root, S3 bucket permissions, exposed access keys, CloudTrail enabled. Full checks require Business or Enterprise Support. Integrates with Security Hub. Priority-based alerting with EventBridge.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udfe1 Cloud-Portable (Medium Lock-In)\n\nMigration Difficulty: Low. Best practice scanning tools are a commodity.\nControl Plane Dependency: 4/5 \u2014 AWS-proprietary best practice checks. Findings reference AWS resource types.\nData Gravity: 1/5 \u2014 Findings are informational.\nRuntime Coupling: 1/5 \u2014 Advisory, not runtime.\n\nWhy migration is hard: Checks are AWS-specific (e.g., 'S3 bucket permissions', 'exposed access keys').\nWhy migration could work: Prowler (open-source AWS scanner), ScoutSuite, CloudSploit, Wiz \u2014 all provide equivalent and often superior checks.\nDependency risks: Security Hub (finding integration), Support plan (full checks require Business/Enterprise).\nAlternatives: Prowler (open-source, AWS + Azure + GCP), ScoutSuite, CloudSploit, Wiz, Prisma Cloud\nBlast radius if migrating: None. Advisory tool.\n\nHonest take: Prowler is open-source and better than Trusted Advisor. It covers AWS, Azure, and GCP. Use Prowler.",
      "tags": [
        "cloud-portable"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-detective",
      "nodeId": "detective",
      "content": "<p><strong>Domains: 1, 2</strong><br><br>Graph-based investigation and root cause analysis. Automatically correlates data from GuardDuty, CloudTrail, VPC Flow Logs, EKS audit logs, and Security Hub findings. Visualizes resource interactions and API call patterns over time. Pivotal for Domain 2 (Incident Response) \u2014 answering 'what happened and why' after GuardDuty detects a threat.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udd34 AWS-Proprietary (High Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> High. The behavioral graph and ML models are AWS-built and AWS-trapped.<br><strong>Control Plane Dependency:</strong> 5/5 \u2014 AWS-proprietary graph analysis. The behavior models and correlation engine are opaque.<br><strong>Data Gravity:</strong> 3/5 \u2014 No user data stored. But the behavioral graph built over months of data collection doesn't transfer.<br><strong>Runtime Coupling:</strong> 3/5 \u2014 Consumed by analysts, not automated systems. Less runtime coupling.<br><br><strong>Why migration is hard:</strong> Detective builds a behavioral graph from CloudTrail, VPC Flow Logs, GuardDuty findings, and EKS audit logs over time. This graph represents months of learned 'normal' behavior. When you leave, you lose all of that baseline context.<br><strong>Why migration could work:</strong> SIEM tools with UEBA capabilities (Splunk UBA, Azure Sentinel, Chronicle SOAR) provide similar investigation capabilities. They're cloud-agnostic.<br><strong>Dependency risks:</strong> GuardDuty findings, CloudTrail events, VPC Flow Logs, Security Hub findings \u2014 all AWS-specific inputs.<br><strong>Alternatives:</strong> Splunk UBA, Azure Sentinel, Google Chronicle, Datadog Cloud SIEM, CrowdStrike Falcon LogScale<br><strong>Blast radius if migrating:</strong> Medium. Analysts lose their investigation tool. But if you're using a third-party SIEM, Detective is supplementary.<br><br><strong>Honest take:</strong> Detective is useful but not irreplaceable. Any good SIEM with behavioral analytics does similar work. The lock-in risk is moderate because most mature security teams already use a third-party SIEM.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 1, 2\n\nGraph-based investigation and root cause analysis. Automatically correlates data from GuardDuty, CloudTrail, VPC Flow Logs, EKS audit logs, and Security Hub findings. Visualizes resource interactions and API call patterns over time. Pivotal for Domain 2 (Incident Response) \u2014 answering 'what happened and why' after GuardDuty detects a threat.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udd34 AWS-Proprietary (High Lock-In)\n\nMigration Difficulty: High. The behavioral graph and ML models are AWS-built and AWS-trapped.\nControl Plane Dependency: 5/5 \u2014 AWS-proprietary graph analysis. The behavior models and correlation engine are opaque.\nData Gravity: 3/5 \u2014 No user data stored. But the behavioral graph built over months of data collection doesn't transfer.\nRuntime Coupling: 3/5 \u2014 Consumed by analysts, not automated systems. Less runtime coupling.\n\nWhy migration is hard: Detective builds a behavioral graph from CloudTrail, VPC Flow Logs, GuardDuty findings, and EKS audit logs over time. This graph represents months of learned 'normal' behavior. When you leave, you lose all of that baseline context.\nWhy migration could work: SIEM tools with UEBA capabilities (Splunk UBA, Azure Sentinel, Chronicle SOAR) provide similar investigation capabilities. They're cloud-agnostic.\nDependency risks: GuardDuty findings, CloudTrail events, VPC Flow Logs, Security Hub findings \u2014 all AWS-specific inputs.\nAlternatives: Splunk UBA, Azure Sentinel, Google Chronicle, Datadog Cloud SIEM, CrowdStrike Falcon LogScale\nBlast radius if migrating: Medium. Analysts lose their investigation tool. But if you're using a third-party SIEM, Detective is supplementary.\n\nHonest take: Detective is useful but not irreplaceable. Any good SIEM with behavioral analytics does similar work. The lock-in risk is moderate because most mature security teams already use a third-party SIEM.",
      "tags": [
        "aws-proprietary"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-security-lake",
      "nodeId": "security-lake",
      "content": "<p><strong>Domains: 1</strong><br><br>Centralizes security data into a purpose-built data lake in S3. Normalizes logs to OCSF format (Open Cybersecurity Schema Framework \u2014 new C03 topic). Ingests from CloudTrail, VPC Flow Logs, Route 53, Security Hub, Lambda data events, S3 data events, and third-party sources. Query with Athena or feed to OpenSearch/SIEM tools. Subscriber model for data consumers.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udd34 AWS-Proprietary (High Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Medium. OCSF is open, S3 data is exportable. But the automated ingestion from dozens of AWS sources is the value you lose.<br><strong>Control Plane Dependency:</strong> 5/5 \u2014 AWS-managed data lake with OCSF normalization. The ingestion pipeline, subscriber model, and source management are AWS-proprietary.<br><strong>Data Gravity:</strong> 3/5 \u2014 Data stored in S3 (your account) in OCSF format. OCSF is actually an open standard, which helps portability.<br><strong>Runtime Coupling:</strong> 3/5 \u2014 Subscribers query via Athena or consume via third-party tools. The OCSF schema is open.<br><br><strong>Why migration is hard:</strong> Security Lake automatically ingests and normalizes CloudTrail, VPC Flow Logs, Route 53, Security Hub, Lambda, and S3 data events. Rebuilding that ingestion pipeline elsewhere is significant work.<br><strong>Why migration could work:</strong> OCSF is an open standard backed by a consortium (not just AWS). The normalized data in S3 is exportable. Third-party SIEMs already support OCSF ingestion.<br><strong>Dependency risks:</strong> CloudTrail, VPC Flow Logs, Route 53, S3, Lambda, Security Hub \u2014 as data sources. Athena, OpenSearch \u2014 as consumers.<br><strong>Alternatives:</strong> Snowflake Security Data Lake, Google Chronicle, Elastic Security, Splunk with OCSF ingestion, custom S3 data lake with Parquet<br><strong>Blast radius if migrating:</strong> Medium. The data is portable. The automated ingestion pipeline is not.<br><br><strong>Honest take:</strong> Security Lake is one of AWS's more forward-thinking designs \u2014 using the open OCSF standard means your data isn't trapped in a proprietary schema. The lock-in is in the automated ingestion, not the data format. Credit where due.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 1\n\nCentralizes security data into a purpose-built data lake in S3. Normalizes logs to OCSF format (Open Cybersecurity Schema Framework \u2014 new C03 topic). Ingests from CloudTrail, VPC Flow Logs, Route 53, Security Hub, Lambda data events, S3 data events, and third-party sources. Query with Athena or feed to OpenSearch/SIEM tools. Subscriber model for data consumers.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udd34 AWS-Proprietary (High Lock-In)\n\nMigration Difficulty: Medium. OCSF is open, S3 data is exportable. But the automated ingestion from dozens of AWS sources is the value you lose.\nControl Plane Dependency: 5/5 \u2014 AWS-managed data lake with OCSF normalization. The ingestion pipeline, subscriber model, and source management are AWS-proprietary.\nData Gravity: 3/5 \u2014 Data stored in S3 (your account) in OCSF format. OCSF is actually an open standard, which helps portability.\nRuntime Coupling: 3/5 \u2014 Subscribers query via Athena or consume via third-party tools. The OCSF schema is open.\n\nWhy migration is hard: Security Lake automatically ingests and normalizes CloudTrail, VPC Flow Logs, Route 53, Security Hub, Lambda, and S3 data events. Rebuilding that ingestion pipeline elsewhere is significant work.\nWhy migration could work: OCSF is an open standard backed by a consortium (not just AWS). The normalized data in S3 is exportable. Third-party SIEMs already support OCSF ingestion.\nDependency risks: CloudTrail, VPC Flow Logs, Route 53, S3, Lambda, Security Hub \u2014 as data sources. Athena, OpenSearch \u2014 as consumers.\nAlternatives: Snowflake Security Data Lake, Google Chronicle, Elastic Security, Splunk with OCSF ingestion, custom S3 data lake with Parquet\nBlast radius if migrating: Medium. The data is portable. The automated ingestion pipeline is not.\n\nHonest take: Security Lake is one of AWS's more forward-thinking designs \u2014 using the open OCSF standard means your data isn't trapped in a proprietary schema. The lock-in is in the automated ingestion, not the data format. Credit where due.",
      "tags": [
        "aws-proprietary"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-athena",
      "nodeId": "athena",
      "content": "<p><strong>Domains: 1</strong><br><br>Serverless SQL queries against S3-stored logs. Query CloudTrail logs, VPC Flow Logs, ALB access logs, CloudFront logs, Security Lake data. No infrastructure to manage. Partition projection for efficient log querying. Federated queries across data sources. Key tool for ad-hoc incident investigation and log analysis.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udfe2 Platform-Agnostic (Low Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Low. Athena is Presto/Trino with a managed wrapper. Your SQL queries work on any Presto/Trino deployment.<br><strong>Control Plane Dependency:</strong> 2/5 \u2014 Standard SQL over files in S3. Uses Presto/Trino engine under the hood.<br><strong>Data Gravity:</strong> 1/5 \u2014 No data stored. Queries data where it lives (S3).<br><strong>Runtime Coupling:</strong> 2/5 \u2014 SQL queries are mostly portable. Table definitions (Glue Catalog) are AWS-specific, but schemas are DDL.<br><br><strong>Why migration is hard:</strong> Glue Data Catalog table definitions, partition projection, workgroup configurations, and query result locations are AWS-specific metadata.<br><strong>Why migration could work:</strong> Athena IS Trino/Presto. Run Trino on any platform and your SQL queries work with minimal changes. The data is in S3 (or any S3-compatible store). DDL can be recreated.<br><strong>Dependency risks:</strong> S3 (data source), Glue Data Catalog (metadata), CloudTrail/VPC Flow Logs (common query targets).<br><strong>Alternatives:</strong> Trino/Presto (self-hosted, identical engine), BigQuery (GCP), Azure Synapse Serverless SQL, DuckDB (local), Spark SQL<br><strong>Blast radius if migrating:</strong> Low. Queries are SQL. Engine is open-source. Data is in S3-compatible storage. This is one of the easiest migrations.<br><br><strong>Honest take:</strong> Athena is genuinely portable because it's a thin managed layer over Trino. AWS gets credit for not building a proprietary query engine here. Your SQL works elsewhere.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 1\n\nServerless SQL queries against S3-stored logs. Query CloudTrail logs, VPC Flow Logs, ALB access logs, CloudFront logs, Security Lake data. No infrastructure to manage. Partition projection for efficient log querying. Federated queries across data sources. Key tool for ad-hoc incident investigation and log analysis.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udfe2 Platform-Agnostic (Low Lock-In)\n\nMigration Difficulty: Low. Athena is Presto/Trino with a managed wrapper. Your SQL queries work on any Presto/Trino deployment.\nControl Plane Dependency: 2/5 \u2014 Standard SQL over files in S3. Uses Presto/Trino engine under the hood.\nData Gravity: 1/5 \u2014 No data stored. Queries data where it lives (S3).\nRuntime Coupling: 2/5 \u2014 SQL queries are mostly portable. Table definitions (Glue Catalog) are AWS-specific, but schemas are DDL.\n\nWhy migration is hard: Glue Data Catalog table definitions, partition projection, workgroup configurations, and query result locations are AWS-specific metadata.\nWhy migration could work: Athena IS Trino/Presto. Run Trino on any platform and your SQL queries work with minimal changes. The data is in S3 (or any S3-compatible store). DDL can be recreated.\nDependency risks: S3 (data source), Glue Data Catalog (metadata), CloudTrail/VPC Flow Logs (common query targets).\nAlternatives: Trino/Presto (self-hosted, identical engine), BigQuery (GCP), Azure Synapse Serverless SQL, DuckDB (local), Spark SQL\nBlast radius if migrating: Low. Queries are SQL. Engine is open-source. Data is in S3-compatible storage. This is one of the easiest migrations.\n\nHonest take: Athena is genuinely portable because it's a thin managed layer over Trino. AWS gets credit for not building a proprietary query engine here. Your SQL works elsewhere.",
      "tags": [
        "platform-agnostic"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-cloudtrail-lake",
      "nodeId": "cloudtrail-lake",
      "content": "<p><strong>Domains: 1</strong><br><br>Managed data lake for CloudTrail events \u2014 query directly with SQL without setting up S3 + Athena. New in C03. Event data stores with configurable retention. Supports organization-level event collection. Dashboard and saved queries. Alternative to the traditional CloudTrail \u2192 S3 \u2192 Athena pipeline.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udd34 AWS-Proprietary (High Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> High. CloudTrail Lake is convenience over CloudTrail\u2192S3\u2192Athena. The data is AWS audit events, which are inherently non-portable.<br><strong>Control Plane Dependency:</strong> 5/5 \u2014 AWS-proprietary managed data store for CloudTrail events. SQL queries use CloudTrail-specific schema.<br><strong>Data Gravity:</strong> 3/5 \u2014 Data is queryable but stored in AWS-managed infrastructure. You don't control the underlying storage.<br><strong>Runtime Coupling:</strong> 4/5 \u2014 Queries reference AWS API event names and CloudTrail-specific fields.<br><br><strong>Why migration is hard:</strong> The events being queried ARE AWS API calls. The value is the managed infrastructure, but the data itself is only meaningful in an AWS context.<br><strong>Why migration could work:</strong> If you're using CloudTrail\u2192S3\u2192Athena today, you already have the data in S3. CloudTrail Lake is a convenience layer, not a new data silo.<br><strong>Dependency risks:</strong> CloudTrail events as the sole data source. Organization-wide event collection via Organizations.<br><strong>Alternatives:</strong> CloudTrail \u2192 S3 \u2192 Trino (portable query engine), ship CloudTrail to Splunk/Elastic/Chronicle for multi-cloud SIEM<br><strong>Blast radius if migrating:</strong> Low if you maintain CloudTrail\u2192S3 as a parallel path. High if CloudTrail Lake is your only query mechanism.<br><br><strong>Honest take:</strong> CloudTrail Lake is a convenience trap. It simplifies querying but removes the S3-based data path that gave you portability. Keep CloudTrail\u2192S3 as your primary, use Lake as a convenience layer.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 1\n\nManaged data lake for CloudTrail events \u2014 query directly with SQL without setting up S3 + Athena. New in C03. Event data stores with configurable retention. Supports organization-level event collection. Dashboard and saved queries. Alternative to the traditional CloudTrail \u2192 S3 \u2192 Athena pipeline.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udd34 AWS-Proprietary (High Lock-In)\n\nMigration Difficulty: High. CloudTrail Lake is convenience over CloudTrail\u2192S3\u2192Athena. The data is AWS audit events, which are inherently non-portable.\nControl Plane Dependency: 5/5 \u2014 AWS-proprietary managed data store for CloudTrail events. SQL queries use CloudTrail-specific schema.\nData Gravity: 3/5 \u2014 Data is queryable but stored in AWS-managed infrastructure. You don't control the underlying storage.\nRuntime Coupling: 4/5 \u2014 Queries reference AWS API event names and CloudTrail-specific fields.\n\nWhy migration is hard: The events being queried ARE AWS API calls. The value is the managed infrastructure, but the data itself is only meaningful in an AWS context.\nWhy migration could work: If you're using CloudTrail\u2192S3\u2192Athena today, you already have the data in S3. CloudTrail Lake is a convenience layer, not a new data silo.\nDependency risks: CloudTrail events as the sole data source. Organization-wide event collection via Organizations.\nAlternatives: CloudTrail \u2192 S3 \u2192 Trino (portable query engine), ship CloudTrail to Splunk/Elastic/Chronicle for multi-cloud SIEM\nBlast radius if migrating: Low if you maintain CloudTrail\u2192S3 as a parallel path. High if CloudTrail Lake is your only query mechanism.\n\nHonest take: CloudTrail Lake is a convenience trap. It simplifies querying but removes the S3-based data path that gave you portability. Keep CloudTrail\u2192S3 as your primary, use Lake as a convenience layer.",
      "tags": [
        "aws-proprietary"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-opensearch",
      "nodeId": "opensearch",
      "content": "<p><strong>Domains: 1</strong><br><br>Real-time log analytics, visualization, and dashboarding. Ingest via Kinesis Data Firehose, Lambda, or CloudWatch Logs subscription filters. Parse, normalize, and correlate security logs at scale. Used alongside or instead of Athena for continuous monitoring vs ad-hoc queries. Integration with Managed Grafana for visualization.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udfe2 Platform-Agnostic (Low Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Low. OpenSearch is open-source. Run it anywhere.<br><strong>Control Plane Dependency:</strong> 2/5 \u2014 OpenSearch is open-source (forked from Elasticsearch). AWS manages it but doesn't own the engine.<br><strong>Data Gravity:</strong> 3/5 \u2014 Data is in OpenSearch indices. Index data can be snapshotted and restored on any OpenSearch/Elasticsearch cluster.<br><strong>Runtime Coupling:</strong> 2/5 \u2014 OpenSearch APIs are standard. Dashboards are Kibana-compatible.<br><br><strong>Why migration is hard:</strong> Managed features (fine-grained access control via IAM, VPC endpoints, automated snapshots) are AWS-specific convenience. UltraWarm/cold storage tiers are AWS-managed.<br><strong>Why migration could work:</strong> OpenSearch is literally open-source software. Self-host it, run it on Elastic Cloud, or use Azure/GCP managed offerings. Index snapshots transfer directly.<br><strong>Dependency risks:</strong> CloudWatch Logs subscriptions, Kinesis Data Firehose, Lambda \u2014 as data ingestion paths.<br><strong>Alternatives:</strong> Self-hosted OpenSearch, Elastic Cloud (Elasticsearch), GCP managed Elasticsearch, Azure Cognitive Search, Grafana Loki (for logs)<br><strong>Blast radius if migrating:</strong> Low. Snapshot indices, restore elsewhere. Dashboards export. Queries are the same.<br><br><strong>Honest take:</strong> One of the most portable AWS services. It's open-source software with a managed wrapper. The only AWS-specific parts are the IAM integration and the managed infrastructure. Your data and queries transfer directly.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 1\n\nReal-time log analytics, visualization, and dashboarding. Ingest via Kinesis Data Firehose, Lambda, or CloudWatch Logs subscription filters. Parse, normalize, and correlate security logs at scale. Used alongside or instead of Athena for continuous monitoring vs ad-hoc queries. Integration with Managed Grafana for visualization.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udfe2 Platform-Agnostic (Low Lock-In)\n\nMigration Difficulty: Low. OpenSearch is open-source. Run it anywhere.\nControl Plane Dependency: 2/5 \u2014 OpenSearch is open-source (forked from Elasticsearch). AWS manages it but doesn't own the engine.\nData Gravity: 3/5 \u2014 Data is in OpenSearch indices. Index data can be snapshotted and restored on any OpenSearch/Elasticsearch cluster.\nRuntime Coupling: 2/5 \u2014 OpenSearch APIs are standard. Dashboards are Kibana-compatible.\n\nWhy migration is hard: Managed features (fine-grained access control via IAM, VPC endpoints, automated snapshots) are AWS-specific convenience. UltraWarm/cold storage tiers are AWS-managed.\nWhy migration could work: OpenSearch is literally open-source software. Self-host it, run it on Elastic Cloud, or use Azure/GCP managed offerings. Index snapshots transfer directly.\nDependency risks: CloudWatch Logs subscriptions, Kinesis Data Firehose, Lambda \u2014 as data ingestion paths.\nAlternatives: Self-hosted OpenSearch, Elastic Cloud (Elasticsearch), GCP managed Elasticsearch, Azure Cognitive Search, Grafana Loki (for logs)\nBlast radius if migrating: Low. Snapshot indices, restore elsewhere. Dashboards export. Queries are the same.\n\nHonest take: One of the most portable AWS services. It's open-source software with a managed wrapper. The only AWS-specific parts are the IAM integration and the managed infrastructure. Your data and queries transfer directly.",
      "tags": [
        "platform-agnostic"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-managed-grafana",
      "nodeId": "managed-grafana",
      "content": "<p><strong>Domains: 1</strong><br><br>Managed visualization and dashboarding for security metrics and log data. Data sources: CloudWatch, OpenSearch, Athena, X-Ray, and more. New in C03 \u2014 used for security monitoring dashboards and log correlation visualization.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udfe2 Platform-Agnostic (Low Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Very Low. Export dashboards as JSON. Import to any Grafana instance.<br><strong>Control Plane Dependency:</strong> 1/5 \u2014 Grafana is open-source. AWS just manages the deployment.<br><strong>Data Gravity:</strong> 1/5 \u2014 Dashboards are JSON exports. No significant data stored.<br><strong>Runtime Coupling:</strong> 1/5 \u2014 Grafana runs identically anywhere. Dashboard JSON is portable.<br><br><strong>Why migration is hard:</strong> AWS SSO integration and IAM-based data source authentication are AWS-specific. Workspace management is AWS-proprietary.<br><strong>Why migration could work:</strong> Grafana is fully open-source. Self-host it, use Grafana Cloud, or run on any platform. Dashboard definitions are JSON files that import anywhere.<br><strong>Dependency risks:</strong> CloudWatch, OpenSearch, Athena, Prometheus-compatible sources \u2014 as data backends.<br><strong>Alternatives:</strong> Self-hosted Grafana (identical), Grafana Cloud, Kibana, Datadog Dashboards, any visualization tool<br><strong>Blast radius if migrating:</strong> Minimal. Export dashboards, import elsewhere. Done in hours.<br><br><strong>Honest take:</strong> Managed Grafana is genuinely portable. AWS deserves zero lock-in criticism here. This is open-source software with a managed wrapper. The only thing you pay for is not running it yourself.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 1\n\nManaged visualization and dashboarding for security metrics and log data. Data sources: CloudWatch, OpenSearch, Athena, X-Ray, and more. New in C03 \u2014 used for security monitoring dashboards and log correlation visualization.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udfe2 Platform-Agnostic (Low Lock-In)\n\nMigration Difficulty: Very Low. Export dashboards as JSON. Import to any Grafana instance.\nControl Plane Dependency: 1/5 \u2014 Grafana is open-source. AWS just manages the deployment.\nData Gravity: 1/5 \u2014 Dashboards are JSON exports. No significant data stored.\nRuntime Coupling: 1/5 \u2014 Grafana runs identically anywhere. Dashboard JSON is portable.\n\nWhy migration is hard: AWS SSO integration and IAM-based data source authentication are AWS-specific. Workspace management is AWS-proprietary.\nWhy migration could work: Grafana is fully open-source. Self-host it, use Grafana Cloud, or run on any platform. Dashboard definitions are JSON files that import anywhere.\nDependency risks: CloudWatch, OpenSearch, Athena, Prometheus-compatible sources \u2014 as data backends.\nAlternatives: Self-hosted Grafana (identical), Grafana Cloud, Kibana, Datadog Dashboards, any visualization tool\nBlast radius if migrating: Minimal. Export dashboards, import elsewhere. Done in hours.\n\nHonest take: Managed Grafana is genuinely portable. AWS deserves zero lock-in criticism here. This is open-source software with a managed wrapper. The only thing you pay for is not running it yourself.",
      "tags": [
        "platform-agnostic"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-sns",
      "nodeId": "sns",
      "content": "<p><strong>Domains: 1</strong><br><br>Pub/sub notification service. Delivers security alerts to email, SMS, HTTP endpoints, Lambda, SQS. CloudWatch Alarms \u2192 SNS for threshold-based alerts. EventBridge \u2192 SNS for event-based alerts. Message data protection policies can detect and mask PII in messages (new C03 topic). Encryption at rest with KMS.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udfe1 Cloud-Portable (Medium Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Low-Medium. Pub/sub is ubiquitous. Migration is straightforward if you've abstracted the messaging layer.<br><strong>Control Plane Dependency:</strong> 3/5 \u2014 Topic/subscription model is common. But topic ARNs, access policies, and message filtering syntax are AWS-specific.<br><strong>Data Gravity:</strong> 1/5 \u2014 Transient messages, no persistent data.<br><strong>Runtime Coupling:</strong> 3/5 \u2014 Publishers and subscribers reference topic ARNs. Message data protection policies (new C03) are AWS-specific.<br><br><strong>Why migration is hard:</strong> Fan-out patterns (SNS\u2192SQS, SNS\u2192Lambda) and message filtering policies are AWS-specific wiring. Access policies use IAM syntax.<br><strong>Why migration could work:</strong> Every cloud has pub/sub: GCP Pub/Sub, Azure Service Bus, and self-hosted options like Kafka, RabbitMQ, NATS.<br><strong>Dependency risks:</strong> CloudWatch Alarms \u2192 SNS, EventBridge \u2192 SNS, Config \u2192 SNS for notifications. Lambda as subscriber.<br><strong>Alternatives:</strong> GCP Pub/Sub, Azure Service Bus, Apache Kafka, RabbitMQ, NATS, Redis Pub/Sub<br><strong>Blast radius if migrating:</strong> Low. Pub/sub is a commodity. The integration points need rewiring, not the concept.<br><br><strong>Honest take:</strong> SNS itself is not a lock-in risk. The lock-in is in what triggers it (CloudWatch, EventBridge) and what consumes from it (Lambda). The message broker is the easiest part to replace.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 1\n\nPub/sub notification service. Delivers security alerts to email, SMS, HTTP endpoints, Lambda, SQS. CloudWatch Alarms \u2192 SNS for threshold-based alerts. EventBridge \u2192 SNS for event-based alerts. Message data protection policies can detect and mask PII in messages (new C03 topic). Encryption at rest with KMS.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udfe1 Cloud-Portable (Medium Lock-In)\n\nMigration Difficulty: Low-Medium. Pub/sub is ubiquitous. Migration is straightforward if you've abstracted the messaging layer.\nControl Plane Dependency: 3/5 \u2014 Topic/subscription model is common. But topic ARNs, access policies, and message filtering syntax are AWS-specific.\nData Gravity: 1/5 \u2014 Transient messages, no persistent data.\nRuntime Coupling: 3/5 \u2014 Publishers and subscribers reference topic ARNs. Message data protection policies (new C03) are AWS-specific.\n\nWhy migration is hard: Fan-out patterns (SNS\u2192SQS, SNS\u2192Lambda) and message filtering policies are AWS-specific wiring. Access policies use IAM syntax.\nWhy migration could work: Every cloud has pub/sub: GCP Pub/Sub, Azure Service Bus, and self-hosted options like Kafka, RabbitMQ, NATS.\nDependency risks: CloudWatch Alarms \u2192 SNS, EventBridge \u2192 SNS, Config \u2192 SNS for notifications. Lambda as subscriber.\nAlternatives: GCP Pub/Sub, Azure Service Bus, Apache Kafka, RabbitMQ, NATS, Redis Pub/Sub\nBlast radius if migrating: Low. Pub/sub is a commodity. The integration points need rewiring, not the concept.\n\nHonest take: SNS itself is not a lock-in risk. The lock-in is in what triggers it (CloudWatch, EventBridge) and what consumes from it (Lambda). The message broker is the easiest part to replace.",
      "tags": [
        "cloud-portable"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-user-notifications",
      "nodeId": "user-notifications",
      "content": "<p><strong>Domains: 1</strong><br><br>Centralized notification management. Configure delivery channels (email, Chatbot). Aggregate notifications from multiple AWS services. New in C03 in-scope services list.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udfe1 Cloud-Portable (Medium Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Low. Notification routing is trivially replaceable.<br><strong>Control Plane Dependency:</strong> 3/5 \u2014 AWS-specific notification aggregation.<br><strong>Data Gravity:</strong> 1/5 \u2014 No data stored.<br><strong>Runtime Coupling:</strong> 1/5 \u2014 Notification convenience, not runtime dependency.<br><br><strong>Why migration is hard:</strong> Notification rules reference AWS service events.<br><strong>Why migration could work:</strong> PagerDuty, Opsgenie, and email routing do this across platforms.<br><strong>Dependency risks:</strong> AWS services (event sources), email/Chatbot (delivery channels).<br><strong>Alternatives:</strong> PagerDuty, Opsgenie, Slack integrations, custom notification routing<br><strong>Blast radius if migrating:</strong> None.<br><br><strong>Honest take:</strong> Not a meaningful lock-in risk. Use PagerDuty for multi-cloud alerting.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 1\n\nCentralized notification management. Configure delivery channels (email, Chatbot). Aggregate notifications from multiple AWS services. New in C03 in-scope services list.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udfe1 Cloud-Portable (Medium Lock-In)\n\nMigration Difficulty: Low. Notification routing is trivially replaceable.\nControl Plane Dependency: 3/5 \u2014 AWS-specific notification aggregation.\nData Gravity: 1/5 \u2014 No data stored.\nRuntime Coupling: 1/5 \u2014 Notification convenience, not runtime dependency.\n\nWhy migration is hard: Notification rules reference AWS service events.\nWhy migration could work: PagerDuty, Opsgenie, and email routing do this across platforms.\nDependency risks: AWS services (event sources), email/Chatbot (delivery channels).\nAlternatives: PagerDuty, Opsgenie, Slack integrations, custom notification routing\nBlast radius if migrating: None.\n\nHonest take: Not a meaningful lock-in risk. Use PagerDuty for multi-cloud alerting.",
      "tags": [
        "cloud-portable"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-step-functions",
      "nodeId": "step-functions",
      "content": "<p><strong>Domains: 2</strong><br><br>Orchestrates multi-step incident response workflows with visual state machines. Branching logic, error handling, parallel execution, wait states. Example: receive GuardDuty finding \u2192 enrich with Detective \u2192 decide severity \u2192 isolate instance OR just notify \u2192 create ticket. Integrates with Lambda, Systems Manager, SNS, DynamoDB.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udd34 AWS-Proprietary (High Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> High. Amazon States Language is proprietary. Every workflow must be rewritten for a different orchestrator.<br><strong>Control Plane Dependency:</strong> 5/5 \u2014 Amazon States Language (ASL) is AWS-proprietary. State machine definitions reference AWS service integrations by ARN.<br><strong>Data Gravity:</strong> 1/5 \u2014 No persistent data. Execution history is transient.<br><strong>Runtime Coupling:</strong> 5/5 \u2014 Every state in a workflow calls an AWS service by ARN. Task states invoke Lambda, SNS, SQS, DynamoDB, etc. using AWS-specific integration patterns.<br><br><strong>Why migration is hard:</strong> ASL is a proprietary workflow language. Service integrations (.sync, .waitForTaskToken patterns) are deeply AWS-specific. Express vs Standard workflows are AWS concepts. The workflow definitions cannot be imported into any other system.<br><strong>Why migration could work:</strong> Workflow orchestration tools exist everywhere: Azure Durable Functions, GCP Workflows, Temporal, Apache Airflow, Prefect. The business logic is portable; the orchestration definition is not.<br><strong>Dependency risks:</strong> Lambda (task execution), EventBridge (triggers), SNS/SQS (messaging), DynamoDB (state), Systems Manager (operations).<br><strong>Alternatives:</strong> Temporal (open-source, portable), Azure Durable Functions, GCP Workflows, Apache Airflow, Prefect, Argo Workflows (Kubernetes)<br><strong>Blast radius if migrating:</strong> High for incident response workflows. Every automated playbook needs rewriting in a new orchestration language.<br><br><strong>Honest take:</strong> Step Functions is the orchestration trap. ASL looks simple, but it's proprietary glue connecting proprietary services. If portability matters, use Temporal or Argo Workflows from day one. They're better tools anyway.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 2\n\nOrchestrates multi-step incident response workflows with visual state machines. Branching logic, error handling, parallel execution, wait states. Example: receive GuardDuty finding \u2192 enrich with Detective \u2192 decide severity \u2192 isolate instance OR just notify \u2192 create ticket. Integrates with Lambda, Systems Manager, SNS, DynamoDB.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udd34 AWS-Proprietary (High Lock-In)\n\nMigration Difficulty: High. Amazon States Language is proprietary. Every workflow must be rewritten for a different orchestrator.\nControl Plane Dependency: 5/5 \u2014 Amazon States Language (ASL) is AWS-proprietary. State machine definitions reference AWS service integrations by ARN.\nData Gravity: 1/5 \u2014 No persistent data. Execution history is transient.\nRuntime Coupling: 5/5 \u2014 Every state in a workflow calls an AWS service by ARN. Task states invoke Lambda, SNS, SQS, DynamoDB, etc. using AWS-specific integration patterns.\n\nWhy migration is hard: ASL is a proprietary workflow language. Service integrations (.sync, .waitForTaskToken patterns) are deeply AWS-specific. Express vs Standard workflows are AWS concepts. The workflow definitions cannot be imported into any other system.\nWhy migration could work: Workflow orchestration tools exist everywhere: Azure Durable Functions, GCP Workflows, Temporal, Apache Airflow, Prefect. The business logic is portable; the orchestration definition is not.\nDependency risks: Lambda (task execution), EventBridge (triggers), SNS/SQS (messaging), DynamoDB (state), Systems Manager (operations).\nAlternatives: Temporal (open-source, portable), Azure Durable Functions, GCP Workflows, Apache Airflow, Prefect, Argo Workflows (Kubernetes)\nBlast radius if migrating: High for incident response workflows. Every automated playbook needs rewriting in a new orchestration language.\n\nHonest take: Step Functions is the orchestration trap. ASL looks simple, but it's proprietary glue connecting proprietary services. If portability matters, use Temporal or Argo Workflows from day one. They're better tools anyway.",
      "tags": [
        "aws-proprietary"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-systems-manager",
      "nodeId": "systems-manager",
      "content": "<p><strong>Domains: 2, 3, 5</strong><br><br>Swiss army knife for operations. Key capabilities for the exam: Run Command (execute scripts on EC2 without SSH), Automation runbooks (incident response playbooks), Session Manager (secure shell access without opening ports \u2014 Domain 3/5), Patch Manager (automated patching \u2014 Domain 3), State Manager (desired state enforcement \u2014 Domain 1), Parameter Store (SecureString with KMS \u2014 Domain 5), OpsCenter (incident management \u2014 Domain 2). Agent-based \u2014 requires SSM Agent on instances.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udd34 AWS-Proprietary (High Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> High. SSM is deeply embedded in AWS operations. No single tool replaces all its capabilities.<br><strong>Control Plane Dependency:</strong> 5/5 \u2014 SSM agent, Run Command, Automation runbooks, Patch Manager, Session Manager \u2014 all AWS-proprietary.<br><strong>Data Gravity:</strong> 2/5 \u2014 Parameter Store data is exportable. Patch baselines and runbooks are configuration.<br><strong>Runtime Coupling:</strong> 4/5 \u2014 Applications using SSM Parameter Store, runbooks triggering from Config rules, and Session Manager for access are all AWS-coupled.<br><br><strong>Why migration is hard:</strong> Systems Manager is actually 15+ services in a trench coat: Run Command, Session Manager, Patch Manager, State Manager, Automation, Parameter Store, OpsCenter, Distributor, and more. Each would need a different replacement tool. Automation runbooks are AWS-specific YAML.<br><strong>Why migration could work:</strong> Individual capabilities have alternatives: Ansible (run command + patching), HashiCorp Vault (secrets), Teleport (session management). But you'd need 3-4 tools to replace one.<br><strong>Dependency risks:</strong> EC2 instances (SSM agent), Config rules (remediation), Inspector (patch scanning), Lambda (runbook steps), Parameter Store (app config).<br><strong>Alternatives:</strong> Ansible + Ansible Tower (run command, patching), HashiCorp Vault (secrets, config), Teleport (session management), Puppet/Chef (state management). No single alternative.<br><strong>Blast radius if migrating:</strong> High. SSM is woven into operations: patching, access, configuration, remediation. Replacing it requires multiple tools.<br><br><strong>Honest take:</strong> Systems Manager is AWS's most underrated lock-in vector. It looks like a convenience tool, but once your ops team depends on Session Manager for access, Patch Manager for compliance, and Parameter Store for config, you've locked in three different operational patterns simultaneously.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 2, 3, 5\n\nSwiss army knife for operations. Key capabilities for the exam: Run Command (execute scripts on EC2 without SSH), Automation runbooks (incident response playbooks), Session Manager (secure shell access without opening ports \u2014 Domain 3/5), Patch Manager (automated patching \u2014 Domain 3), State Manager (desired state enforcement \u2014 Domain 1), Parameter Store (SecureString with KMS \u2014 Domain 5), OpsCenter (incident management \u2014 Domain 2). Agent-based \u2014 requires SSM Agent on instances.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udd34 AWS-Proprietary (High Lock-In)\n\nMigration Difficulty: High. SSM is deeply embedded in AWS operations. No single tool replaces all its capabilities.\nControl Plane Dependency: 5/5 \u2014 SSM agent, Run Command, Automation runbooks, Patch Manager, Session Manager \u2014 all AWS-proprietary.\nData Gravity: 2/5 \u2014 Parameter Store data is exportable. Patch baselines and runbooks are configuration.\nRuntime Coupling: 4/5 \u2014 Applications using SSM Parameter Store, runbooks triggering from Config rules, and Session Manager for access are all AWS-coupled.\n\nWhy migration is hard: Systems Manager is actually 15+ services in a trench coat: Run Command, Session Manager, Patch Manager, State Manager, Automation, Parameter Store, OpsCenter, Distributor, and more. Each would need a different replacement tool. Automation runbooks are AWS-specific YAML.\nWhy migration could work: Individual capabilities have alternatives: Ansible (run command + patching), HashiCorp Vault (secrets), Teleport (session management). But you'd need 3-4 tools to replace one.\nDependency risks: EC2 instances (SSM agent), Config rules (remediation), Inspector (patch scanning), Lambda (runbook steps), Parameter Store (app config).\nAlternatives: Ansible + Ansible Tower (run command, patching), HashiCorp Vault (secrets, config), Teleport (session management), Puppet/Chef (state management). No single alternative.\nBlast radius if migrating: High. SSM is woven into operations: patching, access, configuration, remediation. Replacing it requires multiple tools.\n\nHonest take: Systems Manager is AWS's most underrated lock-in vector. It looks like a convenience tool, but once your ops team depends on Session Manager for access, Patch Manager for compliance, and Parameter Store for config, you've locked in three different operational patterns simultaneously.",
      "tags": [
        "aws-proprietary"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-api-gateway",
      "nodeId": "api-gateway",
      "content": "<p><strong>Domains: 3, 4, 5</strong><br><br>API management service. Security features: IAM authorization, Lambda authorizers, Cognito authorizers. Resource policies for cross-account access. API keys + usage plans for throttling. Mutual TLS (mTLS). WAF integration. Request validation. CloudWatch logging and X-Ray tracing. Domain 1: logging/troubleshooting (Skill 1.3.1).<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udfe1 Cloud-Portable (Medium Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Medium. OpenAPI spec is portable. AWS-specific integrations need rewiring.<br><strong>Control Plane Dependency:</strong> 4/5 \u2014 AWS-specific API management. But the concept of API gateways is universal.<br><strong>Data Gravity:</strong> 1/5 \u2014 API definitions can be exported as OpenAPI spec.<br><strong>Runtime Coupling:</strong> 4/5 \u2014 Lambda integrations, authorizers, and resource policies are AWS-specific. VTL mapping templates are proprietary.<br><br><strong>Why migration is hard:</strong> Lambda proxy integrations, Cognito/IAM authorizers, VTL mapping templates, usage plans, and resource policies are all AWS-specific. WebSocket APIs have proprietary connection management.<br><strong>Why migration could work:</strong> Export as OpenAPI spec. Import to Kong, Apigee, Azure API Management, or any API gateway. REST endpoints themselves are standard HTTP.<br><strong>Dependency risks:</strong> Lambda (backend), Cognito (authorization), WAF (protection), CloudWatch (logging), ACM (TLS).<br><strong>Alternatives:</strong> Kong (open-source, portable), Apigee (Google), Azure API Management, Tyk (open-source), Express Gateway<br><strong>Blast radius if migrating:</strong> Medium. API definitions are portable (OpenAPI). Backend integrations need rewiring.<br><br><strong>Honest take:</strong> API Gateway is moderately portable thanks to OpenAPI spec support. The lock-in is in Lambda proxy integrations and VTL templates. Use Kong if you want a portable API gateway \u2014 it's open-source and works everywhere.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 3, 4, 5\n\nAPI management service. Security features: IAM authorization, Lambda authorizers, Cognito authorizers. Resource policies for cross-account access. API keys + usage plans for throttling. Mutual TLS (mTLS). WAF integration. Request validation. CloudWatch logging and X-Ray tracing. Domain 1: logging/troubleshooting (Skill 1.3.1).\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udfe1 Cloud-Portable (Medium Lock-In)\n\nMigration Difficulty: Medium. OpenAPI spec is portable. AWS-specific integrations need rewiring.\nControl Plane Dependency: 4/5 \u2014 AWS-specific API management. But the concept of API gateways is universal.\nData Gravity: 1/5 \u2014 API definitions can be exported as OpenAPI spec.\nRuntime Coupling: 4/5 \u2014 Lambda integrations, authorizers, and resource policies are AWS-specific. VTL mapping templates are proprietary.\n\nWhy migration is hard: Lambda proxy integrations, Cognito/IAM authorizers, VTL mapping templates, usage plans, and resource policies are all AWS-specific. WebSocket APIs have proprietary connection management.\nWhy migration could work: Export as OpenAPI spec. Import to Kong, Apigee, Azure API Management, or any API gateway. REST endpoints themselves are standard HTTP.\nDependency risks: Lambda (backend), Cognito (authorization), WAF (protection), CloudWatch (logging), ACM (TLS).\nAlternatives: Kong (open-source, portable), Apigee (Google), Azure API Management, Tyk (open-source), Express Gateway\nBlast radius if migrating: Medium. API definitions are portable (OpenAPI). Backend integrations need rewiring.\n\nHonest take: API Gateway is moderately portable thanks to OpenAPI spec support. The lock-in is in Lambda proxy integrations and VTL templates. Use Kong if you want a portable API gateway \u2014 it's open-source and works everywhere.",
      "tags": [
        "cloud-portable"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-bedrock",
      "nodeId": "bedrock",
      "content": "<p><strong>Domains: 3</strong><br><br>Managed foundation model service. New C03 topic: Skill 3.2.7 \u2014 implement protections and guardrails for generative AI applications. GenAI OWASP Top 10 for LLM Applications. Bedrock Guardrails: content filters, denied topics, word filters, PII redaction. Model access via IAM. Data encryption. VPC endpoints for private access.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udd34 AWS-Proprietary (High Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Medium-High. The foundation models themselves are available from model providers directly. The AWS wrapper is the lock-in.<br><strong>Control Plane Dependency:</strong> 5/5 \u2014 AWS-proprietary AI service. Model access, guardrails, and agents are all AWS-specific.<br><strong>Data Gravity:</strong> 2/5 \u2014 Your data stays yours. But fine-tuned models may not be exportable depending on the base model.<br><strong>Runtime Coupling:</strong> 4/5 \u2014 Application calls Bedrock API. Guardrails configuration is proprietary.<br><br><strong>Why migration is hard:</strong> Bedrock Guardrails (content filters, PII redaction) are AWS-specific. Agent orchestration, knowledge bases, and model customization are proprietary features.<br><strong>Why migration could work:</strong> Foundation models (Claude, Llama, Mistral, Cohere) are available directly from their providers or through Azure/GCP. The models are portable; the orchestration layer is not.<br><strong>Dependency risks:</strong> IAM (model access), S3 (knowledge bases), Lambda (agent actions), CloudWatch (monitoring).<br><strong>Alternatives:</strong> Direct model provider APIs (Anthropic, OpenAI), Azure OpenAI Service, GCP Vertex AI, self-hosted open-source models (Llama via vLLM/Ollama)<br><strong>Blast radius if migrating:</strong> Medium. Switch API endpoints to direct provider or alternative cloud. Guardrails need reimplementation.<br><br><strong>Honest take:</strong> Bedrock's value is convenience (unified API for multiple models). The models themselves are available everywhere. If you're calling Claude, you can call Anthropic directly. The lock-in is in Bedrock-specific features (guardrails, agents, knowledge bases). Use LangChain or LiteLLM as abstraction layers to stay portable.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 3\n\nManaged foundation model service. New C03 topic: Skill 3.2.7 \u2014 implement protections and guardrails for generative AI applications. GenAI OWASP Top 10 for LLM Applications. Bedrock Guardrails: content filters, denied topics, word filters, PII redaction. Model access via IAM. Data encryption. VPC endpoints for private access.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udd34 AWS-Proprietary (High Lock-In)\n\nMigration Difficulty: Medium-High. The foundation models themselves are available from model providers directly. The AWS wrapper is the lock-in.\nControl Plane Dependency: 5/5 \u2014 AWS-proprietary AI service. Model access, guardrails, and agents are all AWS-specific.\nData Gravity: 2/5 \u2014 Your data stays yours. But fine-tuned models may not be exportable depending on the base model.\nRuntime Coupling: 4/5 \u2014 Application calls Bedrock API. Guardrails configuration is proprietary.\n\nWhy migration is hard: Bedrock Guardrails (content filters, PII redaction) are AWS-specific. Agent orchestration, knowledge bases, and model customization are proprietary features.\nWhy migration could work: Foundation models (Claude, Llama, Mistral, Cohere) are available directly from their providers or through Azure/GCP. The models are portable; the orchestration layer is not.\nDependency risks: IAM (model access), S3 (knowledge bases), Lambda (agent actions), CloudWatch (monitoring).\nAlternatives: Direct model provider APIs (Anthropic, OpenAI), Azure OpenAI Service, GCP Vertex AI, self-hosted open-source models (Llama via vLLM/Ollama)\nBlast radius if migrating: Medium. Switch API endpoints to direct provider or alternative cloud. Guardrails need reimplementation.\n\nHonest take: Bedrock's value is convenience (unified API for multiple models). The models themselves are available everywhere. If you're calling Claude, you can call Anthropic directly. The lock-in is in Bedrock-specific features (guardrails, agents, knowledge bases). Use LangChain or LiteLLM as abstraction layers to stay portable.",
      "tags": [
        "aws-proprietary"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-sagemaker",
      "nodeId": "sagemaker",
      "content": "<p><strong>Domains: 2, 5</strong><br><br>ML platform. SageMaker AI notebooks used in incident response for investigation/analysis (Domain 2 Skill 2.1.1). Nitro encryption for inter-node training traffic (Domain 5 Skill 5.1.3). VPC deployment, KMS encryption, IAM roles for training jobs.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udd34 AWS-Proprietary (High Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> High for the platform. Low for the models themselves.<br><strong>Control Plane Dependency:</strong> 5/5 \u2014 AWS-proprietary ML platform. Training jobs, endpoints, pipelines, Studio, and Feature Store are all AWS-specific.<br><strong>Data Gravity:</strong> 3/5 \u2014 Models are files (portable). Training data is in S3 (portable). But SageMaker-specific metadata (experiments, lineage) is trapped.<br><strong>Runtime Coupling:</strong> 4/5 \u2014 Model hosting on SageMaker endpoints is AWS-specific. Real-time inference calls target SageMaker endpoints.<br><br><strong>Why migration is hard:</strong> SageMaker Pipelines, Feature Store, Model Monitor, endpoint auto-scaling, and multi-model endpoints are all proprietary. Training jobs reference SageMaker-specific container images.<br><strong>Why migration could work:</strong> ML models are portable \u2014 they're files (ONNX, PyTorch, TensorFlow). Export the model, deploy on any serving platform. Training code (if using standard frameworks) runs anywhere.<br><strong>Dependency risks:</strong> S3 (training data, model artifacts), IAM (execution roles), KMS (encryption), VPC (deployment), ECR (custom containers).<br><strong>Alternatives:</strong> Databricks MLflow (multi-cloud), GCP Vertex AI, Azure ML, self-hosted MLflow + KServe, Hugging Face Inference Endpoints<br><strong>Blast radius if migrating:</strong> High for the ML platform. Low for individual models (export and re-deploy).<br><br><strong>Honest take:</strong> SageMaker is AWS's ML lock-in play. The models are portable (ONNX, PyTorch files). The platform (pipelines, feature store, endpoints) is not. Use MLflow as your experiment tracking and model registry \u2014 it works across all clouds and self-hosted.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 2, 5\n\nML platform. SageMaker AI notebooks used in incident response for investigation/analysis (Domain 2 Skill 2.1.1). Nitro encryption for inter-node training traffic (Domain 5 Skill 5.1.3). VPC deployment, KMS encryption, IAM roles for training jobs.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udd34 AWS-Proprietary (High Lock-In)\n\nMigration Difficulty: High for the platform. Low for the models themselves.\nControl Plane Dependency: 5/5 \u2014 AWS-proprietary ML platform. Training jobs, endpoints, pipelines, Studio, and Feature Store are all AWS-specific.\nData Gravity: 3/5 \u2014 Models are files (portable). Training data is in S3 (portable). But SageMaker-specific metadata (experiments, lineage) is trapped.\nRuntime Coupling: 4/5 \u2014 Model hosting on SageMaker endpoints is AWS-specific. Real-time inference calls target SageMaker endpoints.\n\nWhy migration is hard: SageMaker Pipelines, Feature Store, Model Monitor, endpoint auto-scaling, and multi-model endpoints are all proprietary. Training jobs reference SageMaker-specific container images.\nWhy migration could work: ML models are portable \u2014 they're files (ONNX, PyTorch, TensorFlow). Export the model, deploy on any serving platform. Training code (if using standard frameworks) runs anywhere.\nDependency risks: S3 (training data, model artifacts), IAM (execution roles), KMS (encryption), VPC (deployment), ECR (custom containers).\nAlternatives: Databricks MLflow (multi-cloud), GCP Vertex AI, Azure ML, self-hosted MLflow + KServe, Hugging Face Inference Endpoints\nBlast radius if migrating: High for the ML platform. Low for individual models (export and re-deploy).\n\nHonest take: SageMaker is AWS's ML lock-in play. The models are portable (ONNX, PyTorch files). The platform (pipelines, feature store, endpoints) is not. Use MLflow as your experiment tracking and model registry \u2014 it works across all clouds and self-hosted.",
      "tags": [
        "aws-proprietary"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-forensics-orchestrator",
      "nodeId": "forensics-orchestrator",
      "content": "<p><strong>Domains: 2</strong><br><br>AWS solution for automated forensic evidence collection from EC2 instances. Captures memory dumps, disk snapshots, instance metadata. Uses Step Functions for orchestration. New in C03 \u2014 specifically called out in incident response task 2.1.4.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udd34 AWS-Proprietary (High Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> High. This is an AWS solution template, not a portable tool.<br><strong>Control Plane Dependency:</strong> 5/5 \u2014 AWS-specific solution using Step Functions, Lambda, and EC2 APIs for forensic collection.<br><strong>Data Gravity:</strong> 2/5 \u2014 Forensic artifacts (EBS snapshots, memory dumps) are data you can export, but slowly.<br><strong>Runtime Coupling:</strong> 5/5 \u2014 Entirely built on AWS services. The orchestration IS the AWS dependency.<br><br><strong>Why migration is hard:</strong> It's literally a Step Functions workflow that calls EC2/EBS/Lambda APIs. Every component is AWS-specific.<br><strong>Why migration could work:</strong> Forensic collection is a process, not a product. You can build equivalent tooling with Velociraptor, GRR, or custom scripts on any platform.<br><strong>Dependency risks:</strong> Step Functions, Lambda, EC2, EBS, S3 \u2014 all AWS services.<br><strong>Alternatives:</strong> Velociraptor (open-source DFIR), GRR Rapid Response (Google, open-source), custom forensic scripts, CrowdStrike Falcon Forensics<br><strong>Blast radius if migrating:</strong> Low. This is an automation convenience, not a data store. Rebuild the process on new tooling.<br><br><strong>Honest take:</strong> This is a reference architecture, not a product. Build forensic tooling on open-source (Velociraptor) if you want portability. The forensic PROCESS is what matters, and that's portable.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 2\n\nAWS solution for automated forensic evidence collection from EC2 instances. Captures memory dumps, disk snapshots, instance metadata. Uses Step Functions for orchestration. New in C03 \u2014 specifically called out in incident response task 2.1.4.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udd34 AWS-Proprietary (High Lock-In)\n\nMigration Difficulty: High. This is an AWS solution template, not a portable tool.\nControl Plane Dependency: 5/5 \u2014 AWS-specific solution using Step Functions, Lambda, and EC2 APIs for forensic collection.\nData Gravity: 2/5 \u2014 Forensic artifacts (EBS snapshots, memory dumps) are data you can export, but slowly.\nRuntime Coupling: 5/5 \u2014 Entirely built on AWS services. The orchestration IS the AWS dependency.\n\nWhy migration is hard: It's literally a Step Functions workflow that calls EC2/EBS/Lambda APIs. Every component is AWS-specific.\nWhy migration could work: Forensic collection is a process, not a product. You can build equivalent tooling with Velociraptor, GRR, or custom scripts on any platform.\nDependency risks: Step Functions, Lambda, EC2, EBS, S3 \u2014 all AWS services.\nAlternatives: Velociraptor (open-source DFIR), GRR Rapid Response (Google, open-source), custom forensic scripts, CrowdStrike Falcon Forensics\nBlast radius if migrating: Low. This is an automation convenience, not a data store. Rebuild the process on new tooling.\n\nHonest take: This is a reference architecture, not a product. Build forensic tooling on open-source (Velociraptor) if you want portability. The forensic PROCESS is what matters, and that's portable.",
      "tags": [
        "aws-proprietary"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-fis",
      "nodeId": "fis",
      "content": "<p><strong>Domains: 2</strong><br><br>Chaos engineering service to test incident response plans. Inject failures into EC2, ECS, EKS, RDS. Validates that monitoring detects issues and runbooks execute correctly. New in C03 \u2014 tests and validates effectiveness of incident response plans.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udfe1 Cloud-Portable (Medium Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Medium. Chaos engineering tools are platform-specific by nature but the practices transfer.<br><strong>Control Plane Dependency:</strong> 4/5 \u2014 Experiment templates target AWS resource types. Fault actions are AWS-specific.<br><strong>Data Gravity:</strong> 1/5 \u2014 No significant data.<br><strong>Runtime Coupling:</strong> 2/5 \u2014 Used for testing, not production runtime.<br><br><strong>Why migration is hard:</strong> Experiment templates reference AWS resource ARNs and AWS-specific fault actions.<br><strong>Why migration could work:</strong> Chaos engineering is a practice, not a product. Gremlin, Litmus Chaos, and Chaos Monkey are multi-cloud.<br><strong>Dependency risks:</strong> EC2, ECS, EKS, RDS \u2014 as fault injection targets.<br><strong>Alternatives:</strong> Gremlin (multi-cloud), Litmus Chaos (Kubernetes), Chaos Monkey (Netflix, open-source), Steadybit<br><strong>Blast radius if migrating:</strong> Minimal. Testing tool. Replace with any chaos engineering platform.<br><br><strong>Honest take:</strong> FIS is a convenience wrapper. Use Gremlin or Litmus for multi-cloud chaos engineering. The resilience practices are what matter, and those are portable.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 2\n\nChaos engineering service to test incident response plans. Inject failures into EC2, ECS, EKS, RDS. Validates that monitoring detects issues and runbooks execute correctly. New in C03 \u2014 tests and validates effectiveness of incident response plans.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udfe1 Cloud-Portable (Medium Lock-In)\n\nMigration Difficulty: Medium. Chaos engineering tools are platform-specific by nature but the practices transfer.\nControl Plane Dependency: 4/5 \u2014 Experiment templates target AWS resource types. Fault actions are AWS-specific.\nData Gravity: 1/5 \u2014 No significant data.\nRuntime Coupling: 2/5 \u2014 Used for testing, not production runtime.\n\nWhy migration is hard: Experiment templates reference AWS resource ARNs and AWS-specific fault actions.\nWhy migration could work: Chaos engineering is a practice, not a product. Gremlin, Litmus Chaos, and Chaos Monkey are multi-cloud.\nDependency risks: EC2, ECS, EKS, RDS \u2014 as fault injection targets.\nAlternatives: Gremlin (multi-cloud), Litmus Chaos (Kubernetes), Chaos Monkey (Netflix, open-source), Steadybit\nBlast radius if migrating: Minimal. Testing tool. Replace with any chaos engineering platform.\n\nHonest take: FIS is a convenience wrapper. Use Gremlin or Litmus for multi-cloud chaos engineering. The resilience practices are what matter, and those are portable.",
      "tags": [
        "cloud-portable"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-resilience-hub",
      "nodeId": "resilience-hub",
      "content": "<p><strong>Domains: 2</strong><br><br>Assesses application resilience against RTO/RPO targets. Identifies resilience weaknesses. Recommends improvements. New in C03 \u2014 validates incident response and recovery readiness.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udfe1 Cloud-Portable (Medium Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Low. This is an assessment tool. Your RTO/RPO requirements are your own.<br><strong>Control Plane Dependency:</strong> 4/5 \u2014 Evaluates AWS-specific resources against RTO/RPO targets.<br><strong>Data Gravity:</strong> 1/5 \u2014 No significant data stored.<br><strong>Runtime Coupling:</strong> 1/5 \u2014 Assessment tool, not runtime dependency.<br><br><strong>Why migration is hard:</strong> Recommendations are AWS-specific (e.g., 'enable Multi-AZ on RDS', 'add cross-region replication').<br><strong>Why migration could work:</strong> Resilience requirements are platform-agnostic. Any cloud has DR best practices.<br><strong>Dependency risks:</strong> AWS resources being assessed \u2014 CloudFormation stacks, Terraform state files.<br><strong>Alternatives:</strong> Zerto, Veeam, custom DR assessment frameworks, Azure Site Recovery planner<br><strong>Blast radius if migrating:</strong> None. This is advisory tooling.<br><br><strong>Honest take:</strong> Assessment tool. Not a lock-in risk. Your resilience strategy is yours regardless of platform.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 2\n\nAssesses application resilience against RTO/RPO targets. Identifies resilience weaknesses. Recommends improvements. New in C03 \u2014 validates incident response and recovery readiness.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udfe1 Cloud-Portable (Medium Lock-In)\n\nMigration Difficulty: Low. This is an assessment tool. Your RTO/RPO requirements are your own.\nControl Plane Dependency: 4/5 \u2014 Evaluates AWS-specific resources against RTO/RPO targets.\nData Gravity: 1/5 \u2014 No significant data stored.\nRuntime Coupling: 1/5 \u2014 Assessment tool, not runtime dependency.\n\nWhy migration is hard: Recommendations are AWS-specific (e.g., 'enable Multi-AZ on RDS', 'add cross-region replication').\nWhy migration could work: Resilience requirements are platform-agnostic. Any cloud has DR best practices.\nDependency risks: AWS resources being assessed \u2014 CloudFormation stacks, Terraform state files.\nAlternatives: Zerto, Veeam, custom DR assessment frameworks, Azure Site Recovery planner\nBlast radius if migrating: None. This is advisory tooling.\n\nHonest take: Assessment tool. Not a lock-in risk. Your resilience strategy is yours regardless of platform.",
      "tags": [
        "cloud-portable"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-app-recovery-controller",
      "nodeId": "app-recovery-controller",
      "content": "<p><strong>Domains: 2</strong><br><br>Manages application failover across Regions and AZs. Routing controls and readiness checks. Used in disaster recovery scenarios \u2014 critical for incident response recovery phase.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udd34 AWS-Proprietary (High Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> High. Multi-Region failover orchestration is deeply platform-specific.<br><strong>Control Plane Dependency:</strong> 5/5 \u2014 AWS-proprietary multi-Region failover routing. Readiness checks evaluate AWS resources.<br><strong>Data Gravity:</strong> 1/5 \u2014 No significant data.<br><strong>Runtime Coupling:</strong> 4/5 \u2014 Routing controls directly affect traffic flow during failover.<br><br><strong>Why migration is hard:</strong> Routing controls, readiness checks, and recovery groups all reference AWS resources (Route 53, ALB). Failover logic is AWS-specific.<br><strong>Why migration could work:</strong> DR orchestration exists elsewhere \u2014 Azure Site Recovery, GCP regional failover. The failover strategy is a design, not a product.<br><strong>Dependency risks:</strong> Route 53 (DNS failover), ALB (health checks), EC2/RDS (resources being failed over).<br><strong>Alternatives:</strong> Azure Site Recovery, GCP regional failover, DNS-based failover (Cloudflare, NS1), custom failover scripts<br><strong>Blast radius if migrating:</strong> Medium. Critical during actual failover, but replaceable with DNS-based approaches.<br><br><strong>Honest take:</strong> Only matters if you're doing active-active multi-Region on AWS. If you're going multi-cloud, you need a cloud-agnostic failover strategy anyway.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 2\n\nManages application failover across Regions and AZs. Routing controls and readiness checks. Used in disaster recovery scenarios \u2014 critical for incident response recovery phase.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udd34 AWS-Proprietary (High Lock-In)\n\nMigration Difficulty: High. Multi-Region failover orchestration is deeply platform-specific.\nControl Plane Dependency: 5/5 \u2014 AWS-proprietary multi-Region failover routing. Readiness checks evaluate AWS resources.\nData Gravity: 1/5 \u2014 No significant data.\nRuntime Coupling: 4/5 \u2014 Routing controls directly affect traffic flow during failover.\n\nWhy migration is hard: Routing controls, readiness checks, and recovery groups all reference AWS resources (Route 53, ALB). Failover logic is AWS-specific.\nWhy migration could work: DR orchestration exists elsewhere \u2014 Azure Site Recovery, GCP regional failover. The failover strategy is a design, not a product.\nDependency risks: Route 53 (DNS failover), ALB (health checks), EC2/RDS (resources being failed over).\nAlternatives: Azure Site Recovery, GCP regional failover, DNS-based failover (Cloudflare, NS1), custom failover scripts\nBlast radius if migrating: Medium. Critical during actual failover, but replaceable with DNS-based approaches.\n\nHonest take: Only matters if you're doing active-active multi-Region on AWS. If you're going multi-cloud, you need a cloud-agnostic failover strategy anyway.",
      "tags": [
        "aws-proprietary"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-waf",
      "nodeId": "waf",
      "content": "<p><strong>Domains: 3</strong><br><br>Web application firewall. Attach to CloudFront, ALB, API Gateway, AppSync, Cognito. Rule types: rate-based, IP set, regex, geo-match, SQL injection, XSS, custom. Managed rule groups (AWS + Marketplace) \u2014 including Bot Control and Account Takeover Prevention. OWASP Top 10 protection (explicitly tested). Third-party WAF rules via Marketplace (new C03). Logging to S3, CloudWatch, Kinesis Data Firehose. Firewall Manager deploys WAF rules at scale.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udfe1 Cloud-Portable (Medium Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Medium. WAF rules need rewriting but the concepts are universal.<br><strong>Control Plane Dependency:</strong> 4/5 \u2014 WAF rules are AWS-specific JSON. Rule groups, IP sets, and managed rules are AWS-proprietary.<br><strong>Data Gravity:</strong> 1/5 \u2014 Rules are configuration, not data.<br><strong>Runtime Coupling:</strong> 3/5 \u2014 Attached to CloudFront, ALB, API Gateway. Rule evaluation is AWS-specific.<br><br><strong>Why migration is hard:</strong> AWS WAF rule syntax is proprietary. Managed rule groups (AWS and Marketplace) don't transfer. IP sets and regex patterns need recreation. Firewall Manager policy deployment is AWS-specific.<br><strong>Why migration could work:</strong> WAF is a mature market. Cloudflare WAF, Azure WAF, GCP Cloud Armor, Imperva, Akamai \u2014 all provide equivalent rule capabilities. OWASP rules are universal concepts.<br><strong>Dependency risks:</strong> CloudFront, ALB, API Gateway (attachment points), Firewall Manager (centralized deployment), Shield (DDoS integration).<br><strong>Alternatives:</strong> Cloudflare WAF (excellent + free tier), Azure WAF, GCP Cloud Armor, Imperva WAF, ModSecurity (open-source)<br><strong>Blast radius if migrating:</strong> Medium. Rules need rewriting. Managed rule subscriptions lost. But WAF migration is a well-understood process.<br><br><strong>Honest take:</strong> AWS WAF is fine but overpriced. Cloudflare's free WAF is arguably better for most web applications. The lock-in is moderate \u2014 rules are configuration, not code. Budget 1-2 weeks to rewrite rules on a new platform.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 3\n\nWeb application firewall. Attach to CloudFront, ALB, API Gateway, AppSync, Cognito. Rule types: rate-based, IP set, regex, geo-match, SQL injection, XSS, custom. Managed rule groups (AWS + Marketplace) \u2014 including Bot Control and Account Takeover Prevention. OWASP Top 10 protection (explicitly tested). Third-party WAF rules via Marketplace (new C03). Logging to S3, CloudWatch, Kinesis Data Firehose. Firewall Manager deploys WAF rules at scale.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udfe1 Cloud-Portable (Medium Lock-In)\n\nMigration Difficulty: Medium. WAF rules need rewriting but the concepts are universal.\nControl Plane Dependency: 4/5 \u2014 WAF rules are AWS-specific JSON. Rule groups, IP sets, and managed rules are AWS-proprietary.\nData Gravity: 1/5 \u2014 Rules are configuration, not data.\nRuntime Coupling: 3/5 \u2014 Attached to CloudFront, ALB, API Gateway. Rule evaluation is AWS-specific.\n\nWhy migration is hard: AWS WAF rule syntax is proprietary. Managed rule groups (AWS and Marketplace) don't transfer. IP sets and regex patterns need recreation. Firewall Manager policy deployment is AWS-specific.\nWhy migration could work: WAF is a mature market. Cloudflare WAF, Azure WAF, GCP Cloud Armor, Imperva, Akamai \u2014 all provide equivalent rule capabilities. OWASP rules are universal concepts.\nDependency risks: CloudFront, ALB, API Gateway (attachment points), Firewall Manager (centralized deployment), Shield (DDoS integration).\nAlternatives: Cloudflare WAF (excellent + free tier), Azure WAF, GCP Cloud Armor, Imperva WAF, ModSecurity (open-source)\nBlast radius if migrating: Medium. Rules need rewriting. Managed rule subscriptions lost. But WAF migration is a well-understood process.\n\nHonest take: AWS WAF is fine but overpriced. Cloudflare's free WAF is arguably better for most web applications. The lock-in is moderate \u2014 rules are configuration, not code. Budget 1-2 weeks to rewrite rules on a new platform.",
      "tags": [
        "cloud-portable"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-cloudfront",
      "nodeId": "cloudfront",
      "content": "<p><strong>Domains: 3, 1</strong><br><br>Global CDN. Origin Access Control (OAC) for S3 \u2014 replaces legacy OAI. TLS/SSL certificate management via ACM. Custom headers for origin validation. Field-level encryption for sensitive form data. Geo-restriction. Signed URLs and signed cookies for private content. CloudFront Functions and Lambda@Edge for request/response manipulation. Access logs to S3. Integration with WAF and Shield for edge protection. CORS headers configuration (Domain 3 edge security).<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udfe1 Cloud-Portable (Medium Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Medium. CDN migration is straightforward unless you depend on edge compute.<br><strong>Control Plane Dependency:</strong> 3/5 \u2014 CDN configuration is cloud-specific but the concept is universal. Origin Access Control, cache behaviors are AWS-specific.<br><strong>Data Gravity:</strong> 1/5 \u2014 CDN caches content, stores nothing permanently.<br><strong>Runtime Coupling:</strong> 3/5 \u2014 Lambda@Edge and CloudFront Functions are AWS-specific edge compute. If you use them, lock-in increases significantly.<br><br><strong>Why migration is hard:</strong> Lambda@Edge/CloudFront Functions lock you into AWS edge compute. OAC configuration is AWS-specific. Cache invalidation patterns differ across CDNs. Field-level encryption is AWS-proprietary.<br><strong>Why migration could work:</strong> CDN is a commodity: Cloudflare, Akamai, Fastly, Azure CDN, GCP Cloud CDN. CNAME switch is how you migrate. If you're just caching, it's trivial.<br><strong>Dependency risks:</strong> S3 (origin), ACM (certificates), WAF (protection), Route 53 (DNS), Shield (DDoS).<br><strong>Alternatives:</strong> Cloudflare (best free tier in industry), Akamai, Fastly, Azure Front Door, GCP Cloud CDN<br><strong>Blast radius if migrating:</strong> Low if just caching. High if using Lambda@Edge extensively.<br><br><strong>Honest take:</strong> CloudFront is a fine CDN but Cloudflare is better and has a more generous free tier. The only meaningful lock-in is Lambda@Edge \u2014 if you've put business logic at the edge, you're coupled. Avoid CloudFront Functions and Lambda@Edge if portability matters.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 3, 1\n\nGlobal CDN. Origin Access Control (OAC) for S3 \u2014 replaces legacy OAI. TLS/SSL certificate management via ACM. Custom headers for origin validation. Field-level encryption for sensitive form data. Geo-restriction. Signed URLs and signed cookies for private content. CloudFront Functions and Lambda@Edge for request/response manipulation. Access logs to S3. Integration with WAF and Shield for edge protection. CORS headers configuration (Domain 3 edge security).\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udfe1 Cloud-Portable (Medium Lock-In)\n\nMigration Difficulty: Medium. CDN migration is straightforward unless you depend on edge compute.\nControl Plane Dependency: 3/5 \u2014 CDN configuration is cloud-specific but the concept is universal. Origin Access Control, cache behaviors are AWS-specific.\nData Gravity: 1/5 \u2014 CDN caches content, stores nothing permanently.\nRuntime Coupling: 3/5 \u2014 Lambda@Edge and CloudFront Functions are AWS-specific edge compute. If you use them, lock-in increases significantly.\n\nWhy migration is hard: Lambda@Edge/CloudFront Functions lock you into AWS edge compute. OAC configuration is AWS-specific. Cache invalidation patterns differ across CDNs. Field-level encryption is AWS-proprietary.\nWhy migration could work: CDN is a commodity: Cloudflare, Akamai, Fastly, Azure CDN, GCP Cloud CDN. CNAME switch is how you migrate. If you're just caching, it's trivial.\nDependency risks: S3 (origin), ACM (certificates), WAF (protection), Route 53 (DNS), Shield (DDoS).\nAlternatives: Cloudflare (best free tier in industry), Akamai, Fastly, Azure Front Door, GCP Cloud CDN\nBlast radius if migrating: Low if just caching. High if using Lambda@Edge extensively.\n\nHonest take: CloudFront is a fine CDN but Cloudflare is better and has a more generous free tier. The only meaningful lock-in is Lambda@Edge \u2014 if you've put business logic at the edge, you're coupled. Avoid CloudFront Functions and Lambda@Edge if portability matters.",
      "tags": [
        "cloud-portable"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-route53",
      "nodeId": "route53",
      "content": "<p><strong>Domains: 3, 1</strong><br><br>DNS service with DNSSEC signing for domain integrity. Health checks and DNS failover. Routing policies: simple, weighted, latency, failover, geolocation, geoproximity, multivalue. Resolver DNS Firewall \u2014 block DNS queries to known-bad domains (data exfiltration prevention). Resolver query logging \u2014 critical log source for Domain 1 detection. Transit gateway flow logs capture cross-VPC DNS traffic.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udfe1 Cloud-Portable (Medium Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Low-Medium. DNS is the most portable thing in IT. Zone file export \u2192 import elsewhere.<br><strong>Control Plane Dependency:</strong> 3/5 \u2014 DNS is a standard protocol. But Route 53-specific features (alias records, health checks, Resolver DNS Firewall) are AWS-proprietary.<br><strong>Data Gravity:</strong> 1/5 \u2014 DNS zones are exportable as BIND zone files.<br><strong>Runtime Coupling:</strong> 2/5 \u2014 DNS is DNS. Unless you use alias records (AWS-specific) or advanced routing policies.<br><br><strong>Why migration is hard:</strong> Alias records (Route 53-specific, point to AWS resources without CNAME penalties), geolocation/geoproximity routing policies, health check configurations, and Resolver DNS Firewall rules are AWS-specific.<br><strong>Why migration could work:</strong> DNS zones export as standard zone files. Every DNS provider accepts them. Migration is literally an NS record change.<br><strong>Dependency risks:</strong> CloudFront (alias records), ELB (alias records), S3 (static website alias), Resolver DNS Firewall (security).<br><strong>Alternatives:</strong> Cloudflare DNS (free, fastest), GCP Cloud DNS, Azure DNS, NS1, Dyn, self-hosted BIND/PowerDNS<br><strong>Blast radius if migrating:</strong> Low. DNS migration is well-understood. Careful TTL management required.<br><br><strong>Honest take:</strong> Route 53 is competent but unremarkable. Cloudflare DNS is faster and free. The only stickiness is alias records (which are admittedly convenient for pointing to ALBs and CloudFront). DNS is always portable.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 3, 1\n\nDNS service with DNSSEC signing for domain integrity. Health checks and DNS failover. Routing policies: simple, weighted, latency, failover, geolocation, geoproximity, multivalue. Resolver DNS Firewall \u2014 block DNS queries to known-bad domains (data exfiltration prevention). Resolver query logging \u2014 critical log source for Domain 1 detection. Transit gateway flow logs capture cross-VPC DNS traffic.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udfe1 Cloud-Portable (Medium Lock-In)\n\nMigration Difficulty: Low-Medium. DNS is the most portable thing in IT. Zone file export \u2192 import elsewhere.\nControl Plane Dependency: 3/5 \u2014 DNS is a standard protocol. But Route 53-specific features (alias records, health checks, Resolver DNS Firewall) are AWS-proprietary.\nData Gravity: 1/5 \u2014 DNS zones are exportable as BIND zone files.\nRuntime Coupling: 2/5 \u2014 DNS is DNS. Unless you use alias records (AWS-specific) or advanced routing policies.\n\nWhy migration is hard: Alias records (Route 53-specific, point to AWS resources without CNAME penalties), geolocation/geoproximity routing policies, health check configurations, and Resolver DNS Firewall rules are AWS-specific.\nWhy migration could work: DNS zones export as standard zone files. Every DNS provider accepts them. Migration is literally an NS record change.\nDependency risks: CloudFront (alias records), ELB (alias records), S3 (static website alias), Resolver DNS Firewall (security).\nAlternatives: Cloudflare DNS (free, fastest), GCP Cloud DNS, Azure DNS, NS1, Dyn, self-hosted BIND/PowerDNS\nBlast radius if migrating: Low. DNS migration is well-understood. Careful TTL management required.\n\nHonest take: Route 53 is competent but unremarkable. Cloudflare DNS is faster and free. The only stickiness is alias records (which are admittedly convenient for pointing to ALBs and CloudFront). DNS is always portable.",
      "tags": [
        "cloud-portable"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-elb",
      "nodeId": "elb",
      "content": "<p><strong>Domains: 3, 5</strong><br><br>ALB (Layer 7): WAF integration, OIDC authentication, access logs, redirect HTTP\u2192HTTPS. NLB (Layer 4): TLS termination, static IPs, preserves source IP. Security policies control TLS cipher suites (Domain 5 \u2014 data in transit). Access logs to S3 for analysis. Target group health checks. Mutual TLS (mTLS) with ACM Private CA.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udfe1 Cloud-Portable (Medium Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Medium. Load balancing is a commodity. Configuration recreation is the work.<br><strong>Control Plane Dependency:</strong> 3/5 \u2014 Load balancing concepts are universal. Listener rules, target group configs, and security policies are AWS-specific.<br><strong>Data Gravity:</strong> 1/5 \u2014 No persistent data. Access logs go to S3.<br><strong>Runtime Coupling:</strong> 3/5 \u2014 Applications reference ALB/NLB endpoints. OIDC authentication integration is AWS-specific.<br><br><strong>Why migration is hard:</strong> ALB authentication integration (Cognito, OIDC), listener rules with path-based routing, WAF attachment, and access log formats are AWS-specific. NLB's static IPs and PrivateLink integration are unique.<br><strong>Why migration could work:</strong> HAProxy, Nginx, Envoy, Traefik \u2014 load balancing is the most commoditized infrastructure service. Azure App Gateway and GCP Cloud Load Balancing are direct equivalents.<br><strong>Dependency risks:</strong> WAF (ALB attachment), ACM (certificates), CloudWatch (metrics), Target groups (EC2, Lambda, IP).<br><strong>Alternatives:</strong> Azure Application Gateway, GCP Cloud Load Balancing, Nginx/HAProxy (self-hosted), Envoy, Traefik, Cloudflare Load Balancing<br><strong>Blast radius if migrating:</strong> Medium. Endpoint URLs change. Health check configurations need recreation. But the concept is identical everywhere.<br><br><strong>Honest take:</strong> Load balancing is not a lock-in risk. It's infrastructure plumbing that exists everywhere. The AWS-specific convenience features (ALB auth, WAF attachment) are nice but not irreplaceable.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 3, 5\n\nALB (Layer 7): WAF integration, OIDC authentication, access logs, redirect HTTP\u2192HTTPS. NLB (Layer 4): TLS termination, static IPs, preserves source IP. Security policies control TLS cipher suites (Domain 5 \u2014 data in transit). Access logs to S3 for analysis. Target group health checks. Mutual TLS (mTLS) with ACM Private CA.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udfe1 Cloud-Portable (Medium Lock-In)\n\nMigration Difficulty: Medium. Load balancing is a commodity. Configuration recreation is the work.\nControl Plane Dependency: 3/5 \u2014 Load balancing concepts are universal. Listener rules, target group configs, and security policies are AWS-specific.\nData Gravity: 1/5 \u2014 No persistent data. Access logs go to S3.\nRuntime Coupling: 3/5 \u2014 Applications reference ALB/NLB endpoints. OIDC authentication integration is AWS-specific.\n\nWhy migration is hard: ALB authentication integration (Cognito, OIDC), listener rules with path-based routing, WAF attachment, and access log formats are AWS-specific. NLB's static IPs and PrivateLink integration are unique.\nWhy migration could work: HAProxy, Nginx, Envoy, Traefik \u2014 load balancing is the most commoditized infrastructure service. Azure App Gateway and GCP Cloud Load Balancing are direct equivalents.\nDependency risks: WAF (ALB attachment), ACM (certificates), CloudWatch (metrics), Target groups (EC2, Lambda, IP).\nAlternatives: Azure Application Gateway, GCP Cloud Load Balancing, Nginx/HAProxy (self-hosted), Envoy, Traefik, Cloudflare Load Balancing\nBlast radius if migrating: Medium. Endpoint URLs change. Health check configurations need recreation. But the concept is identical everywhere.\n\nHonest take: Load balancing is not a lock-in risk. It's infrastructure plumbing that exists everywhere. The AWS-specific convenience features (ALB auth, WAF attachment) are nice but not irreplaceable.",
      "tags": [
        "cloud-portable"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-network-firewall",
      "nodeId": "network-firewall",
      "content": "<p><strong>Domains: 3</strong><br><br>Managed stateful firewall for VPCs. Layer 3-7 filtering. Suricata-compatible IDS/IPS rules. Deep packet inspection. TLS inspection for encrypted traffic. Centralize via Firewall Manager across accounts. Deployed in dedicated firewall subnets. Alert and flow logs to S3, CloudWatch, Kinesis. North/south (internet) and east/west (inter-VPC) traffic protection.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udfe1 Cloud-Portable (Medium Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Medium. Suricata rules are portable. AWS deployment architecture is not.<br><strong>Control Plane Dependency:</strong> 4/5 \u2014 Managed firewall with AWS-specific deployment model. But Suricata rule format is open standard.<br><strong>Data Gravity:</strong> 1/5 \u2014 Rules are configuration.<br><strong>Runtime Coupling:</strong> 3/5 \u2014 Suricata-compatible rules are portable. AWS deployment model (firewall endpoints, routing) is not.<br><br><strong>Why migration is hard:</strong> Firewall endpoint deployment, VPC routing integration, Firewall Manager centralized policy, and TLS inspection configuration are AWS-specific. The 'how you deploy it' doesn't transfer.<br><strong>Why migration could work:</strong> Suricata rules are an open standard. Export them and import into any Suricata-compatible firewall. Palo Alto, Fortinet, and pfSense all support Suricata rules.<br><strong>Dependency risks:</strong> VPC routing (traffic flows through firewall endpoints), Firewall Manager (centralized deployment), S3/CloudWatch (logging).<br><strong>Alternatives:</strong> Palo Alto VM-Series (multi-cloud), Fortinet FortiGate, pfSense/OPNsense (self-hosted), Azure Firewall, GCP Cloud Firewall<br><strong>Blast radius if migrating:</strong> Medium. Suricata rules are portable. Network topology changes are the hard part.<br><br><strong>Honest take:</strong> AWS Network Firewall's saving grace is Suricata compatibility. Your IDS/IPS rules are portable. The deployment model isn't, but that's true of any cloud-managed firewall. This is about as portable as a managed firewall can be.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 3\n\nManaged stateful firewall for VPCs. Layer 3-7 filtering. Suricata-compatible IDS/IPS rules. Deep packet inspection. TLS inspection for encrypted traffic. Centralize via Firewall Manager across accounts. Deployed in dedicated firewall subnets. Alert and flow logs to S3, CloudWatch, Kinesis. North/south (internet) and east/west (inter-VPC) traffic protection.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udfe1 Cloud-Portable (Medium Lock-In)\n\nMigration Difficulty: Medium. Suricata rules are portable. AWS deployment architecture is not.\nControl Plane Dependency: 4/5 \u2014 Managed firewall with AWS-specific deployment model. But Suricata rule format is open standard.\nData Gravity: 1/5 \u2014 Rules are configuration.\nRuntime Coupling: 3/5 \u2014 Suricata-compatible rules are portable. AWS deployment model (firewall endpoints, routing) is not.\n\nWhy migration is hard: Firewall endpoint deployment, VPC routing integration, Firewall Manager centralized policy, and TLS inspection configuration are AWS-specific. The 'how you deploy it' doesn't transfer.\nWhy migration could work: Suricata rules are an open standard. Export them and import into any Suricata-compatible firewall. Palo Alto, Fortinet, and pfSense all support Suricata rules.\nDependency risks: VPC routing (traffic flows through firewall endpoints), Firewall Manager (centralized deployment), S3/CloudWatch (logging).\nAlternatives: Palo Alto VM-Series (multi-cloud), Fortinet FortiGate, pfSense/OPNsense (self-hosted), Azure Firewall, GCP Cloud Firewall\nBlast radius if migrating: Medium. Suricata rules are portable. Network topology changes are the hard part.\n\nHonest take: AWS Network Firewall's saving grace is Suricata compatibility. Your IDS/IPS rules are portable. The deployment model isn't, but that's true of any cloud-managed firewall. This is about as portable as a managed firewall can be.",
      "tags": [
        "cloud-portable"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-direct-connect",
      "nodeId": "direct-connect",
      "content": "<p><strong>Domains: 3, 5</strong><br><br>Dedicated private network connection to AWS. Does NOT encrypt by default. MACsec (Layer 2 encryption) for dedicated connections \u2014 new emphasis in C03. Virtual interfaces: private VIF (VPC), public VIF (public AWS services), transit VIF (Transit Gateway). Combine with VPN for encrypted overlay. Domain 5: secure connectivity for data in transit.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udd34 AWS-Proprietary (High Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> High. Physical infrastructure with 1-3 month lead times. You're literally cabling to AWS.<br><strong>Control Plane Dependency:</strong> 4/5 \u2014 Physical cross-connect to AWS data centers. AWS-specific LAG, VIF, and gateway configurations.<br><strong>Data Gravity:</strong> 1/5 \u2014 Physical connection, no data stored.<br><strong>Runtime Coupling:</strong> 3/5 \u2014 Network traffic flows over physical infrastructure. MACsec configuration is AWS-specific.<br><br><strong>Why migration is hard:</strong> Physical cross-connects in co-location facilities are vendor-specific. Virtual interfaces, Direct Connect Gateways, and MACsec configurations are AWS-proprietary. Lead times for new connections are weeks to months.<br><strong>Why migration could work:</strong> Most co-location facilities (Equinix, CyrusOne) support connections to multiple clouds. You can run parallel connections to AWS and Azure/GCP from the same cage.<br><strong>Dependency risks:</strong> VPC (private VIF), Transit Gateway (transit VIF), Route 53 (public VIF), VPN (encrypted overlay).<br><strong>Alternatives:</strong> Azure ExpressRoute, GCP Cloud Interconnect, Megaport/PacketFabric (multi-cloud network fabric)<br><strong>Blast radius if migrating:</strong> High due to physical lead times. Plan 2-3 months for alternative connectivity.<br><br><strong>Honest take:</strong> Direct Connect is physical lock-in \u2014 you're literally plugging a cable into AWS. The mitigation is co-location facilities that support multi-cloud connections (Equinix, Megaport). Run connections to multiple clouds in parallel if portability matters.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 3, 5\n\nDedicated private network connection to AWS. Does NOT encrypt by default. MACsec (Layer 2 encryption) for dedicated connections \u2014 new emphasis in C03. Virtual interfaces: private VIF (VPC), public VIF (public AWS services), transit VIF (Transit Gateway). Combine with VPN for encrypted overlay. Domain 5: secure connectivity for data in transit.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udd34 AWS-Proprietary (High Lock-In)\n\nMigration Difficulty: High. Physical infrastructure with 1-3 month lead times. You're literally cabling to AWS.\nControl Plane Dependency: 4/5 \u2014 Physical cross-connect to AWS data centers. AWS-specific LAG, VIF, and gateway configurations.\nData Gravity: 1/5 \u2014 Physical connection, no data stored.\nRuntime Coupling: 3/5 \u2014 Network traffic flows over physical infrastructure. MACsec configuration is AWS-specific.\n\nWhy migration is hard: Physical cross-connects in co-location facilities are vendor-specific. Virtual interfaces, Direct Connect Gateways, and MACsec configurations are AWS-proprietary. Lead times for new connections are weeks to months.\nWhy migration could work: Most co-location facilities (Equinix, CyrusOne) support connections to multiple clouds. You can run parallel connections to AWS and Azure/GCP from the same cage.\nDependency risks: VPC (private VIF), Transit Gateway (transit VIF), Route 53 (public VIF), VPN (encrypted overlay).\nAlternatives: Azure ExpressRoute, GCP Cloud Interconnect, Megaport/PacketFabric (multi-cloud network fabric)\nBlast radius if migrating: High due to physical lead times. Plan 2-3 months for alternative connectivity.\n\nHonest take: Direct Connect is physical lock-in \u2014 you're literally plugging a cable into AWS. The mitigation is co-location facilities that support multi-cloud connections (Equinix, Megaport). Run connections to multiple clouds in parallel if portability matters.",
      "tags": [
        "aws-proprietary"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-vpn",
      "nodeId": "vpn",
      "content": "<p><strong>Domains: 3, 5</strong><br><br>Site-to-Site VPN: IPSec tunnels over internet to VPC or Transit Gateway. Client VPN: OpenVPN-based remote access with AD/SAML authentication. Domain 5: encrypted private connectivity (PrivateLink, VPC endpoints as alternatives). Accelerated Site-to-Site VPN uses Global Accelerator for improved performance.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udfe2 Platform-Agnostic (Low Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Low. VPN is standard networking. Point your tunnels to new endpoints.<br><strong>Control Plane Dependency:</strong> 2/5 \u2014 IPSec VPN is an open standard. AWS-specific: VPN gateway management and accelerated VPN.<br><strong>Data Gravity:</strong> 1/5 \u2014 No data stored.<br><strong>Runtime Coupling:</strong> 1/5 \u2014 IPSec is IPSec. Works with any VPN endpoint.<br><br><strong>Why migration is hard:</strong> AWS VPN configuration (customer gateway, virtual private gateway, transit gateway attachment) is AWS-specific. But the tunnels themselves are standard IPSec.<br><strong>Why migration could work:</strong> IPSec VPN works with any endpoint: other clouds, on-prem firewalls, open-source (StrongSwan, WireGuard). Reconfigure tunnels in hours.<br><strong>Dependency risks:</strong> VPC or Transit Gateway (attachment), Route 53 (DNS resolution).<br><strong>Alternatives:</strong> Any IPSec-capable device/service. Azure VPN Gateway, GCP Cloud VPN, StrongSwan, WireGuard<br><strong>Blast radius if migrating:</strong> Low. Standard protocol. Reconfigure in hours.<br><br><strong>Honest take:</strong> VPN is genuinely portable because IPSec is a standard protocol. AWS gets no lock-in from VPN. Use it freely.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 3, 5\n\nSite-to-Site VPN: IPSec tunnels over internet to VPC or Transit Gateway. Client VPN: OpenVPN-based remote access with AD/SAML authentication. Domain 5: encrypted private connectivity (PrivateLink, VPC endpoints as alternatives). Accelerated Site-to-Site VPN uses Global Accelerator for improved performance.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udfe2 Platform-Agnostic (Low Lock-In)\n\nMigration Difficulty: Low. VPN is standard networking. Point your tunnels to new endpoints.\nControl Plane Dependency: 2/5 \u2014 IPSec VPN is an open standard. AWS-specific: VPN gateway management and accelerated VPN.\nData Gravity: 1/5 \u2014 No data stored.\nRuntime Coupling: 1/5 \u2014 IPSec is IPSec. Works with any VPN endpoint.\n\nWhy migration is hard: AWS VPN configuration (customer gateway, virtual private gateway, transit gateway attachment) is AWS-specific. But the tunnels themselves are standard IPSec.\nWhy migration could work: IPSec VPN works with any endpoint: other clouds, on-prem firewalls, open-source (StrongSwan, WireGuard). Reconfigure tunnels in hours.\nDependency risks: VPC or Transit Gateway (attachment), Route 53 (DNS resolution).\nAlternatives: Any IPSec-capable device/service. Azure VPN Gateway, GCP Cloud VPN, StrongSwan, WireGuard\nBlast radius if migrating: Low. Standard protocol. Reconfigure in hours.\n\nHonest take: VPN is genuinely portable because IPSec is a standard protocol. AWS gets no lock-in from VPN. Use it freely.",
      "tags": [
        "platform-agnostic"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-transit-gateway",
      "nodeId": "transit-gateway",
      "content": "<p><strong>Domains: 3</strong><br><br>Hub-and-spoke network connectivity. Route tables for network segmentation between VPCs. Transit gateway flow logs for network monitoring. Peering across Regions. Integrates with Network Firewall for centralized inspection. Domain 3: network segmentation based on security requirements (north/south, east/west).<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udfe1 Cloud-Portable (Medium Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Medium-High. Network topology redesign required. Concept exists elsewhere but implementation differs.<br><strong>Control Plane Dependency:</strong> 4/5 \u2014 AWS-proprietary hub-spoke network service. Route tables, attachments, and peering are AWS-specific.<br><strong>Data Gravity:</strong> 1/5 \u2014 Network topology, no data.<br><strong>Runtime Coupling:</strong> 3/5 \u2014 Traffic routing depends on TGW route tables. Removing TGW requires redesigning network topology.<br><br><strong>Why migration is hard:</strong> Transit Gateway route tables, attachment models, peering across Regions, and integration with Network Firewall for centralized inspection are AWS-specific architectures.<br><strong>Why migration could work:</strong> Hub-spoke networking exists everywhere. Azure Virtual WAN Hub, GCP Network Connectivity Center, or self-hosted routers. The topology is rebuildable.<br><strong>Dependency risks:</strong> VPCs (attachments), VPN/Direct Connect (hybrid attachments), Network Firewall (inspection), Route 53.<br><strong>Alternatives:</strong> Azure Virtual WAN, GCP Network Connectivity Center, Aviatrix (multi-cloud network fabric), self-hosted routers<br><strong>Blast radius if migrating:</strong> High if you have 20+ VPCs connected through TGW. The routing redesign is significant.<br><br><strong>Honest take:</strong> Transit Gateway is AWS's answer to complex networking. Aviatrix exists specifically because enterprises want multi-cloud network fabrics that aren't locked to one provider. Consider Aviatrix if multi-cloud is on the roadmap.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 3\n\nHub-and-spoke network connectivity. Route tables for network segmentation between VPCs. Transit gateway flow logs for network monitoring. Peering across Regions. Integrates with Network Firewall for centralized inspection. Domain 3: network segmentation based on security requirements (north/south, east/west).\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udfe1 Cloud-Portable (Medium Lock-In)\n\nMigration Difficulty: Medium-High. Network topology redesign required. Concept exists elsewhere but implementation differs.\nControl Plane Dependency: 4/5 \u2014 AWS-proprietary hub-spoke network service. Route tables, attachments, and peering are AWS-specific.\nData Gravity: 1/5 \u2014 Network topology, no data.\nRuntime Coupling: 3/5 \u2014 Traffic routing depends on TGW route tables. Removing TGW requires redesigning network topology.\n\nWhy migration is hard: Transit Gateway route tables, attachment models, peering across Regions, and integration with Network Firewall for centralized inspection are AWS-specific architectures.\nWhy migration could work: Hub-spoke networking exists everywhere. Azure Virtual WAN Hub, GCP Network Connectivity Center, or self-hosted routers. The topology is rebuildable.\nDependency risks: VPCs (attachments), VPN/Direct Connect (hybrid attachments), Network Firewall (inspection), Route 53.\nAlternatives: Azure Virtual WAN, GCP Network Connectivity Center, Aviatrix (multi-cloud network fabric), self-hosted routers\nBlast radius if migrating: High if you have 20+ VPCs connected through TGW. The routing redesign is significant.\n\nHonest take: Transit Gateway is AWS's answer to complex networking. Aviatrix exists specifically because enterprises want multi-cloud network fabrics that aren't locked to one provider. Consider Aviatrix if multi-cloud is on the roadmap.",
      "tags": [
        "cloud-portable"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-verified-access",
      "nodeId": "verified-access",
      "content": "<p><strong>Domains: 3, 5</strong><br><br>Zero-trust network access to applications WITHOUT a VPN. Verifies user identity (via IAM Identity Center, OIDC) AND device posture before granting access. New emphasis in C03 \u2014 appears in Domain 3 (network security) and Domain 5 (secure private access). Replaces traditional VPN for application access in many scenarios.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udd34 AWS-Proprietary (High Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> High. Zero-trust access is a growing market with alternatives, but migration requires changing user access patterns.<br><strong>Control Plane Dependency:</strong> 5/5 \u2014 AWS-proprietary zero-trust access service. Trust providers, policy language, and access groups are AWS-specific.<br><strong>Data Gravity:</strong> 1/5 \u2014 No data stored.<br><strong>Runtime Coupling:</strong> 4/5 \u2014 Application access paths route through Verified Access. Removing it changes how users reach applications.<br><br><strong>Why migration is hard:</strong> Verified Access trust policies, device posture integration, and the Cedar-based policy language are AWS-specific. User access flows change.<br><strong>Why migration could work:</strong> Zero-trust is a hot market: Cloudflare Access, Zscaler ZPA, Google BeyondCorp, Palo Alto Prisma Access. All provide equivalent functionality.<br><strong>Dependency risks:</strong> IAM Identity Center (identity), EC2/ECS (target applications), CloudWatch (logging).<br><strong>Alternatives:</strong> Cloudflare Access (excellent, free tier), Zscaler ZPA, Google BeyondCorp Enterprise, Palo Alto Prisma Access, Tailscale<br><strong>Blast radius if migrating:</strong> Medium. User access patterns change. But zero-trust alternatives are mature.<br><br><strong>Honest take:</strong> Verified Access solves a real problem but Cloudflare Access is more mature, cheaper, and cloud-agnostic. Unless you need deep IAM Identity Center integration, Cloudflare Access is the better choice for portability.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 3, 5\n\nZero-trust network access to applications WITHOUT a VPN. Verifies user identity (via IAM Identity Center, OIDC) AND device posture before granting access. New emphasis in C03 \u2014 appears in Domain 3 (network security) and Domain 5 (secure private access). Replaces traditional VPN for application access in many scenarios.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udd34 AWS-Proprietary (High Lock-In)\n\nMigration Difficulty: High. Zero-trust access is a growing market with alternatives, but migration requires changing user access patterns.\nControl Plane Dependency: 5/5 \u2014 AWS-proprietary zero-trust access service. Trust providers, policy language, and access groups are AWS-specific.\nData Gravity: 1/5 \u2014 No data stored.\nRuntime Coupling: 4/5 \u2014 Application access paths route through Verified Access. Removing it changes how users reach applications.\n\nWhy migration is hard: Verified Access trust policies, device posture integration, and the Cedar-based policy language are AWS-specific. User access flows change.\nWhy migration could work: Zero-trust is a hot market: Cloudflare Access, Zscaler ZPA, Google BeyondCorp, Palo Alto Prisma Access. All provide equivalent functionality.\nDependency risks: IAM Identity Center (identity), EC2/ECS (target applications), CloudWatch (logging).\nAlternatives: Cloudflare Access (excellent, free tier), Zscaler ZPA, Google BeyondCorp Enterprise, Palo Alto Prisma Access, Tailscale\nBlast radius if migrating: Medium. User access patterns change. But zero-trust alternatives are mature.\n\nHonest take: Verified Access solves a real problem but Cloudflare Access is more mature, cheaper, and cloud-agnostic. Unless you need deep IAM Identity Center integration, Cloudflare Access is the better choice for portability.",
      "tags": [
        "aws-proprietary"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-network-access-analyzer",
      "nodeId": "network-access-analyzer",
      "content": "<p><strong>Domains: 3</strong><br><br>Identifies unintended network access to resources in your VPCs. Evaluates security groups, NACLs, route tables, VPC peering, and VPN connections. New in C03 \u2014 Domain 3 troubleshooting (Skill 3.3.5).<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udfe1 Cloud-Portable (Medium Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Low. Assessment tool, not a runtime dependency.<br><strong>Control Plane Dependency:</strong> 4/5 \u2014 Evaluates AWS-specific network configurations.<br><strong>Data Gravity:</strong> 1/5 \u2014 No data stored.<br><strong>Runtime Coupling:</strong> 1/5 \u2014 Assessment tool.<br><br><strong>Why migration is hard:</strong> Findings reference AWS resource types and VPC constructs.<br><strong>Why migration could work:</strong> Network assessment tools exist everywhere.<br><strong>Dependency risks:</strong> VPC, security groups, NACLs, route tables \u2014 as analysis targets.<br><strong>Alternatives:</strong> Tufin, FireMon, AlgoSec, open-source network scanners, manual auditing<br><strong>Blast radius if migrating:</strong> None. Advisory tool.<br><br><strong>Honest take:</strong> Not a lock-in risk. It's a read-only assessment tool.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 3\n\nIdentifies unintended network access to resources in your VPCs. Evaluates security groups, NACLs, route tables, VPC peering, and VPN connections. New in C03 \u2014 Domain 3 troubleshooting (Skill 3.3.5).\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udfe1 Cloud-Portable (Medium Lock-In)\n\nMigration Difficulty: Low. Assessment tool, not a runtime dependency.\nControl Plane Dependency: 4/5 \u2014 Evaluates AWS-specific network configurations.\nData Gravity: 1/5 \u2014 No data stored.\nRuntime Coupling: 1/5 \u2014 Assessment tool.\n\nWhy migration is hard: Findings reference AWS resource types and VPC constructs.\nWhy migration could work: Network assessment tools exist everywhere.\nDependency risks: VPC, security groups, NACLs, route tables \u2014 as analysis targets.\nAlternatives: Tufin, FireMon, AlgoSec, open-source network scanners, manual auditing\nBlast radius if migrating: None. Advisory tool.\n\nHonest take: Not a lock-in risk. It's a read-only assessment tool.",
      "tags": [
        "cloud-portable"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-ec2",
      "nodeId": "ec2",
      "content": "<p><strong>Domains: 2, 3</strong><br><br>Core compute. Security focus areas: IMDSv2 enforcement (prevent SSRF metadata theft), instance profiles for IAM roles (never embed credentials), Nitro Enclaves for confidential computing, Nitro encryption for inter-node traffic (new C03 topic). EC2 Image Builder for hardened AMI pipelines. EC2 Instance Connect for secure SSH without key management. Domain 2: isolate compromised instances (swap security group), snapshot EBS for forensics. Domain 3: compute hardening, vulnerability scanning, patch management.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udfe1 Cloud-Portable (Medium Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Medium. VMs are VMs. The AWS-specific wrapping (AMIs, instance profiles, metadata) needs adaptation.<br><strong>Control Plane Dependency:</strong> 3/5 \u2014 Virtual machines are a universal concept. AMIs and instance types are AWS-specific, but the workload is portable.<br><strong>Data Gravity:</strong> 2/5 \u2014 AMIs can be exported as VMDK/VHD. Data on EBS volumes is exportable.<br><strong>Runtime Coupling:</strong> 3/5 \u2014 IMDSv2, instance profiles, and user data scripts may reference AWS-specific metadata. Nitro Enclaves are AWS-only.<br><br><strong>Why migration is hard:</strong> AMI format is AWS-specific. Instance profiles (IAM roles) need replacement. User data scripts often call AWS APIs. Nitro Enclaves and Nitro encryption are AWS-only confidential computing.<br><strong>Why migration could work:</strong> VMs are the most portable unit of compute. Export as OVA/VMDK/VHD. Azure VMs, GCP Compute Engine, or bare metal all run the same Linux/Windows workloads. Containers are even more portable.<br><strong>Dependency risks:</strong> VPC (networking), EBS (storage), IAM (roles), Systems Manager (management), Security Groups (firewall).<br><strong>Alternatives:</strong> Azure VMs, GCP Compute Engine, bare metal, containers on any platform (most portable option)<br><strong>Blast radius if migrating:</strong> Medium. VM migration is a solved problem (AWS Server Migration Service, CloudEndure). The blast radius is in the integrations, not the VMs.<br><br><strong>Honest take:</strong> EC2 is moderately portable. The VM is portable; the AWS wiring around it is not. If you containerize workloads, you eliminate most EC2-specific coupling. That's the real portability play.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 2, 3\n\nCore compute. Security focus areas: IMDSv2 enforcement (prevent SSRF metadata theft), instance profiles for IAM roles (never embed credentials), Nitro Enclaves for confidential computing, Nitro encryption for inter-node traffic (new C03 topic). EC2 Image Builder for hardened AMI pipelines. EC2 Instance Connect for secure SSH without key management. Domain 2: isolate compromised instances (swap security group), snapshot EBS for forensics. Domain 3: compute hardening, vulnerability scanning, patch management.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udfe1 Cloud-Portable (Medium Lock-In)\n\nMigration Difficulty: Medium. VMs are VMs. The AWS-specific wrapping (AMIs, instance profiles, metadata) needs adaptation.\nControl Plane Dependency: 3/5 \u2014 Virtual machines are a universal concept. AMIs and instance types are AWS-specific, but the workload is portable.\nData Gravity: 2/5 \u2014 AMIs can be exported as VMDK/VHD. Data on EBS volumes is exportable.\nRuntime Coupling: 3/5 \u2014 IMDSv2, instance profiles, and user data scripts may reference AWS-specific metadata. Nitro Enclaves are AWS-only.\n\nWhy migration is hard: AMI format is AWS-specific. Instance profiles (IAM roles) need replacement. User data scripts often call AWS APIs. Nitro Enclaves and Nitro encryption are AWS-only confidential computing.\nWhy migration could work: VMs are the most portable unit of compute. Export as OVA/VMDK/VHD. Azure VMs, GCP Compute Engine, or bare metal all run the same Linux/Windows workloads. Containers are even more portable.\nDependency risks: VPC (networking), EBS (storage), IAM (roles), Systems Manager (management), Security Groups (firewall).\nAlternatives: Azure VMs, GCP Compute Engine, bare metal, containers on any platform (most portable option)\nBlast radius if migrating: Medium. VM migration is a solved problem (AWS Server Migration Service, CloudEndure). The blast radius is in the integrations, not the VMs.\n\nHonest take: EC2 is moderately portable. The VM is portable; the AWS wiring around it is not. If you containerize workloads, you eliminate most EC2-specific coupling. That's the real portability play.",
      "tags": [
        "cloud-portable"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-iot-core",
      "nodeId": "iot-core",
      "content": "<p><strong>Domains: 3</strong><br><br>IoT device connectivity. IoT policies control device permissions. X.509 certificate-based authentication. Domain 3 Skill 3.1.2 \u2014 edge security with IoT policies.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udd34 AWS-Proprietary (High Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> High. Device reconfiguration at scale is the hard part.<br><strong>Control Plane Dependency:</strong> 5/5 \u2014 AWS-specific device management, policies, and certificate handling.<br><strong>Data Gravity:</strong> 3/5 \u2014 Device data can flow to any MQTT broker. Device registry is AWS-specific.<br><strong>Runtime Coupling:</strong> 4/5 \u2014 Devices connect to AWS-specific MQTT endpoints. IoT policies are AWS-proprietary.<br><br><strong>Why migration is hard:</strong> IoT devices in the field connect to AWS MQTT endpoints. Reconfiguring thousands/millions of devices is an operational nightmare. IoT policies, device shadows, and the rules engine are all AWS-specific.<br><strong>Why migration could work:</strong> MQTT is a standard protocol. HiveMQ, EMQX, Azure IoT Hub, GCP IoT Core \u2014 all speak MQTT. New devices can target any broker.<br><strong>Dependency risks:</strong> IAM (device policies), Lambda (rules engine actions), S3/DynamoDB (data storage), ACM (certificates).<br><strong>Alternatives:</strong> Azure IoT Hub, HiveMQ (open-source MQTT), EMQX (open-source), Eclipse Mosquitto, custom MQTT broker<br><strong>Blast radius if migrating:</strong> Very high for deployed device fleets. Low for new deployments.<br><br><strong>Honest take:</strong> IoT is the ultimate lock-in because the devices are in the field. Design for portability by using MQTT + custom broker endpoints from day one. Never hardcode an AWS endpoint into firmware.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 3\n\nIoT device connectivity. IoT policies control device permissions. X.509 certificate-based authentication. Domain 3 Skill 3.1.2 \u2014 edge security with IoT policies.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udd34 AWS-Proprietary (High Lock-In)\n\nMigration Difficulty: High. Device reconfiguration at scale is the hard part.\nControl Plane Dependency: 5/5 \u2014 AWS-specific device management, policies, and certificate handling.\nData Gravity: 3/5 \u2014 Device data can flow to any MQTT broker. Device registry is AWS-specific.\nRuntime Coupling: 4/5 \u2014 Devices connect to AWS-specific MQTT endpoints. IoT policies are AWS-proprietary.\n\nWhy migration is hard: IoT devices in the field connect to AWS MQTT endpoints. Reconfiguring thousands/millions of devices is an operational nightmare. IoT policies, device shadows, and the rules engine are all AWS-specific.\nWhy migration could work: MQTT is a standard protocol. HiveMQ, EMQX, Azure IoT Hub, GCP IoT Core \u2014 all speak MQTT. New devices can target any broker.\nDependency risks: IAM (device policies), Lambda (rules engine actions), S3/DynamoDB (data storage), ACM (certificates).\nAlternatives: Azure IoT Hub, HiveMQ (open-source MQTT), EMQX (open-source), Eclipse Mosquitto, custom MQTT broker\nBlast radius if migrating: Very high for deployed device fleets. Low for new deployments.\n\nHonest take: IoT is the ultimate lock-in because the devices are in the field. Design for portability by using MQTT + custom broker endpoints from day one. Never hardcode an AWS endpoint into firmware.",
      "tags": [
        "aws-proprietary"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-firewall-manager",
      "nodeId": "firewall-manager",
      "content": "<p><strong>Domains: 3, 6</strong><br><br>Central policy management across accounts for: WAF rules, Shield Advanced protections, Network Firewall policies, security groups, Route 53 Resolver DNS Firewall rules. Requires Organizations. Auto-applies policies to new accounts/resources. Domain 6: secure deployment strategy. Domain 3: infrastructure security at scale.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udd34 AWS-Proprietary (High Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> High. No direct equivalent for centralized, cross-account security policy deployment.<br><strong>Control Plane Dependency:</strong> 5/5 \u2014 AWS-proprietary centralized policy management. Requires Organizations.<br><strong>Data Gravity:</strong> 1/5 \u2014 Policies are configuration.<br><strong>Runtime Coupling:</strong> 4/5 \u2014 Auto-applies policies to new accounts/resources. Removing it means manual policy management.<br><br><strong>Why migration is hard:</strong> Firewall Manager auto-deploys WAF rules, Shield Advanced protections, Network Firewall policies, and security group auditing across all accounts. This cross-account automation doesn't exist elsewhere.<br><strong>Why migration could work:</strong> Individual policies (WAF rules, firewall rules) are manageable. The centralized deployment is the loss.<br><strong>Dependency risks:</strong> Organizations (required), WAF, Shield, Network Firewall, security groups \u2014 all managed targets.<br><strong>Alternatives:</strong> Terraform/Pulumi for IaC-based policy deployment, Azure Firewall Manager (Azure-only), Palo Alto Panorama (on-prem/multi-cloud)<br><strong>Blast radius if migrating:</strong> High for organizations with 50+ accounts. Manual policy management at scale is operationally painful.<br><br><strong>Honest take:</strong> Firewall Manager is glue \u2014 it deploys other services' policies at scale. The lock-in is in the automation, not the policies themselves. Use Terraform to centralize policy deployment if you want cloud-agnostic governance.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 3, 6\n\nCentral policy management across accounts for: WAF rules, Shield Advanced protections, Network Firewall policies, security groups, Route 53 Resolver DNS Firewall rules. Requires Organizations. Auto-applies policies to new accounts/resources. Domain 6: secure deployment strategy. Domain 3: infrastructure security at scale.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udd34 AWS-Proprietary (High Lock-In)\n\nMigration Difficulty: High. No direct equivalent for centralized, cross-account security policy deployment.\nControl Plane Dependency: 5/5 \u2014 AWS-proprietary centralized policy management. Requires Organizations.\nData Gravity: 1/5 \u2014 Policies are configuration.\nRuntime Coupling: 4/5 \u2014 Auto-applies policies to new accounts/resources. Removing it means manual policy management.\n\nWhy migration is hard: Firewall Manager auto-deploys WAF rules, Shield Advanced protections, Network Firewall policies, and security group auditing across all accounts. This cross-account automation doesn't exist elsewhere.\nWhy migration could work: Individual policies (WAF rules, firewall rules) are manageable. The centralized deployment is the loss.\nDependency risks: Organizations (required), WAF, Shield, Network Firewall, security groups \u2014 all managed targets.\nAlternatives: Terraform/Pulumi for IaC-based policy deployment, Azure Firewall Manager (Azure-only), Palo Alto Panorama (on-prem/multi-cloud)\nBlast radius if migrating: High for organizations with 50+ accounts. Manual policy management at scale is operationally painful.\n\nHonest take: Firewall Manager is glue \u2014 it deploys other services' policies at scale. The lock-in is in the automation, not the policies themselves. Use Terraform to centralize policy deployment if you want cloud-agnostic governance.",
      "tags": [
        "aws-proprietary"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-control-tower",
      "nodeId": "control-tower",
      "content": "<p><strong>Domains: 6</strong><br><br>Automated landing zone setup. Guardrails: preventive (SCPs), detective (Config rules), proactive (CloudFormation hooks). Account Factory for standardized account provisioning. Optional and custom controls for organizational requirements. Manages log archive and audit accounts automatically. Domain 6 Skill 6.1.2 \u2014 implement in new AND existing environments.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udd34 AWS-Proprietary (High Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> High. Landing zone automation is always platform-specific. Azure Landing Zones and GCP Foundation Toolkit are the equivalents.<br><strong>Control Plane Dependency:</strong> 5/5 \u2014 AWS-proprietary landing zone automation. Guardrails, Account Factory, and controls are all AWS-specific.<br><strong>Data Gravity:</strong> 1/5 \u2014 No data stored. It's governance configuration.<br><strong>Runtime Coupling:</strong> 3/5 \u2014 Guardrails (SCPs + Config rules) actively enforce governance. Disabling Control Tower doesn't remove the underlying SCPs/Config rules.<br><br><strong>Why migration is hard:</strong> Control Tower guardrails, Account Factory, and the landing zone structure are entirely AWS-specific. Customizations via Customizations for Control Tower (CfCT) are CloudFormation-based.<br><strong>Why migration could work:</strong> The concept of a landing zone is universal. Azure Landing Zones (CAF) and GCP Foundation Toolkit provide equivalent functionality. Terraform can build landing zones on any cloud.<br><strong>Dependency risks:</strong> Organizations (required), Config (detective guardrails), SCPs (preventive guardrails), CloudFormation (account provisioning).<br><strong>Alternatives:</strong> Azure Landing Zones (CAF), GCP Foundation Toolkit, Terraform Cloud/Enterprise for multi-cloud governance, Pulumi<br><strong>Blast radius if migrating:</strong> High. Landing zone is the foundation. But it's configuration, not data \u2014 rebuildable.<br><br><strong>Honest take:</strong> Control Tower is a rapid-start tool, not a long-term lock-in risk. The underlying mechanisms (SCPs, Config rules) are what actually enforce governance. You can replicate the pattern with Terraform on any cloud.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 6\n\nAutomated landing zone setup. Guardrails: preventive (SCPs), detective (Config rules), proactive (CloudFormation hooks). Account Factory for standardized account provisioning. Optional and custom controls for organizational requirements. Manages log archive and audit accounts automatically. Domain 6 Skill 6.1.2 \u2014 implement in new AND existing environments.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udd34 AWS-Proprietary (High Lock-In)\n\nMigration Difficulty: High. Landing zone automation is always platform-specific. Azure Landing Zones and GCP Foundation Toolkit are the equivalents.\nControl Plane Dependency: 5/5 \u2014 AWS-proprietary landing zone automation. Guardrails, Account Factory, and controls are all AWS-specific.\nData Gravity: 1/5 \u2014 No data stored. It's governance configuration.\nRuntime Coupling: 3/5 \u2014 Guardrails (SCPs + Config rules) actively enforce governance. Disabling Control Tower doesn't remove the underlying SCPs/Config rules.\n\nWhy migration is hard: Control Tower guardrails, Account Factory, and the landing zone structure are entirely AWS-specific. Customizations via Customizations for Control Tower (CfCT) are CloudFormation-based.\nWhy migration could work: The concept of a landing zone is universal. Azure Landing Zones (CAF) and GCP Foundation Toolkit provide equivalent functionality. Terraform can build landing zones on any cloud.\nDependency risks: Organizations (required), Config (detective guardrails), SCPs (preventive guardrails), CloudFormation (account provisioning).\nAlternatives: Azure Landing Zones (CAF), GCP Foundation Toolkit, Terraform Cloud/Enterprise for multi-cloud governance, Pulumi\nBlast radius if migrating: High. Landing zone is the foundation. But it's configuration, not data \u2014 rebuildable.\n\nHonest take: Control Tower is a rapid-start tool, not a long-term lock-in risk. The underlying mechanisms (SCPs, Config rules) are what actually enforce governance. You can replicate the pattern with Terraform on any cloud.",
      "tags": [
        "aws-proprietary"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-cloudformation",
      "nodeId": "cloudformation",
      "content": "<p><strong>Domains: 6</strong><br><br>Infrastructure as Code. StackSets deploy across multiple accounts/Regions. CloudFormation Guard: policy-as-code validation of templates. cfn-lint for template linting. Drift detection identifies manual changes. Nested stacks for modular templates. Domain 6 Skill 6.2.1 \u2014 secure, consistent, auditable deployments.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udd34 AWS-Proprietary (High Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> High for CloudFormation templates (must rewrite). Low if you use Terraform instead.<br><strong>Control Plane Dependency:</strong> 5/5 \u2014 AWS-proprietary IaC. Template format, intrinsic functions, and resource types are all AWS-specific.<br><strong>Data Gravity:</strong> 1/5 \u2014 Templates are text files. Exportable but useless outside AWS.<br><strong>Runtime Coupling:</strong> 3/5 \u2014 CloudFormation manages stack state. Drift detection and StackSets are AWS-specific. But the deployed resources exist independently.<br><br><strong>Why migration is hard:</strong> CloudFormation templates reference AWS resource types (AWS::EC2::Instance), use AWS-specific intrinsic functions (Fn::Sub, !Ref), and depend on IAM for execution roles. StackSets use Organizations. Guard rules are CloudFormation-specific.<br><strong>Why migration could work:</strong> Terraform is the industry-standard portable IaC. If you're already using Terraform, CloudFormation lock-in is zero. CDK generates CloudFormation \u2014 also locked in.<br><strong>Dependency risks:</strong> Every AWS resource type (via templates), Organizations (StackSets), IAM (execution roles), S3 (template storage).<br><strong>Alternatives:</strong> Terraform (industry standard, multi-cloud), Pulumi (multi-cloud, real programming languages), OpenTofu (open-source Terraform fork), Crossplane (Kubernetes-native)<br><strong>Blast radius if migrating:</strong> High if CloudFormation is your only IaC. Zero if you use Terraform.<br><br><strong>Honest take:</strong> CloudFormation is the most unnecessary lock-in on AWS. Terraform does everything CloudFormation does, works across clouds, has a larger ecosystem, and is what the industry standardized on. If you're writing CloudFormation in 2026, you're choosing lock-in. Use Terraform.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 6\n\nInfrastructure as Code. StackSets deploy across multiple accounts/Regions. CloudFormation Guard: policy-as-code validation of templates. cfn-lint for template linting. Drift detection identifies manual changes. Nested stacks for modular templates. Domain 6 Skill 6.2.1 \u2014 secure, consistent, auditable deployments.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udd34 AWS-Proprietary (High Lock-In)\n\nMigration Difficulty: High for CloudFormation templates (must rewrite). Low if you use Terraform instead.\nControl Plane Dependency: 5/5 \u2014 AWS-proprietary IaC. Template format, intrinsic functions, and resource types are all AWS-specific.\nData Gravity: 1/5 \u2014 Templates are text files. Exportable but useless outside AWS.\nRuntime Coupling: 3/5 \u2014 CloudFormation manages stack state. Drift detection and StackSets are AWS-specific. But the deployed resources exist independently.\n\nWhy migration is hard: CloudFormation templates reference AWS resource types (AWS::EC2::Instance), use AWS-specific intrinsic functions (Fn::Sub, !Ref), and depend on IAM for execution roles. StackSets use Organizations. Guard rules are CloudFormation-specific.\nWhy migration could work: Terraform is the industry-standard portable IaC. If you're already using Terraform, CloudFormation lock-in is zero. CDK generates CloudFormation \u2014 also locked in.\nDependency risks: Every AWS resource type (via templates), Organizations (StackSets), IAM (execution roles), S3 (template storage).\nAlternatives: Terraform (industry standard, multi-cloud), Pulumi (multi-cloud, real programming languages), OpenTofu (open-source Terraform fork), Crossplane (Kubernetes-native)\nBlast radius if migrating: High if CloudFormation is your only IaC. Zero if you use Terraform.\n\nHonest take: CloudFormation is the most unnecessary lock-in on AWS. Terraform does everything CloudFormation does, works across clouds, has a larger ecosystem, and is what the industry standardized on. If you're writing CloudFormation in 2026, you're choosing lock-in. Use Terraform.",
      "tags": [
        "aws-proprietary"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-service-catalog",
      "nodeId": "service-catalog",
      "content": "<p><strong>Domains: 6</strong><br><br>Pre-approved CloudFormation templates as products. Portfolios shared across accounts. Launch constraints control which IAM role provisions resources. TagOption library enforces tagging. Self-service provisioning with governance. Domain 6 Skill 6.2.4 \u2014 securely share resources across accounts.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udd34 AWS-Proprietary (High Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> High. Products are CloudFormation (AWS-specific). Portfolio and constraint model is AWS-specific.<br><strong>Control Plane Dependency:</strong> 5/5 \u2014 AWS-proprietary provisioning catalog. Products are CloudFormation templates. Portfolio sharing is Organizations-based.<br><strong>Data Gravity:</strong> 1/5 \u2014 Catalog is configuration.<br><strong>Runtime Coupling:</strong> 2/5 \u2014 Users provision via catalog. The provisioned resources exist independently.<br><br><strong>Why migration is hard:</strong> Products are CloudFormation templates. Launch constraints use IAM. TagOption library is AWS-specific.<br><strong>Why migration could work:</strong> Terraform modules + a self-service portal (Backstage, Port, Env0) provide equivalent functionality and are cloud-agnostic.<br><strong>Dependency risks:</strong> CloudFormation (product templates), Organizations (portfolio sharing), IAM (launch constraints).<br><strong>Alternatives:</strong> Backstage.io (open-source service catalog, CNCF), Port.io, Env0, Terraform Cloud private registry, Pulumi<br><strong>Blast radius if migrating:</strong> Medium. Replace with Backstage + Terraform modules for a portable self-service catalog.<br><br><strong>Honest take:</strong> Service Catalog is CloudFormation lock-in with a GUI on top. Use Backstage (Spotify's open-source service catalog, now CNCF) with Terraform modules instead. It's more capable and portable.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 6\n\nPre-approved CloudFormation templates as products. Portfolios shared across accounts. Launch constraints control which IAM role provisions resources. TagOption library enforces tagging. Self-service provisioning with governance. Domain 6 Skill 6.2.4 \u2014 securely share resources across accounts.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udd34 AWS-Proprietary (High Lock-In)\n\nMigration Difficulty: High. Products are CloudFormation (AWS-specific). Portfolio and constraint model is AWS-specific.\nControl Plane Dependency: 5/5 \u2014 AWS-proprietary provisioning catalog. Products are CloudFormation templates. Portfolio sharing is Organizations-based.\nData Gravity: 1/5 \u2014 Catalog is configuration.\nRuntime Coupling: 2/5 \u2014 Users provision via catalog. The provisioned resources exist independently.\n\nWhy migration is hard: Products are CloudFormation templates. Launch constraints use IAM. TagOption library is AWS-specific.\nWhy migration could work: Terraform modules + a self-service portal (Backstage, Port, Env0) provide equivalent functionality and are cloud-agnostic.\nDependency risks: CloudFormation (product templates), Organizations (portfolio sharing), IAM (launch constraints).\nAlternatives: Backstage.io (open-source service catalog, CNCF), Port.io, Env0, Terraform Cloud private registry, Pulumi\nBlast radius if migrating: Medium. Replace with Backstage + Terraform modules for a portable self-service catalog.\n\nHonest take: Service Catalog is CloudFormation lock-in with a GUI on top. Use Backstage (Spotify's open-source service catalog, now CNCF) with Terraform modules instead. It's more capable and portable.",
      "tags": [
        "aws-proprietary"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-ram",
      "nodeId": "ram",
      "content": "<p><strong>Domains: 6</strong><br><br>Share AWS resources across accounts without duplication. Shareable: subnets, Transit Gateways, Route 53 Resolver rules, License Manager configs, and more. Integration with Organizations for trusted sharing. Domain 6 Skill 6.2.4.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udfe1 Cloud-Portable (Medium Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Low-Medium. RAM is glue. The resources it shares are the real consideration.<br><strong>Control Plane Dependency:</strong> 4/5 \u2014 AWS-proprietary resource sharing. But the shared resources themselves may be portable.<br><strong>Data Gravity:</strong> 1/5 \u2014 No data stored. Configuration only.<br><strong>Runtime Coupling:</strong> 2/5 \u2014 Shared resources (subnets, TGWs) are accessed normally by consuming accounts.<br><br><strong>Why migration is hard:</strong> Resource sharing model is AWS-specific. Shared subnet access, TGW attachments, and license configs are AWS concepts.<br><strong>Why migration could work:</strong> The shared resources themselves (subnets, TGWs) would be redesigned in any migration anyway. RAM is configuration.<br><strong>Dependency risks:</strong> Organizations (trusted sharing), VPC (shared subnets), Transit Gateway, License Manager.<br><strong>Alternatives:</strong> Azure Resource Group sharing, GCP Shared VPC, Terraform for cross-account resource management<br><strong>Blast radius if migrating:</strong> Low. RAM is organizational glue, not a data dependency.<br><br><strong>Honest take:</strong> RAM is not a significant lock-in risk. It's organizational convenience. The resources it shares are the real lock-in concern.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 6\n\nShare AWS resources across accounts without duplication. Shareable: subnets, Transit Gateways, Route 53 Resolver rules, License Manager configs, and more. Integration with Organizations for trusted sharing. Domain 6 Skill 6.2.4.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udfe1 Cloud-Portable (Medium Lock-In)\n\nMigration Difficulty: Low-Medium. RAM is glue. The resources it shares are the real consideration.\nControl Plane Dependency: 4/5 \u2014 AWS-proprietary resource sharing. But the shared resources themselves may be portable.\nData Gravity: 1/5 \u2014 No data stored. Configuration only.\nRuntime Coupling: 2/5 \u2014 Shared resources (subnets, TGWs) are accessed normally by consuming accounts.\n\nWhy migration is hard: Resource sharing model is AWS-specific. Shared subnet access, TGW attachments, and license configs are AWS concepts.\nWhy migration could work: The shared resources themselves (subnets, TGWs) would be redesigned in any migration anyway. RAM is configuration.\nDependency risks: Organizations (trusted sharing), VPC (shared subnets), Transit Gateway, License Manager.\nAlternatives: Azure Resource Group sharing, GCP Shared VPC, Terraform for cross-account resource management\nBlast radius if migrating: Low. RAM is organizational glue, not a data dependency.\n\nHonest take: RAM is not a significant lock-in risk. It's organizational convenience. The resources it shares are the real lock-in concern.",
      "tags": [
        "cloud-portable"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-audit-manager",
      "nodeId": "audit-manager",
      "content": "<p><strong>Domains: 6</strong><br><br>Continuously collects evidence for compliance audits. Pre-built frameworks: SOC 2, PCI DSS, HIPAA, GDPR, CIS, NIST, FedRAMP. Automated evidence from Config, CloudTrail, Security Hub. Manual evidence uploads. Assessment reports for auditors. Domain 6 Skill 6.3.2.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udd34 AWS-Proprietary (High Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Medium-High. Compliance evidence collection needs to be re-established on a new platform.<br><strong>Control Plane Dependency:</strong> 5/5 \u2014 AWS-proprietary evidence collection. Frameworks map to AWS-specific controls. Assessment reports are AWS-formatted.<br><strong>Data Gravity:</strong> 3/5 \u2014 Evidence is exportable as reports. But the automated collection pipeline is AWS-specific.<br><strong>Runtime Coupling:</strong> 2/5 \u2014 Assessment tool, not runtime dependency.<br><br><strong>Why migration is hard:</strong> Pre-built frameworks (SOC 2, PCI DSS on AWS) map to AWS-specific controls and services. Automated evidence from Config, CloudTrail, and Security Hub is AWS-only. Custom frameworks reference AWS controls.<br><strong>Why migration could work:</strong> Compliance requirements are platform-agnostic. Drata, Vanta, and Lacework provide multi-cloud compliance automation.<br><strong>Dependency risks:</strong> Config (automated evidence), CloudTrail (API audit evidence), Security Hub (finding evidence), Organizations (multi-account).<br><strong>Alternatives:</strong> Drata (multi-cloud compliance), Vanta, Lacework, Prisma Cloud Compliance, manual evidence collection<br><strong>Blast radius if migrating:</strong> Medium. Compliance programs need re-establishment. But compliance requirements don't change \u2014 only the evidence collection mechanism.<br><br><strong>Honest take:</strong> Audit Manager is convenient but Drata and Vanta do this across clouds. If you're SOC 2 audited, you need multi-cloud compliance anyway. Use a platform-agnostic GRC tool.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 6\n\nContinuously collects evidence for compliance audits. Pre-built frameworks: SOC 2, PCI DSS, HIPAA, GDPR, CIS, NIST, FedRAMP. Automated evidence from Config, CloudTrail, Security Hub. Manual evidence uploads. Assessment reports for auditors. Domain 6 Skill 6.3.2.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udd34 AWS-Proprietary (High Lock-In)\n\nMigration Difficulty: Medium-High. Compliance evidence collection needs to be re-established on a new platform.\nControl Plane Dependency: 5/5 \u2014 AWS-proprietary evidence collection. Frameworks map to AWS-specific controls. Assessment reports are AWS-formatted.\nData Gravity: 3/5 \u2014 Evidence is exportable as reports. But the automated collection pipeline is AWS-specific.\nRuntime Coupling: 2/5 \u2014 Assessment tool, not runtime dependency.\n\nWhy migration is hard: Pre-built frameworks (SOC 2, PCI DSS on AWS) map to AWS-specific controls and services. Automated evidence from Config, CloudTrail, and Security Hub is AWS-only. Custom frameworks reference AWS controls.\nWhy migration could work: Compliance requirements are platform-agnostic. Drata, Vanta, and Lacework provide multi-cloud compliance automation.\nDependency risks: Config (automated evidence), CloudTrail (API audit evidence), Security Hub (finding evidence), Organizations (multi-account).\nAlternatives: Drata (multi-cloud compliance), Vanta, Lacework, Prisma Cloud Compliance, manual evidence collection\nBlast radius if migrating: Medium. Compliance programs need re-establishment. But compliance requirements don't change \u2014 only the evidence collection mechanism.\n\nHonest take: Audit Manager is convenient but Drata and Vanta do this across clouds. If you're SOC 2 audited, you need multi-cloud compliance anyway. Use a platform-agnostic GRC tool.",
      "tags": [
        "aws-proprietary"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-artifact",
      "nodeId": "artifact",
      "content": "<p><strong>Domains: 6</strong><br><br>On-demand access to AWS compliance reports and agreements. SOC reports, PCI attestation, ISO certifications, BAAs (Business Associate Agreements for HIPAA). No technical configuration \u2014 purely documentation/evidence retrieval. Domain 6 Skill 6.3.2.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udfe2 Platform-Agnostic (Low Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> None. Download your compliance documents. Done.<br><strong>Control Plane Dependency:</strong> 1/5 \u2014 Document retrieval service. No technical integration.<br><strong>Data Gravity:</strong> 1/5 \u2014 Documents (SOC reports, etc.) are downloadable PDFs.<br><strong>Runtime Coupling:</strong> 0/5 \u2014 Zero runtime coupling. Purely informational.<br><br><strong>Why migration is hard:</strong> Nothing.<br><strong>Why migration could work:</strong> It's a document download portal. Every cloud has one. Azure Trust Center, GCP Compliance Reports.<br><strong>Dependency risks:</strong> None.<br><strong>Alternatives:</strong> Azure Trust Center, GCP Compliance Reports, any GRC platform<br><strong>Blast radius if migrating:</strong> None.<br><br><strong>Honest take:</strong> Not a service. It's a download page. Zero lock-in.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 6\n\nOn-demand access to AWS compliance reports and agreements. SOC reports, PCI attestation, ISO certifications, BAAs (Business Associate Agreements for HIPAA). No technical configuration \u2014 purely documentation/evidence retrieval. Domain 6 Skill 6.3.2.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udfe2 Platform-Agnostic (Low Lock-In)\n\nMigration Difficulty: None. Download your compliance documents. Done.\nControl Plane Dependency: 1/5 \u2014 Document retrieval service. No technical integration.\nData Gravity: 1/5 \u2014 Documents (SOC reports, etc.) are downloadable PDFs.\nRuntime Coupling: 0/5 \u2014 Zero runtime coupling. Purely informational.\n\nWhy migration is hard: Nothing.\nWhy migration could work: It's a document download portal. Every cloud has one. Azure Trust Center, GCP Compliance Reports.\nDependency risks: None.\nAlternatives: Azure Trust Center, GCP Compliance Reports, any GRC platform\nBlast radius if migrating: None.\n\nHonest take: Not a service. It's a download page. Zero lock-in.",
      "tags": [
        "platform-agnostic"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-well-architected",
      "nodeId": "well-architected",
      "content": "<p><strong>Domains: 6</strong><br><br>Self-service review of architectures against AWS best practices. Security Pillar: IAM, detection, infrastructure protection, data protection, incident response. Generates improvement plans. Custom lenses for organizational standards. Domain 6 Skill 6.3.3 \u2014 evaluate architecture for compliance with security best practices.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udfe2 Platform-Agnostic (Low Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> None. It's a review framework, not a dependency.<br><strong>Control Plane Dependency:</strong> 2/5 \u2014 Assessment tool with AWS-specific recommendations. But the Well-Architected Framework principles are universal.<br><strong>Data Gravity:</strong> 1/5 \u2014 Review results exportable.<br><strong>Runtime Coupling:</strong> 0/5 \u2014 Assessment tool. No runtime dependency.<br><br><strong>Why migration is hard:</strong> Recommendations are AWS-specific ('enable GuardDuty', 'use KMS').<br><strong>Why migration could work:</strong> The five pillars (security, reliability, performance, cost, operations) are universal principles. Apply them anywhere.<br><strong>Dependency risks:</strong> None.<br><strong>Alternatives:</strong> Azure Well-Architected Framework, GCP Architecture Framework, TOGAF, any architecture review framework<br><strong>Blast radius if migrating:</strong> None. Advisory tool.<br><br><strong>Honest take:</strong> The Well-Architected Framework is genuinely good architectural thinking, and the principles are cloud-agnostic even though the recommendations are AWS-specific. Not a lock-in risk. Use the thinking, ignore the vendor-specific advice.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 6\n\nSelf-service review of architectures against AWS best practices. Security Pillar: IAM, detection, infrastructure protection, data protection, incident response. Generates improvement plans. Custom lenses for organizational standards. Domain 6 Skill 6.3.3 \u2014 evaluate architecture for compliance with security best practices.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udfe2 Platform-Agnostic (Low Lock-In)\n\nMigration Difficulty: None. It's a review framework, not a dependency.\nControl Plane Dependency: 2/5 \u2014 Assessment tool with AWS-specific recommendations. But the Well-Architected Framework principles are universal.\nData Gravity: 1/5 \u2014 Review results exportable.\nRuntime Coupling: 0/5 \u2014 Assessment tool. No runtime dependency.\n\nWhy migration is hard: Recommendations are AWS-specific ('enable GuardDuty', 'use KMS').\nWhy migration could work: The five pillars (security, reliability, performance, cost, operations) are universal principles. Apply them anywhere.\nDependency risks: None.\nAlternatives: Azure Well-Architected Framework, GCP Architecture Framework, TOGAF, any architecture review framework\nBlast radius if migrating: None. Advisory tool.\n\nHonest take: The Well-Architected Framework is genuinely good architectural thinking, and the principles are cloud-agnostic even though the recommendations are AWS-specific. Not a lock-in risk. Use the thinking, ignore the vendor-specific advice.",
      "tags": [
        "platform-agnostic"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-ec2-image-builder",
      "nodeId": "ec2-image-builder",
      "content": "<p><strong>Domains: 3</strong><br><br>Automated AMI and container image build pipelines. Embed security controls: CIS hardening, patching, vulnerability scanning, custom components. Distributes images across accounts/Regions. Integrates with Inspector for scanning. Ensures compute workloads start from a known-good baseline.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udfe1 Cloud-Portable (Medium Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Medium. Image build pipelines exist everywhere.<br><strong>Control Plane Dependency:</strong> 4/5 \u2014 AWS-specific AMI/container image pipeline. Recipes and components are AWS-proprietary.<br><strong>Data Gravity:</strong> 1/5 \u2014 Output images can be exported.<br><strong>Runtime Coupling:</strong> 2/5 \u2014 Build pipeline, not runtime.<br><br><strong>Why migration is hard:</strong> Recipes, components, and distribution configurations are AWS-specific YAML/JSON. CIS hardening components are AWS-tuned.<br><strong>Why migration could work:</strong> Packer (HashiCorp) is the industry-standard portable image builder. It builds AMIs, Azure images, GCP images, and Docker containers from the same template.<br><strong>Dependency risks:</strong> EC2 (build instances), Inspector (vulnerability scanning), S3 (component storage), KMS (encryption).<br><strong>Alternatives:</strong> HashiCorp Packer (portable, industry standard), Docker/Buildah (containers), Azure Image Builder, GCP VM image import<br><strong>Blast radius if migrating:</strong> Low. Packer is a drop-in replacement that's been doing this longer and does it more portably.<br><br><strong>Honest take:</strong> Use Packer. It does everything Image Builder does, works across clouds, and has a massive community. Image Builder exists because AWS wanted the pipeline to stay in-console. Packer is better.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 3\n\nAutomated AMI and container image build pipelines. Embed security controls: CIS hardening, patching, vulnerability scanning, custom components. Distributes images across accounts/Regions. Integrates with Inspector for scanning. Ensures compute workloads start from a known-good baseline.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udfe1 Cloud-Portable (Medium Lock-In)\n\nMigration Difficulty: Medium. Image build pipelines exist everywhere.\nControl Plane Dependency: 4/5 \u2014 AWS-specific AMI/container image pipeline. Recipes and components are AWS-proprietary.\nData Gravity: 1/5 \u2014 Output images can be exported.\nRuntime Coupling: 2/5 \u2014 Build pipeline, not runtime.\n\nWhy migration is hard: Recipes, components, and distribution configurations are AWS-specific YAML/JSON. CIS hardening components are AWS-tuned.\nWhy migration could work: Packer (HashiCorp) is the industry-standard portable image builder. It builds AMIs, Azure images, GCP images, and Docker containers from the same template.\nDependency risks: EC2 (build instances), Inspector (vulnerability scanning), S3 (component storage), KMS (encryption).\nAlternatives: HashiCorp Packer (portable, industry standard), Docker/Buildah (containers), Azure Image Builder, GCP VM image import\nBlast radius if migrating: Low. Packer is a drop-in replacement that's been doing this longer and does it more portably.\n\nHonest take: Use Packer. It does everything Image Builder does, works across clouds, and has a massive community. Image Builder exists because AWS wanted the pipeline to stay in-console. Packer is better.",
      "tags": [
        "cloud-portable"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-ecr",
      "nodeId": "ecr",
      "content": "<p><strong>Domains: 3</strong><br><br>Container image registry. Basic and enhanced scanning (via Inspector) for vulnerabilities. Image signing for provenance verification. Lifecycle policies for image cleanup. Encryption at rest with KMS. Cross-account and cross-Region replication. Pull-through cache for public registries.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udfe2 Platform-Agnostic (Low Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Low. Container images are portable by design. Change the registry URL in your deployment configs.<br><strong>Control Plane Dependency:</strong> 2/5 \u2014 Container registries are standardized (OCI). ECR-specific features (image scanning, lifecycle policies) are AWS-specific.<br><strong>Data Gravity:</strong> 2/5 \u2014 Container images are OCI standard. Push/pull to any registry.<br><strong>Runtime Coupling:</strong> 2/5 \u2014 Image pull references ECR URLs. Update URLs to point to new registry.<br><br><strong>Why migration is hard:</strong> ECR image scanning (via Inspector) and lifecycle policies need recreation. IAM-based pull authentication is AWS-specific. Cross-Region replication configs don't transfer.<br><strong>Why migration could work:</strong> Container images are OCI standard. Docker push/pull works with any registry. GCR, ACR, Docker Hub, Harbor \u2014 they all store the same images.<br><strong>Dependency risks:</strong> Inspector (scanning), EC2/ECS/EKS (consumers), IAM (authentication), KMS (encryption).<br><strong>Alternatives:</strong> Docker Hub, GCP Artifact Registry, Azure Container Registry, Harbor (self-hosted, open-source), GitHub Container Registry<br><strong>Blast radius if migrating:</strong> Low. Update image URLs in deployment manifests. Harbor is an excellent self-hosted alternative.<br><br><strong>Honest take:</strong> Container registries are one of the most portable infrastructure components. Images are OCI standard. ECR is convenient, not essential. Harbor gives you a self-hosted option with vulnerability scanning included.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 3\n\nContainer image registry. Basic and enhanced scanning (via Inspector) for vulnerabilities. Image signing for provenance verification. Lifecycle policies for image cleanup. Encryption at rest with KMS. Cross-account and cross-Region replication. Pull-through cache for public registries.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udfe2 Platform-Agnostic (Low Lock-In)\n\nMigration Difficulty: Low. Container images are portable by design. Change the registry URL in your deployment configs.\nControl Plane Dependency: 2/5 \u2014 Container registries are standardized (OCI). ECR-specific features (image scanning, lifecycle policies) are AWS-specific.\nData Gravity: 2/5 \u2014 Container images are OCI standard. Push/pull to any registry.\nRuntime Coupling: 2/5 \u2014 Image pull references ECR URLs. Update URLs to point to new registry.\n\nWhy migration is hard: ECR image scanning (via Inspector) and lifecycle policies need recreation. IAM-based pull authentication is AWS-specific. Cross-Region replication configs don't transfer.\nWhy migration could work: Container images are OCI standard. Docker push/pull works with any registry. GCR, ACR, Docker Hub, Harbor \u2014 they all store the same images.\nDependency risks: Inspector (scanning), EC2/ECS/EKS (consumers), IAM (authentication), KMS (encryption).\nAlternatives: Docker Hub, GCP Artifact Registry, Azure Container Registry, Harbor (self-hosted, open-source), GitHub Container Registry\nBlast radius if migrating: Low. Update image URLs in deployment manifests. Harbor is an excellent self-hosted alternative.\n\nHonest take: Container registries are one of the most portable infrastructure components. Images are OCI standard. ECR is convenient, not essential. Harbor gives you a self-hosted option with vulnerability scanning included.",
      "tags": [
        "platform-agnostic"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-dlm",
      "nodeId": "dlm",
      "content": "<p><strong>Domains: 5</strong><br><br>Automated EBS snapshot and AMI lifecycle policies. Schedule creation, retention, and cross-Region/cross-account copy of snapshots. Works alongside AWS Backup for complementary coverage. Domain 5 Skill 5.2.4.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udfe1 Cloud-Portable (Medium Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Low. Snapshot automation is simple scripting on any platform.<br><strong>Control Plane Dependency:</strong> 3/5 \u2014 EBS snapshot automation. AWS-specific resource type.<br><strong>Data Gravity:</strong> 2/5 \u2014 EBS snapshots can be exported as VHD/VMDK.<br><strong>Runtime Coupling:</strong> 1/5 \u2014 Scheduled, not runtime.<br><br><strong>Why migration is hard:</strong> DLM policies reference EBS volume tags and AWS-specific lifecycle rules.<br><strong>Why migration could work:</strong> Any scripting language or cron job can automate snapshots. This is not complex functionality.<br><strong>Dependency risks:</strong> EBS (volumes to snapshot), EC2 (AMI lifecycle), S3 (for exported snapshots).<br><strong>Alternatives:</strong> Custom scripts, Veeam, Azure Backup, GCP scheduled snapshots, Terraform with lifecycle management<br><strong>Blast radius if migrating:</strong> Low. Trivially replaceable.<br><br><strong>Honest take:</strong> DLM is a convenience. A cron job + CLI does the same thing. Not a lock-in concern.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 5\n\nAutomated EBS snapshot and AMI lifecycle policies. Schedule creation, retention, and cross-Region/cross-account copy of snapshots. Works alongside AWS Backup for complementary coverage. Domain 5 Skill 5.2.4.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udfe1 Cloud-Portable (Medium Lock-In)\n\nMigration Difficulty: Low. Snapshot automation is simple scripting on any platform.\nControl Plane Dependency: 3/5 \u2014 EBS snapshot automation. AWS-specific resource type.\nData Gravity: 2/5 \u2014 EBS snapshots can be exported as VHD/VMDK.\nRuntime Coupling: 1/5 \u2014 Scheduled, not runtime.\n\nWhy migration is hard: DLM policies reference EBS volume tags and AWS-specific lifecycle rules.\nWhy migration could work: Any scripting language or cron job can automate snapshots. This is not complex functionality.\nDependency risks: EBS (volumes to snapshot), EC2 (AMI lifecycle), S3 (for exported snapshots).\nAlternatives: Custom scripts, Veeam, Azure Backup, GCP scheduled snapshots, Terraform with lifecycle management\nBlast radius if migrating: Low. Trivially replaceable.\n\nHonest take: DLM is a convenience. A cron job + CLI does the same thing. Not a lock-in concern.",
      "tags": [
        "cloud-portable"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-ebs",
      "nodeId": "ebs",
      "content": "<p><strong>Domains: 2, 5</strong><br><br>Block storage for EC2. Encryption at rest with KMS (default encryption settable per account/Region). Snapshots encrypted with same key. Cross-account snapshot sharing requires key policy grants. Domain 2: snapshot compromised volumes for forensic analysis before termination. Domain 5: encryption and lifecycle management.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udfe1 Cloud-Portable (Medium Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Medium. Disk images are exportable. Volume determines time.<br><strong>Control Plane Dependency:</strong> 3/5 \u2014 Block storage is universal. EBS-specific: snapshot management, volume types, and encryption configuration.<br><strong>Data Gravity:</strong> 3/5 \u2014 Snapshots exportable as VHD/VMDK. But multi-terabyte exports are slow.<br><strong>Runtime Coupling:</strong> 2/5 \u2014 Block storage is transparent to applications. They see a disk.<br><br><strong>Why migration is hard:</strong> EBS snapshot format is AWS-specific. Cross-account snapshot sharing uses KMS grants. io2 Block Express and gp3 performance characteristics are AWS-specific.<br><strong>Why migration could work:</strong> Block storage is block storage. Export as VHD/VMDK. Import to Azure Managed Disks or GCP Persistent Disks.<br><strong>Dependency risks:</strong> EC2 (attached), KMS (encryption), DLM/Backup (lifecycle), Inspector (scanning).<br><strong>Alternatives:</strong> Azure Managed Disks, GCP Persistent Disks, any SAN/NAS, local NVMe<br><strong>Blast radius if migrating:</strong> Medium. Data export is the bottleneck, not technology compatibility.<br><br><strong>Honest take:</strong> Block storage is inherently portable \u2014 it's a virtual disk. The lock-in is in the data volume and the KMS encryption (need to decrypt to export). If your volumes are encrypted with default KMS, factor in decrypt-export-re-encrypt time.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 2, 5\n\nBlock storage for EC2. Encryption at rest with KMS (default encryption settable per account/Region). Snapshots encrypted with same key. Cross-account snapshot sharing requires key policy grants. Domain 2: snapshot compromised volumes for forensic analysis before termination. Domain 5: encryption and lifecycle management.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udfe1 Cloud-Portable (Medium Lock-In)\n\nMigration Difficulty: Medium. Disk images are exportable. Volume determines time.\nControl Plane Dependency: 3/5 \u2014 Block storage is universal. EBS-specific: snapshot management, volume types, and encryption configuration.\nData Gravity: 3/5 \u2014 Snapshots exportable as VHD/VMDK. But multi-terabyte exports are slow.\nRuntime Coupling: 2/5 \u2014 Block storage is transparent to applications. They see a disk.\n\nWhy migration is hard: EBS snapshot format is AWS-specific. Cross-account snapshot sharing uses KMS grants. io2 Block Express and gp3 performance characteristics are AWS-specific.\nWhy migration could work: Block storage is block storage. Export as VHD/VMDK. Import to Azure Managed Disks or GCP Persistent Disks.\nDependency risks: EC2 (attached), KMS (encryption), DLM/Backup (lifecycle), Inspector (scanning).\nAlternatives: Azure Managed Disks, GCP Persistent Disks, any SAN/NAS, local NVMe\nBlast radius if migrating: Medium. Data export is the bottleneck, not technology compatibility.\n\nHonest take: Block storage is inherently portable \u2014 it's a virtual disk. The lock-in is in the data volume and the KMS encryption (need to decrypt to export). If your volumes are encrypted with default KMS, factor in decrypt-export-re-encrypt time.",
      "tags": [
        "cloud-portable"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-q-developer",
      "nodeId": "q-developer",
      "content": "<p><strong>Domains: 3</strong><br><br>AI-powered code assistant with security scanning capabilities. Discovers and remediates vulnerabilities in CI/CD pipelines (Domain 3 Skill 3.2.6). Replaces Amazon CodeGuru Security for code analysis. New in C03.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udfe1 Cloud-Portable (Medium Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Low. Developer tools are swappable.<br><strong>Control Plane Dependency:</strong> 4/5 \u2014 AWS-specific code assistant integrated into IDE.<br><strong>Data Gravity:</strong> 1/5 \u2014 No data stored. Code is yours.<br><strong>Runtime Coupling:</strong> 1/5 \u2014 Developer tool, not runtime dependency. Code output is standard.<br><br><strong>Why migration is hard:</strong> IDE integration and security scanning configurations are AWS-specific.<br><strong>Why migration could work:</strong> GitHub Copilot, Cursor, Cody, and dozens of AI code assistants exist. Developer tools have zero switching cost.<br><strong>Dependency risks:</strong> None at runtime.<br><strong>Alternatives:</strong> GitHub Copilot, Cursor, Sourcegraph Cody, Tabnine, JetBrains AI, Claude for code<br><strong>Blast radius if migrating:</strong> None. Developer tool. No runtime dependency.<br><br><strong>Honest take:</strong> Not a lock-in risk. Use whatever code assistant works best. They're all interchangeable.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 3\n\nAI-powered code assistant with security scanning capabilities. Discovers and remediates vulnerabilities in CI/CD pipelines (Domain 3 Skill 3.2.6). Replaces Amazon CodeGuru Security for code analysis. New in C03.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udfe1 Cloud-Portable (Medium Lock-In)\n\nMigration Difficulty: Low. Developer tools are swappable.\nControl Plane Dependency: 4/5 \u2014 AWS-specific code assistant integrated into IDE.\nData Gravity: 1/5 \u2014 No data stored. Code is yours.\nRuntime Coupling: 1/5 \u2014 Developer tool, not runtime dependency. Code output is standard.\n\nWhy migration is hard: IDE integration and security scanning configurations are AWS-specific.\nWhy migration could work: GitHub Copilot, Cursor, Cody, and dozens of AI code assistants exist. Developer tools have zero switching cost.\nDependency risks: None at runtime.\nAlternatives: GitHub Copilot, Cursor, Sourcegraph Cody, Tabnine, JetBrains AI, Claude for code\nBlast radius if migrating: None. Developer tool. No runtime dependency.\n\nHonest take: Not a lock-in risk. Use whatever code assistant works best. They're all interchangeable.",
      "tags": [
        "cloud-portable"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-codeguru",
      "nodeId": "codeguru",
      "content": "<p><strong>Domains: 3</strong><br><br>Static application security testing (SAST) for code repositories. Detects security vulnerabilities, hardcoded secrets, and code quality issues. Domain 3 Skill 3.2.6 \u2014 discover and remediate vulnerabilities within a pipeline.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udfe1 Cloud-Portable (Medium Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Low. SAST tools are commoditized.<br><strong>Control Plane Dependency:</strong> 3/5 \u2014 AWS-specific SAST tool, but static analysis is a standard concept.<br><strong>Data Gravity:</strong> 1/5 \u2014 Findings are informational. Code is yours.<br><strong>Runtime Coupling:</strong> 1/5 \u2014 CI/CD integration, not runtime.<br><br><strong>Why migration is hard:</strong> CodeGuru integration with AWS CI/CD (CodePipeline, CodeBuild) is AWS-specific.<br><strong>Why migration could work:</strong> Snyk, SonarQube, Checkmarx, Semgrep \u2014 the SAST market is massive and competitive.<br><strong>Dependency risks:</strong> CI/CD pipeline.<br><strong>Alternatives:</strong> Snyk (excellent, multi-platform), SonarQube (open-source), Checkmarx, Semgrep (open-source), Veracode<br><strong>Blast radius if migrating:</strong> None. Replace SAST scanner in CI/CD config.<br><br><strong>Honest take:</strong> Use Snyk or Semgrep. Both are better products, work everywhere, and have larger rule databases. CodeGuru is a me-too product.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 3\n\nStatic application security testing (SAST) for code repositories. Detects security vulnerabilities, hardcoded secrets, and code quality issues. Domain 3 Skill 3.2.6 \u2014 discover and remediate vulnerabilities within a pipeline.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udfe1 Cloud-Portable (Medium Lock-In)\n\nMigration Difficulty: Low. SAST tools are commoditized.\nControl Plane Dependency: 3/5 \u2014 AWS-specific SAST tool, but static analysis is a standard concept.\nData Gravity: 1/5 \u2014 Findings are informational. Code is yours.\nRuntime Coupling: 1/5 \u2014 CI/CD integration, not runtime.\n\nWhy migration is hard: CodeGuru integration with AWS CI/CD (CodePipeline, CodeBuild) is AWS-specific.\nWhy migration could work: Snyk, SonarQube, Checkmarx, Semgrep \u2014 the SAST market is massive and competitive.\nDependency risks: CI/CD pipeline.\nAlternatives: Snyk (excellent, multi-platform), SonarQube (open-source), Checkmarx, Semgrep (open-source), Veracode\nBlast radius if migrating: None. Replace SAST scanner in CI/CD config.\n\nHonest take: Use Snyk or Semgrep. Both are better products, work everywhere, and have larger rule databases. CodeGuru is a me-too product.",
      "tags": [
        "cloud-portable"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-eks",
      "nodeId": "eks",
      "content": "<p><strong>Domains: 3, 5</strong><br><br>Managed Kubernetes. Pod security standards, RBAC, IRSA (IAM Roles for Service Accounts). GuardDuty EKS audit log monitoring and runtime monitoring. Inter-node encryption in transit (Domain 5 Skill 5.1.3). ECR for container images. Inspector for container vulnerability scanning.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udfe2 Platform-Agnostic (Low Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Low. Kubernetes manifests are portable across EKS, AKS, GKE, and self-hosted.<br><strong>Control Plane Dependency:</strong> 2/5 \u2014 Kubernetes is an open standard. EKS is a managed wrapper. IRSA is AWS-specific, but standard K8s RBAC is portable.<br><strong>Data Gravity:</strong> 1/5 \u2014 Workload definitions (YAML manifests) are Kubernetes standard.<br><strong>Runtime Coupling:</strong> 2/5 \u2014 Applications use Kubernetes APIs, not AWS APIs. IRSA and EBS CSI driver are AWS-specific but have portable alternatives.<br><br><strong>Why migration is hard:</strong> IRSA (IAM Roles for Service Accounts) is AWS-specific identity binding. EBS CSI driver, ALB Ingress Controller, and AWS-specific storage classes need replacement. VPC CNI is AWS-specific.<br><strong>Why migration could work:</strong> Kubernetes is Kubernetes. Manifests work on any K8s cluster. Replace IRSA with GCP Workload Identity or Azure Workload Identity. Replace ALB Ingress with Nginx Ingress. This is well-documented.<br><strong>Dependency risks:</strong> IAM (IRSA), VPC (networking), EBS (storage), ALB (ingress), ECR (images).<br><strong>Alternatives:</strong> GKE (Google), AKS (Azure), self-hosted Kubernetes, Rancher, OpenShift, k3s<br><strong>Blast radius if migrating:</strong> Low. Kubernetes portability is real. The AWS-specific add-ons need swapping, but manifests are portable.<br><br><strong>Honest take:</strong> EKS is one of AWS's most portable services because Kubernetes IS the portability layer. If you've avoided AWS-specific controllers (use Nginx Ingress instead of ALB Controller, use CSI standard instead of EBS-specific), migration is surprisingly straightforward.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 3, 5\n\nManaged Kubernetes. Pod security standards, RBAC, IRSA (IAM Roles for Service Accounts). GuardDuty EKS audit log monitoring and runtime monitoring. Inter-node encryption in transit (Domain 5 Skill 5.1.3). ECR for container images. Inspector for container vulnerability scanning.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udfe2 Platform-Agnostic (Low Lock-In)\n\nMigration Difficulty: Low. Kubernetes manifests are portable across EKS, AKS, GKE, and self-hosted.\nControl Plane Dependency: 2/5 \u2014 Kubernetes is an open standard. EKS is a managed wrapper. IRSA is AWS-specific, but standard K8s RBAC is portable.\nData Gravity: 1/5 \u2014 Workload definitions (YAML manifests) are Kubernetes standard.\nRuntime Coupling: 2/5 \u2014 Applications use Kubernetes APIs, not AWS APIs. IRSA and EBS CSI driver are AWS-specific but have portable alternatives.\n\nWhy migration is hard: IRSA (IAM Roles for Service Accounts) is AWS-specific identity binding. EBS CSI driver, ALB Ingress Controller, and AWS-specific storage classes need replacement. VPC CNI is AWS-specific.\nWhy migration could work: Kubernetes is Kubernetes. Manifests work on any K8s cluster. Replace IRSA with GCP Workload Identity or Azure Workload Identity. Replace ALB Ingress with Nginx Ingress. This is well-documented.\nDependency risks: IAM (IRSA), VPC (networking), EBS (storage), ALB (ingress), ECR (images).\nAlternatives: GKE (Google), AKS (Azure), self-hosted Kubernetes, Rancher, OpenShift, k3s\nBlast radius if migrating: Low. Kubernetes portability is real. The AWS-specific add-ons need swapping, but manifests are portable.\n\nHonest take: EKS is one of AWS's most portable services because Kubernetes IS the portability layer. If you've avoided AWS-specific controllers (use Nginx Ingress instead of ALB Controller, use CSI standard instead of EBS-specific), migration is surprisingly straightforward.",
      "tags": [
        "platform-agnostic"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-emr",
      "nodeId": "emr",
      "content": "<p><strong>Domains: 5</strong><br><br>Big data processing. Security: Kerberos authentication, encryption in transit (TLS) between nodes, encryption at rest with KMS. Lake Formation integration for fine-grained access. Domain 5 Skill 5.1.3 \u2014 inter-resource encryption in transit.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udfe1 Cloud-Portable (Medium Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Medium. Spark workloads are portable. Cluster management needs replacement.<br><strong>Control Plane Dependency:</strong> 4/5 \u2014 Spark/Hadoop concepts are open-source. EMR cluster configuration and step management are AWS-specific.<br><strong>Data Gravity:</strong> 2/5 \u2014 Data is in S3 (portable). Processing logic (Spark jobs) is open-source.<br><strong>Runtime Coupling:</strong> 2/5 \u2014 Spark jobs are Spark jobs. The cluster management is the AWS part.<br><br><strong>Why migration is hard:</strong> EMR-specific configurations (instance fleets, managed scaling, EMR Studio, Spark optimization settings) are AWS-specific.<br><strong>Why migration could work:</strong> Apache Spark runs everywhere: Databricks (multi-cloud), GCP Dataproc, Azure HDInsight, self-hosted. Your Spark code is portable.<br><strong>Dependency risks:</strong> S3 (data storage), IAM (access), KMS (encryption), VPC (networking).<br><strong>Alternatives:</strong> Databricks (multi-cloud, excellent), GCP Dataproc, Azure HDInsight, self-hosted Spark on Kubernetes<br><strong>Blast radius if migrating:</strong> Low for Spark workloads. Medium for EMR-specific features (managed notebooks, step API).<br><br><strong>Honest take:</strong> Use Databricks if you want portable Spark. It works on all three major clouds and is a better product than EMR. Your Spark code is the same regardless of platform.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 5\n\nBig data processing. Security: Kerberos authentication, encryption in transit (TLS) between nodes, encryption at rest with KMS. Lake Formation integration for fine-grained access. Domain 5 Skill 5.1.3 \u2014 inter-resource encryption in transit.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udfe1 Cloud-Portable (Medium Lock-In)\n\nMigration Difficulty: Medium. Spark workloads are portable. Cluster management needs replacement.\nControl Plane Dependency: 4/5 \u2014 Spark/Hadoop concepts are open-source. EMR cluster configuration and step management are AWS-specific.\nData Gravity: 2/5 \u2014 Data is in S3 (portable). Processing logic (Spark jobs) is open-source.\nRuntime Coupling: 2/5 \u2014 Spark jobs are Spark jobs. The cluster management is the AWS part.\n\nWhy migration is hard: EMR-specific configurations (instance fleets, managed scaling, EMR Studio, Spark optimization settings) are AWS-specific.\nWhy migration could work: Apache Spark runs everywhere: Databricks (multi-cloud), GCP Dataproc, Azure HDInsight, self-hosted. Your Spark code is portable.\nDependency risks: S3 (data storage), IAM (access), KMS (encryption), VPC (networking).\nAlternatives: Databricks (multi-cloud, excellent), GCP Dataproc, Azure HDInsight, self-hosted Spark on Kubernetes\nBlast radius if migrating: Low for Spark workloads. Medium for EMR-specific features (managed notebooks, step API).\n\nHonest take: Use Databricks if you want portable Spark. It works on all three major clouds and is a better product than EMR. Your Spark code is the same regardless of platform.",
      "tags": [
        "cloud-portable"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-identity-center",
      "nodeId": "identity-center",
      "content": "<p><strong>Domains: 4</strong><br><br>Centralized SSO across all AWS accounts in your Organization. Permission sets define what users can do in each account. Federation with external IdPs via SAML 2.0 and OIDC. Built-in identity store or connect to AD. Troubleshooting: permission sets, user/group assignments, session duration (Domain 4 Skill 4.1.3). Integrates with Verified Access for zero-trust application access.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udd34 AWS-Proprietary (High Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> High. SSO configuration is always platform-specific. But the identity source (external IdP) is usually portable.<br><strong>Control Plane Dependency:</strong> 5/5 \u2014 AWS-proprietary SSO. Permission sets, user/group assignments, and the identity store are all AWS-specific.<br><strong>Data Gravity:</strong> 2/5 \u2014 User identities may be in external IdP (portable) or AWS identity store (locked in).<br><strong>Runtime Coupling:</strong> 4/5 \u2014 Applications using Identity Center for SAML federation reference AWS-specific entityIDs and metadata URLs.<br><br><strong>Why migration is hard:</strong> Permission sets are AWS-specific role definitions. Account assignments map users to AWS accounts \u2014 a concept that doesn't exist elsewhere. If using the built-in identity store, you're storing identities in AWS.<br><strong>Why migration could work:</strong> If your identity source is an external IdP (Okta, Azure AD, Google Workspace), the identities are portable. You'd reconfigure SAML/OIDC federation on the new platform.<br><strong>Dependency risks:</strong> Organizations (account structure), IAM (permission sets become roles), Verified Access (trust provider).<br><strong>Alternatives:</strong> Okta, Azure Entra ID, Google Workspace, Ping Identity, JumpCloud \u2014 as IdPs that federate to any cloud<br><strong>Blast radius if migrating:</strong> High for AWS-specific access patterns. Low if your source of truth is an external IdP.<br><br><strong>Honest take:</strong> The lock-in mitigation is simple: use an external IdP (Okta, Azure AD) as your identity source, not the IAM Identity Center built-in store. Then your identities are portable and you only reconfigure federation.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 4\n\nCentralized SSO across all AWS accounts in your Organization. Permission sets define what users can do in each account. Federation with external IdPs via SAML 2.0 and OIDC. Built-in identity store or connect to AD. Troubleshooting: permission sets, user/group assignments, session duration (Domain 4 Skill 4.1.3). Integrates with Verified Access for zero-trust application access.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udd34 AWS-Proprietary (High Lock-In)\n\nMigration Difficulty: High. SSO configuration is always platform-specific. But the identity source (external IdP) is usually portable.\nControl Plane Dependency: 5/5 \u2014 AWS-proprietary SSO. Permission sets, user/group assignments, and the identity store are all AWS-specific.\nData Gravity: 2/5 \u2014 User identities may be in external IdP (portable) or AWS identity store (locked in).\nRuntime Coupling: 4/5 \u2014 Applications using Identity Center for SAML federation reference AWS-specific entityIDs and metadata URLs.\n\nWhy migration is hard: Permission sets are AWS-specific role definitions. Account assignments map users to AWS accounts \u2014 a concept that doesn't exist elsewhere. If using the built-in identity store, you're storing identities in AWS.\nWhy migration could work: If your identity source is an external IdP (Okta, Azure AD, Google Workspace), the identities are portable. You'd reconfigure SAML/OIDC federation on the new platform.\nDependency risks: Organizations (account structure), IAM (permission sets become roles), Verified Access (trust provider).\nAlternatives: Okta, Azure Entra ID, Google Workspace, Ping Identity, JumpCloud \u2014 as IdPs that federate to any cloud\nBlast radius if migrating: High for AWS-specific access patterns. Low if your source of truth is an external IdP.\n\nHonest take: The lock-in mitigation is simple: use an external IdP (Okta, Azure AD) as your identity source, not the IAM Identity Center built-in store. Then your identities are portable and you only reconfigure federation.",
      "tags": [
        "aws-proprietary"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-cognito",
      "nodeId": "cognito",
      "content": "<p><strong>Domains: 4</strong><br><br>User Pools: application-level authentication with sign-up/sign-in, MFA, password policies, hosted UI, Lambda triggers for customization. Federation with social and SAML IdPs. Identity Pools: federated access to AWS resources (S3, DynamoDB, API Gateway). Token-based auth (ID token, access token, refresh token). Troubleshooting auth issues (Domain 4 Skill 4.1.3). WAF integration for bot protection.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udfe1 Cloud-Portable (Medium Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Medium-High. User migration is the hard part \u2014 you can't export password hashes, so users must reset passwords.<br><strong>Control Plane Dependency:</strong> 4/5 \u2014 User Pools and Identity Pools are AWS-proprietary. But standard protocols (OAuth2/OIDC/SAML) are used.<br><strong>Data Gravity:</strong> 4/5 \u2014 User database (emails, passwords, MFA settings) lives in Cognito. Export is possible but painful (no bulk export API for passwords).<br><strong>Runtime Coupling:</strong> 4/5 \u2014 Application code references Cognito-specific endpoints, token structures, and Lambda trigger hooks.<br><br><strong>Why migration is hard:</strong> User pool data (password hashes) CANNOT be exported. Users must reset passwords during migration. Lambda triggers (pre-sign-up, post-confirmation) are AWS-specific hooks. Custom attributes and advanced security features are proprietary.<br><strong>Why migration could work:</strong> Cognito uses standard OAuth2/OIDC. If your app uses OIDC tokens, switching the IdP is config, not code. Auth0, Firebase Auth, Keycloak all speak the same protocols.<br><strong>Dependency risks:</strong> API Gateway (authorizer), Lambda (triggers), IAM (Identity Pool roles), ALB (authentication).<br><strong>Alternatives:</strong> Auth0 (multi-cloud, excellent), Firebase Authentication, Keycloak (open-source, self-hosted), Azure AD B2C, FusionAuth<br><strong>Blast radius if migrating:</strong> High due to user migration (password resets). Medium for application code changes (if using standard OIDC).<br><br><strong>Honest take:</strong> Cognito's real lock-in is the user database. You can't export password hashes, so every user must reset their password during migration. If you're starting fresh, use Auth0 or Keycloak \u2014 both are more portable and frankly better products. Cognito's UI and developer experience are widely criticized.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 4\n\nUser Pools: application-level authentication with sign-up/sign-in, MFA, password policies, hosted UI, Lambda triggers for customization. Federation with social and SAML IdPs. Identity Pools: federated access to AWS resources (S3, DynamoDB, API Gateway). Token-based auth (ID token, access token, refresh token). Troubleshooting auth issues (Domain 4 Skill 4.1.3). WAF integration for bot protection.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udfe1 Cloud-Portable (Medium Lock-In)\n\nMigration Difficulty: Medium-High. User migration is the hard part \u2014 you can't export password hashes, so users must reset passwords.\nControl Plane Dependency: 4/5 \u2014 User Pools and Identity Pools are AWS-proprietary. But standard protocols (OAuth2/OIDC/SAML) are used.\nData Gravity: 4/5 \u2014 User database (emails, passwords, MFA settings) lives in Cognito. Export is possible but painful (no bulk export API for passwords).\nRuntime Coupling: 4/5 \u2014 Application code references Cognito-specific endpoints, token structures, and Lambda trigger hooks.\n\nWhy migration is hard: User pool data (password hashes) CANNOT be exported. Users must reset passwords during migration. Lambda triggers (pre-sign-up, post-confirmation) are AWS-specific hooks. Custom attributes and advanced security features are proprietary.\nWhy migration could work: Cognito uses standard OAuth2/OIDC. If your app uses OIDC tokens, switching the IdP is config, not code. Auth0, Firebase Auth, Keycloak all speak the same protocols.\nDependency risks: API Gateway (authorizer), Lambda (triggers), IAM (Identity Pool roles), ALB (authentication).\nAlternatives: Auth0 (multi-cloud, excellent), Firebase Authentication, Keycloak (open-source, self-hosted), Azure AD B2C, FusionAuth\nBlast radius if migrating: High due to user migration (password resets). Medium for application code changes (if using standard OIDC).\n\nHonest take: Cognito's real lock-in is the user database. You can't export password hashes, so every user must reset their password during migration. If you're starting fresh, use Auth0 or Keycloak \u2014 both are more portable and frankly better products. Cognito's UI and developer experience are widely criticized.",
      "tags": [
        "cloud-portable"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-sts",
      "nodeId": "sts",
      "content": "<p><strong>Domains: 4</strong><br><br>Issues temporary security credentials. Key API calls: AssumeRole (cross-account), AssumeRoleWithSAML (enterprise federation), AssumeRoleWithWebIdentity (web/mobile apps), GetSessionToken (MFA-protected API access), GetFederationToken. Session policies scope down assumed role permissions. Trust policies on roles control WHO can assume them. S3 presigned URLs use STS credentials for time-limited object access.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udfe1 Cloud-Portable (Medium Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Medium. The authentication patterns need redesign, but the concept of temporary credentials is universal.<br><strong>Control Plane Dependency:</strong> 4/5 \u2014 AssumeRole, session tokens, and trust policies are AWS-specific. But temporary credentials is a universal concept.<br><strong>Data Gravity:</strong> 1/5 \u2014 No persistent data. Tokens are ephemeral.<br><strong>Runtime Coupling:</strong> 4/5 \u2014 Application code calling AssumeRole, presigned URL generation, and cross-account access patterns are AWS-specific.<br><br><strong>Why migration is hard:</strong> Every cross-account role assumption, SAML federation, and web identity federation call is STS-specific. Presigned URLs depend on STS credentials.<br><strong>Why migration could work:</strong> Azure managed identities, GCP service accounts, and self-hosted token services provide equivalent temporary credential mechanisms.<br><strong>Dependency risks:</strong> IAM (roles and trust policies), Identity Center (federation), Cognito (web identity), S3 (presigned URLs).<br><strong>Alternatives:</strong> Azure managed identities + token endpoint, GCP service account key-less authentication, HashiCorp Vault dynamic secrets<br><strong>Blast radius if migrating:</strong> Medium. Authentication patterns need redesign but the concept transfers.<br><br><strong>Honest take:</strong> STS is the mechanism that makes IAM roles work. The concept of temporary, scoped credentials is universal and exists on every platform. The implementation is AWS-specific, but the patterns translate.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 4\n\nIssues temporary security credentials. Key API calls: AssumeRole (cross-account), AssumeRoleWithSAML (enterprise federation), AssumeRoleWithWebIdentity (web/mobile apps), GetSessionToken (MFA-protected API access), GetFederationToken. Session policies scope down assumed role permissions. Trust policies on roles control WHO can assume them. S3 presigned URLs use STS credentials for time-limited object access.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udfe1 Cloud-Portable (Medium Lock-In)\n\nMigration Difficulty: Medium. The authentication patterns need redesign, but the concept of temporary credentials is universal.\nControl Plane Dependency: 4/5 \u2014 AssumeRole, session tokens, and trust policies are AWS-specific. But temporary credentials is a universal concept.\nData Gravity: 1/5 \u2014 No persistent data. Tokens are ephemeral.\nRuntime Coupling: 4/5 \u2014 Application code calling AssumeRole, presigned URL generation, and cross-account access patterns are AWS-specific.\n\nWhy migration is hard: Every cross-account role assumption, SAML federation, and web identity federation call is STS-specific. Presigned URLs depend on STS credentials.\nWhy migration could work: Azure managed identities, GCP service accounts, and self-hosted token services provide equivalent temporary credential mechanisms.\nDependency risks: IAM (roles and trust policies), Identity Center (federation), Cognito (web identity), S3 (presigned URLs).\nAlternatives: Azure managed identities + token endpoint, GCP service account key-less authentication, HashiCorp Vault dynamic secrets\nBlast radius if migrating: Medium. Authentication patterns need redesign but the concept transfers.\n\nHonest take: STS is the mechanism that makes IAM roles work. The concept of temporary, scoped credentials is universal and exists on every platform. The implementation is AWS-specific, but the patterns translate.",
      "tags": [
        "cloud-portable"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-directory-service",
      "nodeId": "directory-service",
      "content": "<p><strong>Domains: 4</strong><br><br>Managed Microsoft Active Directory for hybrid environments. AD Connector: proxy for on-prem AD (no data replicated to cloud). Simple AD: basic AD-compatible directory. Managed Microsoft AD: full Windows AD, trust relationships with on-prem. Integration with IAM Identity Center, Amazon RDS, WorkSpaces, FSx. Domain 4 Skill 4.1.3 troubleshooting.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udfe2 Platform-Agnostic (Low Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Low-Medium. Active Directory is AD wherever it runs. The data is in standard format.<br><strong>Control Plane Dependency:</strong> 2/5 \u2014 Microsoft Active Directory is a standard protocol. AWS just manages the infrastructure.<br><strong>Data Gravity:</strong> 3/5 \u2014 AD objects (users, groups, GPOs) are in standard LDAP/AD format. Replication to on-prem is common.<br><strong>Runtime Coupling:</strong> 2/5 \u2014 Applications use standard LDAP/Kerberos. Not AWS-specific.<br><br><strong>Why migration is hard:</strong> AWS Managed Microsoft AD trusts, AD Connector proxy configurations, and integration with IAM Identity Center are AWS-specific.<br><strong>Why migration could work:</strong> AD is AD. Replicate to on-prem, Azure AD DS, or self-hosted AD. LDAP and Kerberos work everywhere.<br><strong>Dependency risks:</strong> IAM Identity Center (federation), RDS (AD authentication), WorkSpaces, FSx for Windows.<br><strong>Alternatives:</strong> Azure Active Directory Domain Services, self-hosted Active Directory, FreeIPA (open-source), JumpCloud<br><strong>Blast radius if migrating:</strong> Low. AD replicates. Standard protocols. Well-understood migration path.<br><br><strong>Honest take:</strong> Directory Service is one of AWS's most portable managed services because it's literally running Microsoft's Active Directory. The protocol is the standard. Migration is well-understood.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 4\n\nManaged Microsoft Active Directory for hybrid environments. AD Connector: proxy for on-prem AD (no data replicated to cloud). Simple AD: basic AD-compatible directory. Managed Microsoft AD: full Windows AD, trust relationships with on-prem. Integration with IAM Identity Center, Amazon RDS, WorkSpaces, FSx. Domain 4 Skill 4.1.3 troubleshooting.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udfe2 Platform-Agnostic (Low Lock-In)\n\nMigration Difficulty: Low-Medium. Active Directory is AD wherever it runs. The data is in standard format.\nControl Plane Dependency: 2/5 \u2014 Microsoft Active Directory is a standard protocol. AWS just manages the infrastructure.\nData Gravity: 3/5 \u2014 AD objects (users, groups, GPOs) are in standard LDAP/AD format. Replication to on-prem is common.\nRuntime Coupling: 2/5 \u2014 Applications use standard LDAP/Kerberos. Not AWS-specific.\n\nWhy migration is hard: AWS Managed Microsoft AD trusts, AD Connector proxy configurations, and integration with IAM Identity Center are AWS-specific.\nWhy migration could work: AD is AD. Replicate to on-prem, Azure AD DS, or self-hosted AD. LDAP and Kerberos work everywhere.\nDependency risks: IAM Identity Center (federation), RDS (AD authentication), WorkSpaces, FSx for Windows.\nAlternatives: Azure Active Directory Domain Services, self-hosted Active Directory, FreeIPA (open-source), JumpCloud\nBlast radius if migrating: Low. AD replicates. Standard protocols. Well-understood migration path.\n\nHonest take: Directory Service is one of AWS's most portable managed services because it's literally running Microsoft's Active Directory. The protocol is the standard. Migration is well-understood.",
      "tags": [
        "platform-agnostic"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-verified-permissions",
      "nodeId": "verified-permissions",
      "content": "<p><strong>Domains: 4</strong><br><br>Fine-grained authorization for custom applications using Cedar policy language. Centralized policy store. Evaluates authorization requests against policies. New in C03 \u2014 Domain 4 Skill 4.2.1. Decouples authorization logic from application code. Schema-based policy validation. Integrates with Cognito for identity tokens.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udd34 AWS-Proprietary (High Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> High. Cedar is a new, AWS-specific policy language. Authorization logic needs rewriting.<br><strong>Control Plane Dependency:</strong> 5/5 \u2014 Cedar policy language is AWS-developed. Policy store, schema, and API are AWS-proprietary.<br><strong>Data Gravity:</strong> 1/5 \u2014 Policies are text. Exportable.<br><strong>Runtime Coupling:</strong> 5/5 \u2014 Application authorization calls go to Verified Permissions API. Every authz decision depends on it.<br><br><strong>Why migration is hard:</strong> Cedar policy language is AWS-created. While it's open-source, the ecosystem is nascent and the only production deployment is Verified Permissions. Authorization decisions embedded in application runtime are critical-path dependencies.<br><strong>Why migration could work:</strong> Cedar is open-source (technically portable). OPA (Open Policy Agent) with Rego is the industry-standard portable alternative with massive ecosystem support.<br><strong>Dependency risks:</strong> Cognito (identity tokens), API Gateway/Lambda (application integration).<br><strong>Alternatives:</strong> Open Policy Agent (OPA) with Rego (industry standard, portable), Casbin (open-source), SpiceDB (Zanzibar-inspired), Cerbos<br><strong>Blast radius if migrating:</strong> High. Every authorization decision in your application breaks. Runtime dependency.<br><br><strong>Honest take:</strong> Cedar is technically open-source, but using Verified Permissions means your authorization logic is an AWS runtime dependency. Use OPA/Rego instead \u2014 it's the industry standard, runs anywhere, and has 10x the ecosystem. OPA is simply the better bet for authorization.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 4\n\nFine-grained authorization for custom applications using Cedar policy language. Centralized policy store. Evaluates authorization requests against policies. New in C03 \u2014 Domain 4 Skill 4.2.1. Decouples authorization logic from application code. Schema-based policy validation. Integrates with Cognito for identity tokens.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udd34 AWS-Proprietary (High Lock-In)\n\nMigration Difficulty: High. Cedar is a new, AWS-specific policy language. Authorization logic needs rewriting.\nControl Plane Dependency: 5/5 \u2014 Cedar policy language is AWS-developed. Policy store, schema, and API are AWS-proprietary.\nData Gravity: 1/5 \u2014 Policies are text. Exportable.\nRuntime Coupling: 5/5 \u2014 Application authorization calls go to Verified Permissions API. Every authz decision depends on it.\n\nWhy migration is hard: Cedar policy language is AWS-created. While it's open-source, the ecosystem is nascent and the only production deployment is Verified Permissions. Authorization decisions embedded in application runtime are critical-path dependencies.\nWhy migration could work: Cedar is open-source (technically portable). OPA (Open Policy Agent) with Rego is the industry-standard portable alternative with massive ecosystem support.\nDependency risks: Cognito (identity tokens), API Gateway/Lambda (application integration).\nAlternatives: Open Policy Agent (OPA) with Rego (industry standard, portable), Casbin (open-source), SpiceDB (Zanzibar-inspired), Cerbos\nBlast radius if migrating: High. Every authorization decision in your application breaks. Runtime dependency.\n\nHonest take: Cedar is technically open-source, but using Verified Permissions means your authorization logic is an AWS runtime dependency. Use OPA/Rego instead \u2014 it's the industry standard, runs anywhere, and has 10x the ecosystem. OPA is simply the better bet for authorization.",
      "tags": [
        "aws-proprietary"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-iam-roles-anywhere",
      "nodeId": "iam-roles-anywhere",
      "content": "<p><strong>Domains: 4</strong><br><br>Extends IAM roles to workloads outside AWS using X.509 certificates. On-prem servers, IoT devices, and other external systems can obtain temporary AWS credentials. Trust anchors use ACM Private CA or external CAs. New in C03 \u2014 Domain 4 Skill 4.2.1 authorization controls.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udfe1 Cloud-Portable (Medium Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Medium. X.509-based workload identity is an emerging standard (SPIFFE/SPIRE). The certificates are portable.<br><strong>Control Plane Dependency:</strong> 4/5 \u2014 AWS-specific service for extending IAM to external workloads. Trust anchors reference ACM Private CA.<br><strong>Data Gravity:</strong> 1/5 \u2014 No data stored.<br><strong>Runtime Coupling:</strong> 3/5 \u2014 External workloads obtain temporary AWS credentials. The credential mechanism is AWS-specific, but X.509 certificates are standard.<br><br><strong>Why migration is hard:</strong> Trust anchors, profiles, and session policies are AWS-specific configuration. The IAM Roles Anywhere credential helper is an AWS agent.<br><strong>Why migration could work:</strong> X.509 certificates are standard. SPIFFE/SPIRE provides cloud-agnostic workload identity. HashiCorp Vault can issue certificates and credentials for any platform.<br><strong>Dependency risks:</strong> IAM (roles), ACM Private CA (trust anchors), X.509 certificates.<br><strong>Alternatives:</strong> SPIFFE/SPIRE (open standard for workload identity), HashiCorp Vault (multi-cloud secrets/identity), GCP Workload Identity Federation, Azure workload identity<br><strong>Blast radius if migrating:</strong> Medium. External workloads need credential reconfiguration.<br><br><strong>Honest take:</strong> IAM Roles Anywhere solves a real problem (on-prem workload identity) but the SPIFFE/SPIRE standard is the portable answer. Use SPIFFE if multi-cloud is on the roadmap.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 4\n\nExtends IAM roles to workloads outside AWS using X.509 certificates. On-prem servers, IoT devices, and other external systems can obtain temporary AWS credentials. Trust anchors use ACM Private CA or external CAs. New in C03 \u2014 Domain 4 Skill 4.2.1 authorization controls.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udfe1 Cloud-Portable (Medium Lock-In)\n\nMigration Difficulty: Medium. X.509-based workload identity is an emerging standard (SPIFFE/SPIRE). The certificates are portable.\nControl Plane Dependency: 4/5 \u2014 AWS-specific service for extending IAM to external workloads. Trust anchors reference ACM Private CA.\nData Gravity: 1/5 \u2014 No data stored.\nRuntime Coupling: 3/5 \u2014 External workloads obtain temporary AWS credentials. The credential mechanism is AWS-specific, but X.509 certificates are standard.\n\nWhy migration is hard: Trust anchors, profiles, and session policies are AWS-specific configuration. The IAM Roles Anywhere credential helper is an AWS agent.\nWhy migration could work: X.509 certificates are standard. SPIFFE/SPIRE provides cloud-agnostic workload identity. HashiCorp Vault can issue certificates and credentials for any platform.\nDependency risks: IAM (roles), ACM Private CA (trust anchors), X.509 certificates.\nAlternatives: SPIFFE/SPIRE (open standard for workload identity), HashiCorp Vault (multi-cloud secrets/identity), GCP Workload Identity Federation, Azure workload identity\nBlast radius if migrating: Medium. External workloads need credential reconfiguration.\n\nHonest take: IAM Roles Anywhere solves a real problem (on-prem workload identity) but the SPIFFE/SPIRE standard is the portable answer. Use SPIFFE if multi-cloud is on the roadmap.",
      "tags": [
        "cloud-portable"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-cloudhsm",
      "nodeId": "cloudhsm",
      "content": "<p><strong>Domains: 5</strong><br><br>Dedicated hardware security modules in AWS cloud. FIPS 140-2 Level 3 validated. You control the keys \u2014 AWS has NO access (unlike KMS where AWS manages infrastructure). Use cases: regulatory requirements for dedicated HSMs, custom key stores for KMS, SSL/TLS offloading, Oracle TDE, code signing. Cluster of 2+ HSMs for HA. VPC-based deployment. Can back a KMS custom key store for envelope encryption pattern with CloudHSM-held root keys.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udfe2 Platform-Agnostic (Low Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Low-Medium. PKCS#11 is a standard interface. Key material is exportable. This is how you avoid KMS lock-in.<br><strong>Control Plane Dependency:</strong> 3/5 \u2014 PKCS#11, JCE, and CNG interfaces are industry standards. HSM management is AWS-specific.<br><strong>Data Gravity:</strong> 2/5 \u2014 Key material CAN be exported (unlike KMS). This is CloudHSM's key advantage for portability.<br><strong>Runtime Coupling:</strong> 2/5 \u2014 Applications use standard cryptographic interfaces (PKCS#11), not AWS APIs.<br><br><strong>Why migration is hard:</strong> Cluster management, backup/restore, and user management are AWS-specific. Moving physical HSMs requires key ceremony.<br><strong>Why migration could work:</strong> PKCS#11 is the industry standard HSM interface. Applications using it work with any HSM: Thales Luna, Utimaco, self-hosted SoftHSM. Key material can be extracted and moved.<br><strong>Dependency risks:</strong> KMS custom key store (optional integration), VPC (deployment).<br><strong>Alternatives:</strong> Thales Luna HSM, Utimaco SecurityServer, Azure Dedicated HSM, GCP Cloud HSM, self-hosted HSMs, SoftHSM (dev/test)<br><strong>Blast radius if migrating:</strong> Low. PKCS#11 applications work with any HSM. Key material exports enable migration.<br><br><strong>Honest take:</strong> CloudHSM is the ANTI-lock-in choice for encryption. Unlike KMS, you can export key material. Use CloudHSM-backed KMS custom key stores if you want KMS convenience with portability escape hatch. This is the right answer for organizations that care about crypto-agility.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 5\n\nDedicated hardware security modules in AWS cloud. FIPS 140-2 Level 3 validated. You control the keys \u2014 AWS has NO access (unlike KMS where AWS manages infrastructure). Use cases: regulatory requirements for dedicated HSMs, custom key stores for KMS, SSL/TLS offloading, Oracle TDE, code signing. Cluster of 2+ HSMs for HA. VPC-based deployment. Can back a KMS custom key store for envelope encryption pattern with CloudHSM-held root keys.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udfe2 Platform-Agnostic (Low Lock-In)\n\nMigration Difficulty: Low-Medium. PKCS#11 is a standard interface. Key material is exportable. This is how you avoid KMS lock-in.\nControl Plane Dependency: 3/5 \u2014 PKCS#11, JCE, and CNG interfaces are industry standards. HSM management is AWS-specific.\nData Gravity: 2/5 \u2014 Key material CAN be exported (unlike KMS). This is CloudHSM's key advantage for portability.\nRuntime Coupling: 2/5 \u2014 Applications use standard cryptographic interfaces (PKCS#11), not AWS APIs.\n\nWhy migration is hard: Cluster management, backup/restore, and user management are AWS-specific. Moving physical HSMs requires key ceremony.\nWhy migration could work: PKCS#11 is the industry standard HSM interface. Applications using it work with any HSM: Thales Luna, Utimaco, self-hosted SoftHSM. Key material can be extracted and moved.\nDependency risks: KMS custom key store (optional integration), VPC (deployment).\nAlternatives: Thales Luna HSM, Utimaco SecurityServer, Azure Dedicated HSM, GCP Cloud HSM, self-hosted HSMs, SoftHSM (dev/test)\nBlast radius if migrating: Low. PKCS#11 applications work with any HSM. Key material exports enable migration.\n\nHonest take: CloudHSM is the ANTI-lock-in choice for encryption. Unlike KMS, you can export key material. Use CloudHSM-backed KMS custom key stores if you want KMS convenience with portability escape hatch. This is the right answer for organizations that care about crypto-agility.",
      "tags": [
        "platform-agnostic"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-acm",
      "nodeId": "acm",
      "content": "<p><strong>Domains: 5</strong><br><br>Provisions and manages TLS/SSL certificates. Free public certificates with auto-renewal. Automatic deployment to CloudFront, ELB, API Gateway. DNS or email validation. Cannot export ACM-managed private keys (enforces AWS-managed lifecycle). For private CAs or on-prem use \u2192 AWS Private Certificate Authority.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udfe1 Cloud-Portable (Medium Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Low-Medium. Re-issue certificates with new provider. TLS certificates are not unique data.<br><strong>Control Plane Dependency:</strong> 3/5 \u2014 Certificate management concept is universal. ACM-managed private keys cannot be exported.<br><strong>Data Gravity:</strong> 2/5 \u2014 Public certificates are free and re-issuable anywhere. Private key non-exportability is a design choice.<br><strong>Runtime Coupling:</strong> 2/5 \u2014 Certificates deploy to CloudFront, ELB, API Gateway. Standard TLS otherwise.<br><br><strong>Why migration is hard:</strong> ACM private keys cannot be exported. If you need the same key for non-AWS use, you can't get it. Auto-renewal integration is AWS-specific.<br><strong>Why migration could work:</strong> TLS certificates are reissuable. Let's Encrypt is free and works everywhere. Private certificates use ACM Private CA (separate analysis).<br><strong>Dependency risks:</strong> CloudFront, ELB, API Gateway (deployment targets), Route 53 (DNS validation).<br><strong>Alternatives:</strong> Let's Encrypt (free, automated, portable), DigiCert, GCP Certificate Manager, Azure Key Vault certificates, cert-manager (Kubernetes)<br><strong>Blast radius if migrating:</strong> Low. Re-issue certificates. Let's Encrypt makes this free and fast.<br><br><strong>Honest take:</strong> ACM is convenient but Let's Encrypt is free, portable, and works everywhere. The lock-in from ACM is minimal. Use cert-manager + Let's Encrypt in Kubernetes for maximum portability.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 5\n\nProvisions and manages TLS/SSL certificates. Free public certificates with auto-renewal. Automatic deployment to CloudFront, ELB, API Gateway. DNS or email validation. Cannot export ACM-managed private keys (enforces AWS-managed lifecycle). For private CAs or on-prem use \u2192 AWS Private Certificate Authority.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udfe1 Cloud-Portable (Medium Lock-In)\n\nMigration Difficulty: Low-Medium. Re-issue certificates with new provider. TLS certificates are not unique data.\nControl Plane Dependency: 3/5 \u2014 Certificate management concept is universal. ACM-managed private keys cannot be exported.\nData Gravity: 2/5 \u2014 Public certificates are free and re-issuable anywhere. Private key non-exportability is a design choice.\nRuntime Coupling: 2/5 \u2014 Certificates deploy to CloudFront, ELB, API Gateway. Standard TLS otherwise.\n\nWhy migration is hard: ACM private keys cannot be exported. If you need the same key for non-AWS use, you can't get it. Auto-renewal integration is AWS-specific.\nWhy migration could work: TLS certificates are reissuable. Let's Encrypt is free and works everywhere. Private certificates use ACM Private CA (separate analysis).\nDependency risks: CloudFront, ELB, API Gateway (deployment targets), Route 53 (DNS validation).\nAlternatives: Let's Encrypt (free, automated, portable), DigiCert, GCP Certificate Manager, Azure Key Vault certificates, cert-manager (Kubernetes)\nBlast radius if migrating: Low. Re-issue certificates. Let's Encrypt makes this free and fast.\n\nHonest take: ACM is convenient but Let's Encrypt is free, portable, and works everywhere. The lock-in from ACM is minimal. Use cert-manager + Let's Encrypt in Kubernetes for maximum portability.",
      "tags": [
        "cloud-portable"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-private-ca",
      "nodeId": "private-ca",
      "content": "<p><strong>Domains: 5</strong><br><br>Managed private CA for issuing private certificates. Mutual TLS (mTLS), code signing, IoT device certificates. Multi-Region support for cross-Region private certificates (new C03 Skill 5.3.5). Integrates with IAM Roles Anywhere for on-prem workload authentication. Hierarchical CA structure (root + subordinate).<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udfe1 Cloud-Portable (Medium Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Medium. CA migration requires new root certificate trust \u2014 a significant operational change.<br><strong>Control Plane Dependency:</strong> 4/5 \u2014 AWS-managed CA infrastructure. Certificate issuance APIs are AWS-specific.<br><strong>Data Gravity:</strong> 3/5 \u2014 CA private key material cannot be exported. Issued certificates are standard X.509.<br><strong>Runtime Coupling:</strong> 3/5 \u2014 Applications use standard X.509 certificates, not AWS-specific interfaces.<br><br><strong>Why migration is hard:</strong> Root CA private key cannot be exported. All certificates chain to this CA. Migrating means issuing a new CA, re-issuing all certificates, and updating trust stores everywhere. This is an operational project, not a technical one.<br><strong>Why migration could work:</strong> X.509 certificates are standard. Self-hosted CAs (EJBCA, step-ca) work identically. The operational process for CA migration is well-documented.<br><strong>Dependency risks:</strong> IAM Roles Anywhere (trust anchors), mTLS configurations (ELB, API Gateway), code signing.<br><strong>Alternatives:</strong> EJBCA (open-source CA), step-ca (open-source, excellent), Azure Key Vault CA, GCP Certificate Authority Service, Venafi<br><strong>Blast radius if migrating:</strong> High operationally (re-issue all certificates, update trust stores). Low technically (standard X.509).<br><br><strong>Honest take:</strong> Private CA lock-in is real but manageable. The mitigation: use an intermediate CA under ACM Private CA so you can rotate the root. Or use step-ca (open-source) from day one \u2014 it's excellent.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 5\n\nManaged private CA for issuing private certificates. Mutual TLS (mTLS), code signing, IoT device certificates. Multi-Region support for cross-Region private certificates (new C03 Skill 5.3.5). Integrates with IAM Roles Anywhere for on-prem workload authentication. Hierarchical CA structure (root + subordinate).\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udfe1 Cloud-Portable (Medium Lock-In)\n\nMigration Difficulty: Medium. CA migration requires new root certificate trust \u2014 a significant operational change.\nControl Plane Dependency: 4/5 \u2014 AWS-managed CA infrastructure. Certificate issuance APIs are AWS-specific.\nData Gravity: 3/5 \u2014 CA private key material cannot be exported. Issued certificates are standard X.509.\nRuntime Coupling: 3/5 \u2014 Applications use standard X.509 certificates, not AWS-specific interfaces.\n\nWhy migration is hard: Root CA private key cannot be exported. All certificates chain to this CA. Migrating means issuing a new CA, re-issuing all certificates, and updating trust stores everywhere. This is an operational project, not a technical one.\nWhy migration could work: X.509 certificates are standard. Self-hosted CAs (EJBCA, step-ca) work identically. The operational process for CA migration is well-documented.\nDependency risks: IAM Roles Anywhere (trust anchors), mTLS configurations (ELB, API Gateway), code signing.\nAlternatives: EJBCA (open-source CA), step-ca (open-source, excellent), Azure Key Vault CA, GCP Certificate Authority Service, Venafi\nBlast radius if migrating: High operationally (re-issue all certificates, update trust stores). Low technically (standard X.509).\n\nHonest take: Private CA lock-in is real but manageable. The mitigation: use an intermediate CA under ACM Private CA so you can rotate the root. Or use step-ca (open-source) from day one \u2014 it's excellent.",
      "tags": [
        "cloud-portable"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-secrets-manager",
      "nodeId": "secrets-manager",
      "content": "<p><strong>Domains: 5</strong><br><br>Manages database credentials, API keys, OAuth tokens, and other secrets. Automatic rotation using Lambda functions (built-in for RDS, Redshift, DocumentDB). Cross-account sharing via resource policies. Encryption with KMS. Versioning (AWSCURRENT, AWSPREVIOUS, AWSPENDING). Domain 5 Skill 5.3.1 \u2014 credential management and rotation strategy.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udfe1 Cloud-Portable (Medium Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Medium. Secrets are exportable data. The rotation automation needs rebuilding.<br><strong>Control Plane Dependency:</strong> 4/5 \u2014 AWS-proprietary secret storage. But secrets are just key-value pairs.<br><strong>Data Gravity:</strong> 2/5 \u2014 Secrets are exportable via API. No bulk export, but scriptable.<br><strong>Runtime Coupling:</strong> 4/5 \u2014 Application code calls Secrets Manager API. Lambda rotation functions reference Secrets Manager ARNs.<br><br><strong>Why migration is hard:</strong> Lambda rotation functions are AWS-specific. Cross-account sharing via resource policies is AWS-specific. Integration with RDS (native rotation) is proprietary.<br><strong>Why migration could work:</strong> Secrets are just strings. Export via API, import to HashiCorp Vault, Azure Key Vault, or GCP Secret Manager. The data is trivially portable.<br><strong>Dependency risks:</strong> KMS (encryption), Lambda (rotation), RDS/Redshift (native rotation targets), application code (SDK calls).<br><strong>Alternatives:</strong> HashiCorp Vault (industry standard, multi-cloud), Azure Key Vault, GCP Secret Manager, CyberArk, Doppler<br><strong>Blast radius if migrating:</strong> Medium. Secrets are portable. Rotation automation needs rebuilding. Application code needs SDK change.<br><br><strong>Honest take:</strong> Use HashiCorp Vault if portability matters. It's the industry standard, works everywhere, and has better features (dynamic secrets, database credential generation, PKI). Secrets Manager is convenient within AWS but Vault is the better long-term investment.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 5\n\nManages database credentials, API keys, OAuth tokens, and other secrets. Automatic rotation using Lambda functions (built-in for RDS, Redshift, DocumentDB). Cross-account sharing via resource policies. Encryption with KMS. Versioning (AWSCURRENT, AWSPREVIOUS, AWSPENDING). Domain 5 Skill 5.3.1 \u2014 credential management and rotation strategy.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udfe1 Cloud-Portable (Medium Lock-In)\n\nMigration Difficulty: Medium. Secrets are exportable data. The rotation automation needs rebuilding.\nControl Plane Dependency: 4/5 \u2014 AWS-proprietary secret storage. But secrets are just key-value pairs.\nData Gravity: 2/5 \u2014 Secrets are exportable via API. No bulk export, but scriptable.\nRuntime Coupling: 4/5 \u2014 Application code calls Secrets Manager API. Lambda rotation functions reference Secrets Manager ARNs.\n\nWhy migration is hard: Lambda rotation functions are AWS-specific. Cross-account sharing via resource policies is AWS-specific. Integration with RDS (native rotation) is proprietary.\nWhy migration could work: Secrets are just strings. Export via API, import to HashiCorp Vault, Azure Key Vault, or GCP Secret Manager. The data is trivially portable.\nDependency risks: KMS (encryption), Lambda (rotation), RDS/Redshift (native rotation targets), application code (SDK calls).\nAlternatives: HashiCorp Vault (industry standard, multi-cloud), Azure Key Vault, GCP Secret Manager, CyberArk, Doppler\nBlast radius if migrating: Medium. Secrets are portable. Rotation automation needs rebuilding. Application code needs SDK change.\n\nHonest take: Use HashiCorp Vault if portability matters. It's the industry standard, works everywhere, and has better features (dynamic secrets, database credential generation, PKI). Secrets Manager is convenient within AWS but Vault is the better long-term investment.",
      "tags": [
        "cloud-portable"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-parameter-store",
      "nodeId": "parameter-store",
      "content": "<p><strong>Domains: 5</strong><br><br>Hierarchical configuration store. Standard and Advanced tiers. SecureString parameters encrypted with KMS. Free for standard parameters. Versioning and labels. Policies for expiration notifications. vs Secrets Manager: no built-in rotation (need custom Lambda), no cross-account sharing, but free, supports hierarchies, and integrates natively with CloudFormation/SSM.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udfe1 Cloud-Portable (Medium Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Low-Medium. Export parameters, import to any config management system.<br><strong>Control Plane Dependency:</strong> 3/5 \u2014 Simple key-value config store. AWS-specific API but the concept is universal.<br><strong>Data Gravity:</strong> 1/5 \u2014 Parameters are exportable strings.<br><strong>Runtime Coupling:</strong> 3/5 \u2014 Application code references parameter paths. SDK calls are AWS-specific.<br><br><strong>Why migration is hard:</strong> Hierarchical parameter paths, SecureString encryption with KMS, and IAM-based access control are AWS-specific.<br><strong>Why migration could work:</strong> Parameters are strings. Export them, import to HashiCorp Consul, Azure App Configuration, GCP Runtime Configurator, or environment variables.<br><strong>Dependency risks:</strong> KMS (SecureString encryption), IAM (access control), CloudFormation (dynamic references).<br><strong>Alternatives:</strong> HashiCorp Consul (portable), Azure App Configuration, GCP Runtime Configurator, environment variables, etcd<br><strong>Blast radius if migrating:</strong> Low. Configuration data is trivially portable.<br><br><strong>Honest take:</strong> Parameter Store is not a significant lock-in risk. The data is simple and exportable. Use environment variables or HashiCorp Consul if you want zero cloud coupling.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 5\n\nHierarchical configuration store. Standard and Advanced tiers. SecureString parameters encrypted with KMS. Free for standard parameters. Versioning and labels. Policies for expiration notifications. vs Secrets Manager: no built-in rotation (need custom Lambda), no cross-account sharing, but free, supports hierarchies, and integrates natively with CloudFormation/SSM.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udfe1 Cloud-Portable (Medium Lock-In)\n\nMigration Difficulty: Low-Medium. Export parameters, import to any config management system.\nControl Plane Dependency: 3/5 \u2014 Simple key-value config store. AWS-specific API but the concept is universal.\nData Gravity: 1/5 \u2014 Parameters are exportable strings.\nRuntime Coupling: 3/5 \u2014 Application code references parameter paths. SDK calls are AWS-specific.\n\nWhy migration is hard: Hierarchical parameter paths, SecureString encryption with KMS, and IAM-based access control are AWS-specific.\nWhy migration could work: Parameters are strings. Export them, import to HashiCorp Consul, Azure App Configuration, GCP Runtime Configurator, or environment variables.\nDependency risks: KMS (SecureString encryption), IAM (access control), CloudFormation (dynamic references).\nAlternatives: HashiCorp Consul (portable), Azure App Configuration, GCP Runtime Configurator, environment variables, etcd\nBlast radius if migrating: Low. Configuration data is trivially portable.\n\nHonest take: Parameter Store is not a significant lock-in risk. The data is simple and exportable. Use environment variables or HashiCorp Consul if you want zero cloud coupling.",
      "tags": [
        "cloud-portable"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-backup",
      "nodeId": "backup",
      "content": "<p><strong>Domains: 5</strong><br><br>Centralized backup management across EC2, EBS, RDS, DynamoDB, EFS, FSx, S3, and more. Backup plans with schedules and lifecycle rules. Backup vaults with access policies and vault lock (WORM). Cross-account and cross-Region backup copies. Ransomware protection: immutable backups via vault lock. Domain 5 Skill 5.2.4 \u2014 secure data replication and backup.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udfe1 Cloud-Portable (Medium Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Medium. Backup data can be exported. Backup policies need recreation.<br><strong>Control Plane Dependency:</strong> 4/5 \u2014 AWS-proprietary backup management. Backup plans, vaults, and policies are AWS-specific.<br><strong>Data Gravity:</strong> 3/5 \u2014 Backups are stored as AWS-specific artifacts (EBS snapshots, RDS snapshots, S3 data). Export is possible but slow.<br><strong>Runtime Coupling:</strong> 2/5 \u2014 Backup is not a runtime dependency. Scheduled operations.<br><br><strong>Why migration is hard:</strong> Vault lock (WORM) policies are AWS-specific. Cross-account backup sharing uses Organizations. Backup artifacts are in AWS-specific formats (EBS snapshots, RDS snapshots).<br><strong>Why migration could work:</strong> Backup is a solved problem. Veeam, Commvault, Rubrik, and Cohesity work across clouds. Backup data can be exported.<br><strong>Dependency risks:</strong> EC2/EBS (snapshots), RDS (snapshots), S3 (data copies), DynamoDB (backups), EFS (backups), KMS (encryption).<br><strong>Alternatives:</strong> Veeam (multi-cloud), Commvault, Rubrik, Cohesity, Clumio, Druva<br><strong>Blast radius if migrating:</strong> Medium. Backup data migration is slow for large datasets. But backup providers are cloud-agnostic.<br><br><strong>Honest take:</strong> AWS Backup is fine for AWS-only environments. If multi-cloud is possible, use Veeam or Rubrik from day one \u2014 they back up across clouds and on-prem, eliminating this as a migration concern entirely.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 5\n\nCentralized backup management across EC2, EBS, RDS, DynamoDB, EFS, FSx, S3, and more. Backup plans with schedules and lifecycle rules. Backup vaults with access policies and vault lock (WORM). Cross-account and cross-Region backup copies. Ransomware protection: immutable backups via vault lock. Domain 5 Skill 5.2.4 \u2014 secure data replication and backup.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udfe1 Cloud-Portable (Medium Lock-In)\n\nMigration Difficulty: Medium. Backup data can be exported. Backup policies need recreation.\nControl Plane Dependency: 4/5 \u2014 AWS-proprietary backup management. Backup plans, vaults, and policies are AWS-specific.\nData Gravity: 3/5 \u2014 Backups are stored as AWS-specific artifacts (EBS snapshots, RDS snapshots, S3 data). Export is possible but slow.\nRuntime Coupling: 2/5 \u2014 Backup is not a runtime dependency. Scheduled operations.\n\nWhy migration is hard: Vault lock (WORM) policies are AWS-specific. Cross-account backup sharing uses Organizations. Backup artifacts are in AWS-specific formats (EBS snapshots, RDS snapshots).\nWhy migration could work: Backup is a solved problem. Veeam, Commvault, Rubrik, and Cohesity work across clouds. Backup data can be exported.\nDependency risks: EC2/EBS (snapshots), RDS (snapshots), S3 (data copies), DynamoDB (backups), EFS (backups), KMS (encryption).\nAlternatives: Veeam (multi-cloud), Commvault, Rubrik, Cohesity, Clumio, Druva\nBlast radius if migrating: Medium. Backup data migration is slow for large datasets. But backup providers are cloud-agnostic.\n\nHonest take: AWS Backup is fine for AWS-only environments. If multi-cloud is possible, use Veeam or Rubrik from day one \u2014 they back up across clouds and on-prem, eliminating this as a migration concern entirely.",
      "tags": [
        "cloud-portable"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-datasync",
      "nodeId": "datasync",
      "content": "<p><strong>Domains: 5</strong><br><br>Automated data transfer between on-prem storage and AWS (S3, EFS, FSx). Encryption in transit (TLS). Bandwidth throttling. File integrity verification. New in C03 \u2014 Domain 5 Skill 5.2.4 (secure data replication).<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udfe2 Platform-Agnostic (Low Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Low. Data transfer tools are interchangeable.<br><strong>Control Plane Dependency:</strong> 2/5 \u2014 Data transfer with standard protocols (NFS, SMB, HDFS). Agent is AWS-specific.<br><strong>Data Gravity:</strong> 1/5 \u2014 DataSync transfers data, doesn't store it.<br><strong>Runtime Coupling:</strong> 1/5 \u2014 Scheduled transfer, not runtime dependency.<br><br><strong>Why migration is hard:</strong> DataSync agent and task configurations are AWS-specific.<br><strong>Why migration could work:</strong> rsync, rclone, and cloud provider transfer tools all move data. DataSync is a convenience wrapper.<br><strong>Dependency risks:</strong> S3, EFS, FSx (target storage), Direct Connect/VPN (network path).<br><strong>Alternatives:</strong> rclone (multi-cloud, open-source), rsync, Azure Data Box, GCP Transfer Service, Aspera<br><strong>Blast radius if migrating:</strong> None. Data transfer tool. Replace with rclone.<br><br><strong>Honest take:</strong> rclone is free, open-source, and works with every cloud and protocol. Use it instead if portability matters.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 5\n\nAutomated data transfer between on-prem storage and AWS (S3, EFS, FSx). Encryption in transit (TLS). Bandwidth throttling. File integrity verification. New in C03 \u2014 Domain 5 Skill 5.2.4 (secure data replication).\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udfe2 Platform-Agnostic (Low Lock-In)\n\nMigration Difficulty: Low. Data transfer tools are interchangeable.\nControl Plane Dependency: 2/5 \u2014 Data transfer with standard protocols (NFS, SMB, HDFS). Agent is AWS-specific.\nData Gravity: 1/5 \u2014 DataSync transfers data, doesn't store it.\nRuntime Coupling: 1/5 \u2014 Scheduled transfer, not runtime dependency.\n\nWhy migration is hard: DataSync agent and task configurations are AWS-specific.\nWhy migration could work: rsync, rclone, and cloud provider transfer tools all move data. DataSync is a convenience wrapper.\nDependency risks: S3, EFS, FSx (target storage), Direct Connect/VPN (network path).\nAlternatives: rclone (multi-cloud, open-source), rsync, Azure Data Box, GCP Transfer Service, Aspera\nBlast radius if migrating: None. Data transfer tool. Replace with rclone.\n\nHonest take: rclone is free, open-source, and works with every cloud and protocol. Use it instead if portability matters.",
      "tags": [
        "platform-agnostic"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    },
    {
      "id": "note-efs",
      "nodeId": "efs",
      "content": "<p><strong>Domains: 5</strong><br><br>Managed NFS file system. Encryption at rest (KMS) and in transit (TLS mount helper). IAM authorization and POSIX permissions for access control. EFS Lifecycle policies for cost optimization (IA storage class). EFS access points for application-specific entry points with enforced UID/GID.<br><br><strong>\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501</strong><br><strong>\ud83d\udfe1 Cloud-Portable (Medium Lock-In)</strong><br><br><strong>Migration Difficulty:</strong> Medium. NFS is portable. Data volume determines migration time.<br><strong>Control Plane Dependency:</strong> 3/5 \u2014 NFS protocol is standard. EFS-specific features (access points, lifecycle) are AWS-specific.<br><strong>Data Gravity:</strong> 3/5 \u2014 NFS data is exportable. Large file systems take time to copy.<br><strong>Runtime Coupling:</strong> 2/5 \u2014 NFS mount is standard. Applications don't know they're using EFS.<br><br><strong>Why migration is hard:</strong> EFS access points, IAM authorization, lifecycle policies, and mount targets are AWS-specific configuration.<br><strong>Why migration could work:</strong> NFS is NFS. Mount any NFS server and applications work identically. rsync moves the data.<br><strong>Dependency risks:</strong> EC2/ECS/Lambda (consumers), KMS (encryption), VPC (mount targets).<br><strong>Alternatives:</strong> Azure Files (NFS), GCP Filestore, NetApp Cloud Volumes, self-hosted NFS, GlusterFS, CephFS<br><strong>Blast radius if migrating:</strong> Low-Medium. NFS mount point change + data copy. Standard technology.<br><br><strong>Honest take:</strong> EFS is NFS with a managed wrapper. The protocol is ancient and universal. Data portability depends on volume size, not technology lock-in.</p>",
      "contentType": "tiptap",
      "plainText": "Domains: 5\n\nManaged NFS file system. Encryption at rest (KMS) and in transit (TLS mount helper). IAM authorization and POSIX permissions for access control. EFS Lifecycle policies for cost optimization (IA storage class). EFS access points for application-specific entry points with enforced UID/GID.\n\n\u2501\u2501\u2501 VENDOR LOCK-IN ANALYSIS \u2501\u2501\u2501\n\ud83d\udfe1 Cloud-Portable (Medium Lock-In)\n\nMigration Difficulty: Medium. NFS is portable. Data volume determines migration time.\nControl Plane Dependency: 3/5 \u2014 NFS protocol is standard. EFS-specific features (access points, lifecycle) are AWS-specific.\nData Gravity: 3/5 \u2014 NFS data is exportable. Large file systems take time to copy.\nRuntime Coupling: 2/5 \u2014 NFS mount is standard. Applications don't know they're using EFS.\n\nWhy migration is hard: EFS access points, IAM authorization, lifecycle policies, and mount targets are AWS-specific configuration.\nWhy migration could work: NFS is NFS. Mount any NFS server and applications work identically. rsync moves the data.\nDependency risks: EC2/ECS/Lambda (consumers), KMS (encryption), VPC (mount targets).\nAlternatives: Azure Files (NFS), GCP Filestore, NetApp Cloud Volumes, self-hosted NFS, GlusterFS, CephFS\nBlast radius if migrating: Low-Medium. NFS mount point change + data copy. Standard technology.\n\nHonest take: EFS is NFS with a managed wrapper. The protocol is ancient and universal. Data portability depends on volume size, not technology lock-in.",
      "tags": [
        "cloud-portable"
      ],
      "isPinned": false,
      "createdAt": "2026-02-19T10:19:45.000Z",
      "updatedAt": "2026-02-19T10:33:12.000Z"
    }
  ],
  "exportedAt": "2026-02-19T10:19:45.577Z"
}